<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2021-05-03T06:56:37-05:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Kakao Enterprise AI Research</title><author><name>Kakaoenterprise</name></author><entry><title type="html">U-Convolution Based Residual Echo Suppression With Multiple Encoders</title><link href="http://0.0.0.0:4000/papers/icassp2021-u-convolution-based-residual-eco-suppression" rel="alternate" type="text/html" title="U-Convolution Based Residual Echo Suppression With Multiple Encoders" /><published>2021-06-05T00:00:00-05:00</published><updated>2021-06-05T00:00:00-05:00</updated><id>http://0.0.0.0:4000/papers/icassp2021-u-convolution-based-residual-eco-suppression</id><content type="html" xml:base="http://0.0.0.0:4000/papers/icassp2021-u-convolution-based-residual-eco-suppression"></content><author><name>chris:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"></summary></entry><entry><title type="html">Suppressing Spoof-irrelevant Factors for Domain-agnostic Face Anti-spoofing</title><link href="http://0.0.0.0:4000/papers/ieeeaccess-face-anti-spoofing" rel="alternate" type="text/html" title="Suppressing Spoof-irrelevant Factors for Domain-agnostic Face Anti-spoofing" /><published>2021-04-30T00:00:00-05:00</published><updated>2021-04-30T00:00:00-05:00</updated><id>http://0.0.0.0:4000/papers/ieeeaccess-face-anti-spoofing</id><content type="html" xml:base="http://0.0.0.0:4000/papers/ieeeaccess-face-anti-spoofing">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Face anti-spoofing aims to prevent false authentications of face recognition systems by distinguishing whether an image is originated from a human face or a spoof medium. In this work, we note that images from unseen domains having different spoof-irrelevant factors (e.g., background patterns and subject) induce domain shift between source and target distributions. Also, when the same SiFs are shared by the spoof and genuine images, they show a higher level of visual similarity and this hinders accurate face anti-spoofing. Hence, we aim to minimize the discrepancies among different domains via alleviating the effects of SiFs, and achieve improvements in generalization to unseen domains. To realize our goal, we propose a novel method called a Doubly Adversarial Suppression Network (DASN) that is trained to neglect the irrelevant factors and to focus more on faithful task-relevant factors. Our DASN consists of two types of adversarial learning schemes. In the first adversarial learning scheme, multiple SiFs are suppressed by deploying multiple discrimination heads that are trained against an encoder. In the second adversarial learning scheme, each of the discrimination heads is also adversarially trained to suppress a spoof factor, and the group of the secondary spoof classifier and the encoder aims to intensify the spoof factor by overcoming the suppression. We evaluate the proposed method on four public benchmark datasets, and achieve remarkable evaluation results in generalizing to unseen domains. The results demonstrate the effectiveness of the proposed method.&lt;/p&gt;</content><author><name>김태욱:카카오엔터프라이즈</name></author><category term="papers" /><category term="face anti-spoofing" /><summary type="html">Abstract</summary></entry><entry><title type="html">RYANSQL: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-SQL in Cross-Domain Databases</title><link href="http://0.0.0.0:4000/papers/cljournal2021-ryansql" rel="alternate" type="text/html" title="RYANSQL: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-SQL in Cross-Domain Databases" /><published>2021-03-26T00:00:00-05:00</published><updated>2021-03-26T00:00:00-05:00</updated><id>http://0.0.0.0:4000/papers/cljournal2021-ryansql</id><content type="html" xml:base="http://0.0.0.0:4000/papers/cljournal2021-ryansql">&lt;p&gt;스파이더 챌린지&lt;sup&gt;SPIDER Text-to-SQL Challenge&lt;/sup&gt; 성과를 정리한 공동 연구 논문이 Computational Linguistics에 실렸습니다. 미국 예일대학교에서 주최한 스파이더 챌린지는 각종 데이터를 정리∙보관할 때 사용하는 데이터베이스와 자연어 형태의 사용자 질의가 주어졌을때, 이 질의문을 SQL&lt;sup&gt;Structured Query Language&lt;/sup&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;문으로 변환해주는 Text-to-SQL&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 알고리즘의 정확도를 평가합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-03-26-ryansql/001.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ 표 1 ] 주어진 자연어 문장과 데이터베이스를 이용해 SQL 문을 생성하는 예시&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;자연어 질의문을 SQL 문으로 변환하는 데에는 스케치 기반 슬롯 채우기&lt;sup&gt;sketch-based Slot Filling&lt;/sup&gt;가 주로 활용돼 왔습니다. SELECT&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 문에 몇 개의 열&lt;sup&gt;column&lt;/sup&gt;을 입력해야 하는지, 어떤 열을 선택해야 하는지, 집계 함수&lt;sup&gt;aggregator&lt;/sup&gt;&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;로 무엇을 써야 하는지 등 판별해야 할 정보&lt;sup&gt;slot&lt;/sup&gt;를 먼저 구분하고 나서, 각 정보의 값을 채워넣는 식입니다. 다만 이 방식으로는 쿼리 속에 또 다른 쿼리가 든 중첩 질의&lt;sup&gt;nested query&lt;/sup&gt;를 생성하는 데 한계가 있습니다. SELECT 문의 개수가 정해지지 않아서 전체 설계도 자체를 그릴 수 없기 때문입니다.&lt;/p&gt;

&lt;p&gt;공동 연구팀이 제안한 Text-to-SQL 알고리즘인 RYQNSQL&lt;sup&gt;Recursively Yielding Annotation Network for SQL&lt;/sup&gt;은 대규모 영어 비라벨링 말뭉치를 사전학습한 언어 모델인 BERT에 자체 고안한 SPC&lt;sup&gt;Statement Position Code&lt;/sup&gt; 기법을 적용했습니다. SPC는 슬롯을 채울 때 중첩된 SELECT문을 좀 더 정확하게 생성할 수 있도록 합니다. 실험 결과, 스파이더 벤치마크 데이터셋에 대해 현재 최고 성능의(SOTA)&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; 모델보다 3.2%p 더 높은 58.2%의 정확도를 달성했습니다.&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈는 데이터의 스키마(테이블 이름, 열 이름)뿐만 아니라 실제 값도 활용하는 방식 등으로 자사 Text-to-SQL 알고리즘의 성능과 사용성을 높여 기업 데이터베이스 활용의 문턱을 낮추는 데 기여할 계획입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overall-architecture&quot;&gt;Overall Architecture&lt;/h1&gt;

&lt;p&gt;Figure 1 shows the overall network architecture of the input encoder. The input encoder consists of five layers: Embedding layer, Embedding Encoder layer, Question-Column Alignment layer, Table Encoder layer, and Question-Table Alignment layer. Table 1 shows the proposed sketch for a SELECT statement. The sketch-based slot-filling decoder predicts values for slots of the proposed sketch, as well as the number of slots.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-03-26-ryansql/002.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ Figure 1 ] Network architecture of the proposed input encoder. S represents self-attention.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-03-26-ryansql/003.png&quot; width=&quot;80%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ table 1 ] Proposed sketch for a SELECT statement. $TBL and $COL represent a table and a column, respectively. $AGG is one of {none, max, min, count, sum, avg}, $ARI is one of the arithmetic operators {none, -, +, *, / }, and $COND is one of the conditional operators {between, =, &amp;gt;, &amp;lt;, &amp;gt;=, &amp;lt;=, !=, in, like, is, exists}. $DIST and $NOT are boolean variables representing the existence of keywords DISTINCT and NOT, respectively. $ORD is a binary value for keywords ASC/DESC, and $CONJ is one of conjunctions {AND, OR}. $VAL is the value for WHERE/HAVING condition; $SEL represents the slot for another SELECT statement.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;Table 2 shows that the proposed system RYANSQL improves the previous sketch-based slot filling system RCSQL by a large margin of 15% on the dev set. Note that the RCSQL fine-tuned another well known pretrained language model ELMo. With the use of BERT, among the systems without database content, the proposed systems (RYANSQL + BERT and RYANSQL v2 + BERT) outperforms the previous state-of-the-art by 2.5% and 4.9% respectively on the hidden test dataset. The proposed system still shows competitive results compared to the systems using database content; RATSQL v3 + BERT outperforms the proposed system by better aligning user questions and database schemas using database content.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-03-26-ryansql/005.png&quot; width=&quot;90%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ Table 2 ] Evaluation results of the proposed systems and other state-of-the-art systems.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We evaluated the proposed models on the CSpider dataset. CSpider is a chinese-translated version of the Spider benchmark. Only the question of the spider dataset is translated; database table names and column names remain as English. Evaluation on the CSpider dataset will show if the proposed model could be applied on the different languages, even when the question language and database schema language are different. To handle the case, we used multilingual BERT, which has the same network architecture with BERT-base but is trained using multilingual corpus.&lt;/p&gt;

&lt;p&gt;The results are shown in Table 3. Compared to the exact matching accuracy 51.4% of RYANSQL + BERT-base on Spider dataset, the multilingual version shows 10% lower accuracy on dev set, but still shows comparable results to other state-of-the-art systems which are designed for CSpider dataset. Our proposed system showed 34.7% test accuracy on the test set, and ranked at 2nd place on the leaderboard.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-03-26-ryansql/005.png&quot; width=&quot;90%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ Table 3 ] Evaluation results on CSpider dataset with other state-of-the-art systems.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;footnote&quot;&gt;footnote&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;관계형 데이터베이스 관리를 위해 설계된 특수목적의 프로그래밍 언어 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;NLI2DB&lt;sup&gt;natural language interface to databases&lt;/sup&gt;라고도 부른다. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;테이블 전체 또는 일부 열과 행 값을 호출하는 명령어 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;값 집합에 대한 산술적인 계산(레코드의 수, 값의 합, 값의 평균, 최대값, 최소값)의 결과값을 출력한다. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;논문 제출 시점(2020년 4월) 최고 성능 &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>heuristic:카카오엔터프라이즈</name></author><category term="papers" /><category term="NLI2DB" /><category term="Text-to-SQL" /><summary type="html">스파이더 챌린지SPIDER Text-to-SQL Challenge 성과를 정리한 공동 연구 논문이 Computational Linguistics에 실렸습니다. 미국 예일대학교에서 주최한 스파이더 챌린지는 각종 데이터를 정리∙보관할 때 사용하는 데이터베이스와 자연어 형태의 사용자 질의가 주어졌을때, 이 질의문을 SQLStructured Query Language1문으로 변환해주는 Text-to-SQL2 알고리즘의 정확도를 평가합니다. 관계형 데이터베이스 관리를 위해 설계된 특수목적의 프로그래밍 언어 &amp;#8617; NLI2DBnatural language interface to databases라고도 부른다. &amp;#8617;</summary></entry><entry><title type="html">A Plug-in Method for Representation Factorization in Connectionist Models</title><link href="http://0.0.0.0:4000/papers/ieee2020-fden" rel="alternate" type="text/html" title="A Plug-in Method for Representation Factorization in Connectionist Models" /><published>2021-02-10T00:00:00-06:00</published><updated>2021-02-10T00:00:00-06:00</updated><id>http://0.0.0.0:4000/papers/ieee2020-fden</id><content type="html" xml:base="http://0.0.0.0:4000/papers/ieee2020-fden">&lt;p&gt;일반적으로 딥러닝 모델의 성능은 과제를 수행하는 데 필요한 필수 정보를 추출하고 이를 최대한 압축한 임베딩 벡터&lt;sup&gt;embedding vector&lt;/sup&gt;를 제대로 표현하는 능력에 있다고 해도 과언이 아닙니다. 다만 각 차원이 여러 요인을 함축적으로 포함하는 그 특성상, 사람이 벡터를 해석하는 데에는 한계가 있습니다. 머리 색, 머리 길이, 얼굴형, 눈썹 모양, 얼굴색 등이 얼굴을 구성하는 요소라고 본다면, 딥러닝 모델이 생성한 벡터는 차원1-머리색/머리길이, 차원2-머리길이/얼굴색 등으로 여러 차원에 여러 요인이 복잡하게 얽혀 있죠.&lt;/p&gt;

&lt;p&gt;이에 데이터를 독립 요인&lt;sup&gt;independent factor&lt;/sup&gt;에 상응하는 해석 가능한 표현&lt;sup&gt;disentangled representation&lt;/sup&gt;으로 만드는 방법에 관한 연구가 활발하게 이뤄지고 있습니다. 공동 연구팀 또한 이 대열에 합류, 총 상관계수&lt;sup&gt;total correlation&lt;/sup&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;를 최소화하는 방식으로 의미상 제어할 수 있는 요인으로 임베딩 벡터를 분해하는 기법인 FDEN&lt;sup&gt;Factors Decomposer-Entangler Network&lt;/sup&gt;을 제안했습니다.&lt;/p&gt;

&lt;p&gt;FDEN은 학습된 모델&lt;sup&gt;pre-trained model&lt;/sup&gt;의 가중치&lt;sup&gt;weight&lt;/sup&gt; 값을 변경하지 않고도 사용할 수 있는 플러그인&lt;sup&gt;plug-in&lt;/sup&gt; 방식으로 동작합니다. 모델이 추출한 임베딩 벡터에서 요인에 영향을 미치는 부분을 찾아내는(해석하는) 별도의 단계를 뒀다는 의미입니다. 이 덕분에 우수한 성능을 내는 여러 학습 모델에 바로 적용해볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-10-IEEE-FDEN/001.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ 그림 1 ] 가중치를 고정한 사전학습한 모델에 플러그인 방식으로 동작하는 FDEN은 출력된 임베딩 벡터 z를 해석가능한 표현 \(\tilde{z}\)로 바꾼다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;연구팀은 스타일 변환&lt;sup&gt;style transfer&lt;/sup&gt;, 이미지 간 번역&lt;sup&gt;image-to-image translation&lt;/sup&gt;, 퓨샷 러닝&lt;sup&gt;few-shot learning&lt;/sup&gt; 등 다양한 태스크에서 우수한 성능을 내는 모델에 FDEN을 적용했습니다. 그 결과, FDEN은 사람이 라벨링한 요인을 효과적으로 분해할 수 있었습니다. 독립 요인(\(f_0\)+\(f_1\)+\(f_2\)…+\(f_n\))을 합쳐 기존 데이터(\(z\))와 유사한 데이터(\(\tilde{z}\))도 생성할 수 있음을 확인했습니다. 이는 사람이 해석 가능한 독립 요인을 조절해 새로운 데이터 혹은 사람이 원하는 데이터를 만들어낼 수 있음을 시사합니다.&lt;/p&gt;

&lt;p&gt;연구팀은 비지도학습&lt;sup&gt;unsupervised learning&lt;/sup&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;을 이용해 사람이 찾지 못한 요인을 섬세하게 분해할 방법론에 관한 연구를 진행할 계획입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overall-architecture&quot;&gt;Overall Architecture&lt;/h1&gt;

&lt;p&gt;The objective of FDEN is to decompose input representation \(z\) into independent and semantically interpretable factors without losing the original information in the latent or feature representation \(z\). To achieve this aim, we compose an FDEN with three modules (Figure. 1): Decomposer \(D\), Factorizer \(F\), and Entangler \(E\). Note that because FDEN uses a fixed pretrained network and deals with the latent or feature representation from the network, it allows factorizing the input representation for other new tasks while maintaining the network capacity or power for its original tasks intact.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-10-IEEE-FDEN/002.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Figure 1 ] FDEN is divided into three modules: Decomposer \(D\), Factorizer \(F\), and Entangler \(E\). The model is an autoencoder-like architecture that takes representation \(z\) as the input and reconstructs its original representation (\(\tilde{z}\)). (a) First, Decomposer \(D\) takes a representation \(z\) from a fixed pretrained network as the input and decomposes it into a set of factors \(f_i\) (\(∀_i\) ∈ \(N\)). (b) Next, Factorizer \(F\) uses an information theoretic way to maximize the independence of each factor. (c) Finally, Entangler \(E\) takes the factors and reconstructs their original representation (\(\tilde{z}\)).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;Our objective here is to demonstrate that each module of FDEN is effective at decomposing a latent representation into independent factors. First, we evaluate the effectiveness of factors by performing various downstream tasks. Next, we analyze individual units of factors to verify if a representation is indeed reasonably factorized.  We perform style transfer with human labeled attributes (Fig. 2). The results of style transfer with FDEN confirms a clear transfer of attributes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-10-IEEE-FDEN/003.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Figure 2 ] Results of style transfer for the CelebA-128 dataset with N=7 factors (where \(f_0\) is the style factor). Images in the first and second columns are reconstructed images from Pioneer Network and FDEN, respectively. The following images are reconstructed images with one attribute opposite to the input image (e.g., \(1{^{st}}\) row \(f_3\): “not bald” transferred to “bald”; \(2{^{nd}}\) row \(f_3\): “young” transferred to “not young”). The original attributes of both input images are: “not bald”, “male”, “young”, “without smile”, “without beard”, “without eyeglasses” (note that the 1st row image is annotated as “with goatee, but without beard”).&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;주어진 요인의 독립성&lt;sup&gt;independence&lt;/sup&gt;를 직접 계산할 수 있는 수치 &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;사람이 라벨링하지 않은 데이터를 이용한 학습 방법 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>윤재석:고려대학교</name></author><category term="papers" /><category term="disentangled representation" /><category term="factorial representation" /><summary type="html">일반적으로 딥러닝 모델의 성능은 과제를 수행하는 데 필요한 필수 정보를 추출하고 이를 최대한 압축한 임베딩 벡터embedding vector를 제대로 표현하는 능력에 있다고 해도 과언이 아닙니다. 다만 각 차원이 여러 요인을 함축적으로 포함하는 그 특성상, 사람이 벡터를 해석하는 데에는 한계가 있습니다. 머리 색, 머리 길이, 얼굴형, 눈썹 모양, 얼굴색 등이 얼굴을 구성하는 요소라고 본다면, 딥러닝 모델이 생성한 벡터는 차원1-머리색/머리길이, 차원2-머리길이/얼굴색 등으로 여러 차원에 여러 요인이 복잡하게 얽혀 있죠.</summary></entry><entry><title type="html">Multi-level Distance Regularization for Deep Metric Learning</title><link href="http://0.0.0.0:4000/papers/aaai2021-mdr" rel="alternate" type="text/html" title="Multi-level Distance Regularization for Deep Metric Learning" /><published>2021-02-02T00:00:00-06:00</published><updated>2021-02-02T00:00:00-06:00</updated><id>http://0.0.0.0:4000/papers/aaai2021-mdr</id><content type="html" xml:base="http://0.0.0.0:4000/papers/aaai2021-mdr">&lt;p&gt;딥러닝 기반 거리 학습(DML&lt;sup&gt;Deep Metric Learning&lt;/sup&gt;)은 주어진 두 데이터 간 의미적인 거리&lt;sup&gt;pairwise distance&lt;/sup&gt;를 학습하는 방법입니다. 특히 입력 이미지와 유사한 이미지를 찾는 데 효과적인 이 기술은 이미지 검색&lt;sup&gt;image retrieval&lt;/sup&gt;, 상품 추천, 얼굴 인식&lt;sup&gt;face recognition&lt;/sup&gt;과 같은 태스크에서 핵심적으로 활용되고 있습니다.&lt;/p&gt;

&lt;p&gt;선행 연구에서는 같은 범주의 이미지 간 거리는 더 가깝게, 범주가 다른 이미지의 거리는 더 멀게 만드는 손실 함수&lt;sup&gt;loss function&lt;/sup&gt; 재정의에 집중했습니다. 다만 거리가 아닌 임베딩 벡터&lt;sup&gt;embedding vector&lt;/sup&gt;를 제약하는 L2 정규화&lt;sup&gt;L2 normalization&lt;/sup&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;로는 직접적인 거리 정규화가 어려워서 모델이 쉽게 과적합&lt;sup&gt;overfitting&lt;/sup&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;되는 문제가 있었습니다.&lt;/p&gt;

&lt;p&gt;이에 공동 연구팀은 DML에 더 적합한 새로운 거리 정규화 기법인 MDR&lt;sup&gt;Multi-level Distance Regularization&lt;/sup&gt;을 제안했습니다. MDR은 데이터 간의 거리 분포를 여러 부분으로 나눈 뒤 각 부분을 대표하는 레벨&lt;sup&gt;level&lt;/sup&gt;을 학습하고, 각 레벨에 속하는 두 쌍의 데이터 간 거리를 정규화합니다. MDR는 기존 손실 함수와 함께(jointly) 동작하며 쉬운 샘플&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;이 학습에 영향을 주지 못하거나 어려운 샘플&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;이 학습을 지배하는 현상을 방지합니다.&lt;/p&gt;

&lt;p&gt;트리플릿 손실 함수&lt;sup&gt;Triplet loss function&lt;/sup&gt;에 MDR을 적용해본 결과, 주요 벤치마크 데이터셋에서 유의미한 성능 향상을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈는 이번 연구로 얻은 기술력과 경험을 바탕으로 상품 추천, 얼굴 인식 등 관련 기술을 고도화할 계획입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p class=&quot;tech-ground&quot;&gt;☛ Tech Ground 데모 페이지 바로 가기 : &lt;b&gt;&lt;a href=&quot;https://labs.kakaoi.ai/facedetection&quot;&gt;얼굴 검출&lt;/a&gt;&lt;/b&gt; 데모&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overall-architecture&quot;&gt;Overall Architecture&lt;/h1&gt;

&lt;p&gt;At first, MDR normalizes pairwise distances among the embedding vectors of a mini-batch, with their mean and standard deviation to obtain the objective degree of similarity between a pair by considering overall distribution. MDR defines the multiple levels that represent various degrees of similarity for pairwise distances, and the levels and the be- longing distances are trained to approach each other (Figure 1b). A conventional loss function of DML struggles to optimize a model by overcoming the disturbance from the proposed regularization. Therefore, the learning process succeeds in learning a model with a better generalization ability.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-mdr/001.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Figure 1 ] Conceptual comparison between the conventional learning scheme and our learning scheme. (a) illustrates the triplet learning, which is one of the representative conventional learning. It increases the relative difference between distances of a positive pair and that of a negative pair more than margin m. (b) illustrates our learning combined with the triplet learning. It has multiple levels with disjoint intervals to reflect various degrees of similarity between pairs. It disturbs the learning procedure to construct an efficient embedding space by preventing the pairwise distances from deviating from its belonging level.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-mdr/002.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Figure 2 ] Learning procedure of the proposed MDR. The embedding network generates embedding vectors from given images. Our MDR computes a matrix of pairwise distances for the embedding vectors, and then, the distances are normalized after vectorization. In our learning scheme, a model is trained by simultaneously optimizing the conventional metric learning loss such as Triplet loss (Schroff, Kalenichenko, and Philbin 2015) and the proposed loss, which regularizes the normalized pairwise distances with multiple levels.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;We employ the four standard datasets of deep metric learning for evaluations: CUB-200-2011 (CUB-200), Cars-196, Stanford Online Product (SOP) and In-Shop Clothes Retrieval (In-Shop). Our method significantly outperforms the other state-of-the-art methods in all recall criteria for all datasets.&lt;/p&gt;

&lt;p&gt;We prove the effectiveness of MDR by showing the improvements that greatly exceed the existing methods, and by extensively performing the ablation studies of its behaviors. By applying our MDR, many methods can be significantly improved without any extra burdens at inference time.&lt;/p&gt;

&lt;p&gt;Moreover, our extensive ablation studies show that MDR can be adopted to any backbone networks and any distance-based loss functions to improve the performance of a model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-mdr/003.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-mdr/004.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Table 1 ] Recall@K comparison with state-of-the-art methods. The baseline methods and MDR are grouped in the gray-colored rows. † indicates that the model is trained and tested with large images of 256 × 256 following the setting of (Jacob et al. 2019). We round reported values to the first decimal place.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-mdr/005.png&quot; width=&quot;85%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Figure 3 ] Class centers in the embedding space of two models trained without MDR (Triplet &amp;amp; Triplet+L2 Norm) and one model trained with MDR (Triplet+MDR). We visualize using t-SNE on CUB-200.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-mdr/006.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Table 2 ] Recall@1 comparison with various backbone net- works, loss functions, and level configurations. The models of (a) are trained with Triplet loss. The models of (b) use IBN as the backbone network. In (a) and (b), a column with ✓ indicates that the models are trained with MDR.&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-mdr/007.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-mdr/008.png&quot; width=&quot;55%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Figure 4 ] (a) compares the three methods on various dimensionalities of the embedding vector on CUB-200 and Cars- 196. (b) shows the learning curves of the three methods for the training and test set on CUB-200.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;footnote&quot;&gt;footnote&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;정규화는 모델의 설명력을 높이면서도 복잡도를 줄이는 기법을 가리킨다. 정규화 방법론 중 하나인 L2 정규화는 학습 데이터에 따라 특정 가중치의 값이 지나치게 커지는 걸 방지하도록 한다. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;훈련 데이터에 대한 손실(함수)값이 감소한다고 해서 학습하지 않은 새로운 데이터에서도 같은 정확도를 낸다는 보장은 없다. 이처럼 훈련 데이터에 대해서만 좋은 결과를 내는 현상을 과적합이라고 표현한다. 훈련 데이터에 포함된 노이즈&lt;sup&gt;noise&lt;/sup&gt;마저 버린 상태로, 훈련 데이터가 적을수록 과적합 정도는 심해진다. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;손실값이 작아서 실제 가중치 업데이트에 미비한 영향을 미치는 샘플 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;손실값이 커서 가중치 업데이트에서 지배적인 영향을 미치는 샘플 &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>aiden:카카오엔터프라이즈</name></author><category term="papers" /><category term="deep metric learning" /><category term="regularization" /><summary type="html">딥러닝 기반 거리 학습(DMLDeep Metric Learning)은 주어진 두 데이터 간 의미적인 거리pairwise distance를 학습하는 방법입니다. 특히 입력 이미지와 유사한 이미지를 찾는 데 효과적인 이 기술은 이미지 검색image retrieval, 상품 추천, 얼굴 인식face recognition과 같은 태스크에서 핵심적으로 활용되고 있습니다.</summary></entry><entry><title type="html">Do Response Selection Models Really Know What’s Next? Utterance Manipulation Strategies for Multi-turn Response Selection</title><link href="http://0.0.0.0:4000/papers/aaai2021-multi-turn-response-selection" rel="alternate" type="text/html" title="Do Response Selection Models Really Know What’s Next? Utterance Manipulation Strategies for Multi-turn Response Selection" /><published>2021-02-02T00:00:00-06:00</published><updated>2021-02-02T00:00:00-06:00</updated><id>http://0.0.0.0:4000/papers/aaai2021-multi-turn-response-selection</id><content type="html" xml:base="http://0.0.0.0:4000/papers/aaai2021-multi-turn-response-selection">&lt;p&gt;응답 선택&lt;sup&gt;response selection&lt;/sup&gt;은 다자 간의 대화&lt;sup&gt;multi-turn dialog&lt;/sup&gt;를 보고 후보 문장 중 맥락에 가장 어울리는 문장을 선택하는 태스크를 가리킵니다. 최근에는 BERT, RoBERTa, ELECTRA와 같은 대규모 말뭉치를 사전학습한 언어 모델&lt;sup&gt;language model&lt;/sup&gt;을 이용해 관련 벤치마크 테스트에서 눈에 띄는 성능 향상이 이뤄졌습니다.&lt;/p&gt;

&lt;p&gt;최신 언어 모델 기반한 응답 선택 모델은 대화와 응답 후보군을 입력받으면, 후보 문장의 적정성 여부를 이진 분류&lt;sup&gt;binary classification&lt;/sup&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;한 결과를 내놓습니다. 공동 연구팀은 의미적 유사도를 기반으로 점수를 내는 언어 모델의 특성상, 응답으로 적절하지 않은 문장에 정답보다 더 높은 점수를 부여하는 경향성을 보이는 기존 방식의 한계를 지적했습니다. 이는 기존의 손실 함수&lt;sup&gt;loss function&lt;/sup&gt;가 발화&lt;sup&gt;utterance&lt;/sup&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 간 연관성&lt;sup&gt;coherence&lt;/sup&gt;을 충분히 표현하지 못해서 생기는 거로 분석됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-Utterance-Manipulation-Strategies/001.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ 그림 1 ] LM에 기반한 최신의 응답 선택 모델은 대화의 맥락에 호응하지 않음에도 불구, 의미적 유사도가 높은 문장 b에 더 높은 점수를 부여하고 있다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;공동 연구팀은 기존의 한계를 극복하고자 UMS&lt;sup&gt;Utterance Manipulation Strategies&lt;/sup&gt;를 제안했습니다. 이 기법은 대화에서 특정 발화가 어느 위치에 삽입돼야 하는지(insertion), 현재 대화 흐름에서 어떤 발화가 올바르지 않은지(deletion), 특정 발화의 바로 이전 발화의 위치가 어딘지(search)를 배우는 3가지 태스크로 구성됩니다. 자가지도학습&lt;sup&gt;self-supervised learning&lt;/sup&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;이라 사람이 따로 라벨링 작업을 할 필요가 없고, 기존의 응답 선택 모델을 따로 조정할 필요 없이 미세조정&lt;sup&gt;fine-tuning&lt;/sup&gt;단계에서 합동 훈련&lt;sup&gt;joint-training&lt;/sup&gt;을 진행하면 됩니다&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;실험 결과, UMS를 적용한 응답 선택 모델은 대화 일관성을 효과적으로 학습하며, 여러 언어의 벤치마크&lt;sup&gt;benchmark&lt;/sup&gt;에서 기존 성능을 크게 넘어섰습니다.&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈는 대화의 맥락에 호응하는 응답을 선택하는 모델의 강건성을 향상하는 연구를 계속 진행할 계획입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overall-architecture&quot;&gt;Overall Architecture&lt;/h1&gt;

&lt;p&gt;Figure 1 describes the overview of our proposed method, utterance manipulation strategies. We propose a multi-task learning framework, which consists of three highly effective auxiliary tasks for multi-turn response selection, utterance 1) insertion, 2) deletion, and 3) search. These tasks are jointly trained with the response selection model during the fine-tuning period. To train the auxiliary tasks, we add new special tokens, [INS], [DEL], and [SRCH] for the utterance insertion, deletion, and search tasks, respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-Utterance-Manipulation-Strategies/002.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Figure 1 ] An overview of Utterance Manipulation Strategies. Input sequence for each manipulation strategy is dynamically constructed by extracting k consecutive utterances from the original dialog context during the training period. Also, target utterance is randomly chosen from either the dialog context (Insertion, Search) or the random dialog (Deletion).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;We obtained new state-of-the-art results on multiple public benchmark datasets (i.e., Ubuntu, Douban, and E-Commerce) and significantly improved results on Korean open-domain dialog corpus.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-02-02-AAAI-Utterance-Manipulation-Strategies/003.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Table 2 ] Results on Ubuntu, Douban, and E-Commerce datasets. All the evaluation results except ours are cited from published literature (Tao et al. 2019b; Yuan et al. 2019; Gu et al. 2020). The underlined numbers mean the best performance for each block and the bold numbers mean state-of-the-art performance for each metric. † denotes statistical significance (p-value &amp;lt; 0.05).&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;footnote&quot;&gt;footnote&lt;/h1&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;이진 분류 과정은 다음과 같다. (1) 비선형 활성 함수&lt;sup&gt;activation function&lt;/sup&gt;인 시그모이드&lt;sup&gt;sigmoid&lt;/sup&gt;를 이용해 점수를 산출한다(eg., 0번(올바른 응답) 클래스: 0.87, 1번(올바르지 않은 응답) 클래스: 0.6). (2) 각 클래스 점수값 중 큰 쪽을 선택하는 이진분류를 수행한다. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;대화 속에서 주고 받는 말의 단위 &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;비라벨링 데이터만 주어진 상태에서 입력 데이터 일부를 라벨로 사용하거나, 사전 지식에 따라 라벨을 스스로 만들어 모델을 훈련하는 방식 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;공동 연구팀이 제안한 방법은 4개의 손실값(response selection loss + insertion loss + deletion loss + search loss Loss)을 최소화하는 가중치&lt;sup&gt;weight&lt;/sup&gt; 탐색을 목표로 학습을 진행한다. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>황태선:고려대학교</name></author><category term="papers" /><category term="response selection" /><category term="multi-turn dialog" /><summary type="html">응답 선택response selection은 다자 간의 대화multi-turn dialog를 보고 후보 문장 중 맥락에 가장 어울리는 문장을 선택하는 태스크를 가리킵니다. 최근에는 BERT, RoBERTa, ELECTRA와 같은 대규모 말뭉치를 사전학습한 언어 모델language model을 이용해 관련 벤치마크 테스트에서 눈에 띄는 성능 향상이 이뤄졌습니다.</summary></entry><entry><title type="html">EMNLP 2020 - 다국어 번역 논문 2편을 소개합니다</title><link href="http://0.0.0.0:4000/deepdive/201217" rel="alternate" type="text/html" title="EMNLP 2020 - 다국어 번역 논문 2편을 소개합니다" /><published>2020-12-17T00:00:00-06:00</published><updated>2020-12-17T00:00:00-06:00</updated><id>http://0.0.0.0:4000/deepdive/201217</id><content type="html" xml:base="http://0.0.0.0:4000/deepdive/201217">&lt;p&gt;카카오엔터프라이즈 AI Lab(이하 AI Lab)이 낸 3편의 논문이 EMNLP&lt;sup&gt;Empirical Methods in Natural Language Processing&lt;/sup&gt;에 게재 승인됐습니다. 자연어처리에서 경험적 방법론을 다루는 이 학회는 ACL&lt;sup&gt;Association for Computational Linguistics&lt;/sup&gt;, NAACL&lt;sup&gt;NORTH American Chapter of the ACL&lt;/sup&gt;과 함께 전산언어학 분야에서는 인지도가 높습니다. 올해에는 총 3,677편의 논문 중 754편이 통과됐습니다.&lt;/p&gt;

&lt;p&gt;이번 글에서는 EMNLP에 게재된 자사 논문 중, 신경망 기반 다국어 기계 번역 모델(MNNT&lt;sup&gt;Multilingual Neural machine Translation&lt;/sup&gt;)에 관한 최신 기법을 다룬 2편의 논문&lt;a href=&quot;#fa91:rf:1&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:1&quot;&gt;[1]&lt;/a&gt;&lt;a href=&quot;#fa91:rf:2&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:2&quot;&gt;[2]&lt;/a&gt;을 상세히 다뤄보고자 합니다. 두 논문을 저술한 AI Lab AI기술실 컨택스트팀 소속의 류성원 연구원과 손보경 연구원을 만나봤습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;※보다 명확하고 간결한 방식으로 대상을 지칭하기 위해 본문에서는 다국어 번역을 수행하는 모델을 MNMT로, 두 언어 간의 번역을 수행하는 모델을 NMT라고 명명했습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p class=&quot;dot-line&quot;&gt;∙  ∙  ∙  ∙  ∙  ∙  ∙&lt;/p&gt;

&lt;h1 id=&quot;다국어-번역을-위한-모델-아키텍처&quot;&gt;다국어 번역을 위한 모델 아키텍처&lt;/h1&gt;

&lt;p&gt;두 논문에서 공통적으로 언급하는 다국어 번역 아키텍처에 대한 배경부터 짚어보겠습니다.&lt;/p&gt;

&lt;p&gt;인코더&lt;sup&gt;encoder&lt;/sup&gt;와 디코더&lt;sup&gt;decoder&lt;/sup&gt; 기반 seq2seq&lt;sup&gt;sequence to sequence&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;#fa91:fn:1&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 아키텍처의 NMT는 기계 번역에 큰 혁신을 안겼습니다. 서로 다른 두 언어의 문장으로 구성된 대량의 병렬 코퍼스&lt;sup&gt;parallel corpus&lt;/sup&gt;로부터 단어 의미와 단어 순서, 문장 구조, 단어 간의 의존 관계 등번역에 필요한 모든 정보(문장 벡터)&lt;a href=&quot;#fa91:rf:3&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:3&quot;&gt;[3]&lt;/a&gt;를 스스로 학습하기 때문입니다. 이로써 인간이 기계에 세세한 번역 규칙을 하나씩 가르칠 필요가 없게 됐습니다.&lt;/p&gt;

&lt;p&gt;기존에는 다국어 번역(\(N\)개 언어)을 위해 \(N×(N-1)\)개의 NMT를 각기 따로 두었습니다(Single NMT, [그림 1-(1)]). 영어, 한국어, 스페인어를 번역해야 한다면 [영→한, 한→영, 한→스, 스→한, 영→스, 스→한] 이렇게 총 여섯 개의 NMT를 만드는 식이죠. 다만 문제는 언어 수가 늘어나는 만큼 훈련해야 할 모델 수, 좀 더 자세히 말하면 매개변수&lt;sup&gt;parameter&lt;/sup&gt; 수가 제곱으로 커진다(\(O\)(\(N^{2}\)))는 겁니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-17-201217/001.png&quot; width=&quot;90%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ 그림 1 ] 3개국 언어 간 번역 태스크를 해결하는 3가지 방식&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이에 매개변수 수를 획기적으로 줄인 multi-way MNMT&lt;a href=&quot;#fa91:rf:4&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:4&quot;&gt;[4]&lt;/a&gt;가 새로 제안됐습니다([그림 1-(3)]). 여러 NMT를 각기 따로 두는 대신, 언어별 인코더와 디코더를 두고 여러 방향에서 이를 공유하는 구조&lt;sup&gt;&lt;a href=&quot;#fa91:fn:2&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;를 마련했습니다. N×(N-1)개의 모델, 다시 말해 N×(N-1)개의 인코더-디코더를 훈련해야 하는 Single NMT와 비교해, multi-way MNMT는 2N개의 인코더-디코더만 훈련하기만 하면 됩니다. 관련된 언어의 인코더와 디코더만 활용할 수 있어 추론 시간과 비용도 효과적으로 낮출 수 있습니다. 또한, 새로운 언어를 추가하고 싶다면 해당하는 인코더와 디코더 쌍만 따로 추가 훈련시키면 됩니다.&lt;/p&gt;

&lt;p&gt;하지만 오늘날에는 전체 매개변수 수 대비 더 나은 성능을 내는 1-1 MNMT&lt;a href=&quot;#fa91:rf:5&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:5&quot;&gt;[5]&lt;/a&gt;에 대한 연구가 주류로 자리하고 있습니다([그림 1-(2)]). 이 모델 아키텍처의 이름은 하나의 인코더(1)와 하나의 디코더(1)로만 구성됐다는 부분을 표현하고 있죠. 이처럼 매우 간단한 방법으로 다국어 번역 문제를 해결한 1-1 MNMT는 데이터가 상대적으로 적은 방향의 태스크&lt;sup&gt;low-resource environment&lt;/sup&gt;에서도 일정 수준 이상의 성능을 냅니다. 또한, 학습 데이터가 하나도 주어지지 않은 태스크인 제로샷&lt;sup&gt;zero-shot&lt;/sup&gt; 번역을 최초로 선보였습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;revisiting-modularized-multilingual-nmt-to-meet-industrial-demands-류성원-손보경-양기창-배재경1&quot;&gt;Revisiting modularized multilingual NMT to meet industrial demands (류성원, 손보경, 양기창, 배재경)&lt;a href=&quot;#fa91:rf:1&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:1&quot;&gt;[1]&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;1-1-mnmt가-지닌-한계&quot;&gt;1-1 MNMT가 지닌 한계&lt;/h2&gt;

&lt;p&gt;하지만 기업 환경에서는 1-1 MNMT를 사용하기가 어려운 현실적인 문제가 산재합니다&lt;a href=&quot;#fa91:rf:6&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:6&quot;&gt;[6]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;우선, 훈련해야 할 번역 방향이 늘어날수록 성능은 되려 저하되는 병목 현상&lt;sup&gt;capacity bottleneck&lt;/sup&gt;이 발생합니다. 이 문제는 1-1 모델 크기가 충분히 크지 않아서, 요구되는 성능 대비 수용 능력(모델 크기)이 충분히 크지 않아서 발생합니다. 그렇다고 모델 크기를 무한정 키울 수도 없습니다. 매개변수 수가 기하급수적으로 늘어난 만큼 계산 복잡성이 커져서 최적화&lt;sup&gt;&lt;a href=&quot;#fa91:fn:3&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;가 매우 어려워지기 때문입니다.&lt;/p&gt;

&lt;p&gt;대규모 1-1 MNMT로 모든 방향의 (성능 저하가 없는) 번역이 가능해지더라도, 실용화는 여전히 먼 나라의 일입니다. 기업 환경에서는 사용자가 요청한 하나의 질의&lt;sup&gt;query&lt;/sup&gt;를 처리하는 데 드는 계산량이 비용과 직결되는 만큼 추론에 사용하는 매개변수 수를 중시합니다. 그런데 딥러닝 특성상 특정 방향의 번역 태스크에 일부 매개변수만을 선택적으로 사용할 수 없다 보니 추론 비용이 굉장히 커지게 됩니다. 마찬가지 이유로 새로운 언어를 추가하거나 또는 기존 언어를 제거하기 위해 모델 전체를 재훈련하는 비용도 상당히 높죠.&lt;/p&gt;

&lt;p&gt;이에 많은 기업에서는 비용이 상당히 저렴한 pivot NMT 방식을 좀 더 선호합니다([그림 2]). ‘회전축의 중심’이라는 본래 의미에서 착안, 다국어 번역에서 피벗&lt;sup&gt;pivot&lt;/sup&gt;은 전세계적으로 가장 많이 쓰이는 언어인 영어를 매개로 하는 중간 번역을 의미합니다. 비영어→영어 NMT, 영어→비영어 NMT을 각각 N개만 훈련하면 되어서 비용을 많이 낮출 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-17-201217/002.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ 그림 2 ] 한국어→중국어 번역을 위한 pivot NMT 구조&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;하지만 두 단계에 걸쳐 번역이 이뤄지는 구조 상 추론에 더 많은 시간이 걸립니다. 피벗에 사용되는 두 NMT의 구조가 동일하다고 가정했을 때, 전체 추론 시간은 한 방향 번역에 걸리는 시간의 2배가 되는 셈이죠. 두 NMT 중 하나라도 성능이 낮으면 전체 번역 성능도 낮아지는 점 역시 구조에 기인합니다.&lt;/p&gt;

&lt;p&gt;이에 AI Lab은 1-1 MNMT와 pivot NMT, Single NMT의 단점을 극복한 실용적인 번역 모델을 새롭게 탐색하는 과정에서 multi-way MNMT가 지닌 가치를 재발견하는 연구를 진행하게 됐습니다. 그럼 이전에 제안된 서로 다른 아키텍처를 갖춘 번역 모델의 성능을 직접적으로 비교하는 실험 결과에 대해 상세히 설명해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;모델-구현&quot;&gt;모델 구현&lt;/h2&gt;

&lt;p&gt;본래 multi-way MNMT는 인코더와 디코더를 LSTM으로 구현했습니다. 하지만 LSTM과 같은 RNN 계열의 아키텍처에서는 문장이 길수록 계산 속도가 느려지고 거리가 먼 단어 간 관계를 제대로 표현하지 못하는 문제가 발생합니다. 이런 한계를 극복하고자 나온 최신 아키텍처가 바로 Transformer입니다. Transformer는 컨볼루션&lt;sup&gt;convolution&lt;/sup&gt;이나 순환&lt;sup&gt;recurrence&lt;/sup&gt; 기법을 사용하지 않고 어텐션&lt;sup&gt;attention&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;#fa91:fn:4&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;만으로도 거리가 먼 단어 간의 관계를 효과적으로 표현해 높은 추론 성능을 달성했습니다. AI Lab은 이런 특장점을 갖춘 Transformer로 구현해 훈련한 버전을 M2NMT&lt;a href=&quot;#fa91:rf:1&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:1&quot;&gt;[1]&lt;/a&gt;라 명명했습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;실험-결과&quot;&gt;실험 결과&lt;/h2&gt;

&lt;p&gt;AI Lab은 다국어 번역 태스크를 위한 세 방법론(Single NMT, 1-1 MNMT, M2MNT)의 감독학습&lt;sup&gt;supervised learning&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;#fa91:fn:5&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; 성능을 비교하는 실험을 진행했습니다. 그 결과([그림 3]), 1-1 MNMT는 성능 병목 현상으로 인해 Single NMT보다 더 낮은 성능을 냈습니다. 반면, M2NMT는 셋 중 가장 높은 성능을 선보였습니다. 한 매개변수가 여러 방향의 번역에 공유되는 구조로 인해 다국어 학습 시너지 효과&lt;sup&gt;data-diversification, and regularization effect&lt;/sup&gt;가 난 거로 분석됩니다. 한편, 앞서 설명한대로 1-1 MNMT의 크기를 키우면 성능 병목 현상이 크게 개선된다는 점도 확인할 수 있습니다([그림 4]).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-17-201217/003.png&quot; width=&quot;80%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ 그림 3 ] 12방향의 번역 태스크에서 Single NMT, 1-1 MNMT, M2NMT의 성능&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-17-201217/004.png&quot; width=&quot;80%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ 그림 4 ] 모델 크기가 상대적으로 더 클수록 성능 병목 현상이 완화됨을 확인할 수 있다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;두번째 실험에서 AI Lab은 4개의 언어 간 번역(En, De, Es, Ni 기준) 태스크를 익힌 M2(4)에, 아직 학습하지 않은 새로운 언어(Fr)를 담당하는 인코더와 디코더 쌍을 점진적으로 훈련해나갈 때의 제로샷 성능을 비교했습니다&lt;sup&gt;&lt;a href=&quot;#fa91:fn:6&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. 그 결과[ 표 1-(2) ], M2(4)는 제로샷  태스크에서 Single(5)보다 더 좋은 성능을 보였습니다. 아울러, M2(4)가 더 많은 *↔︎Fr 방향의 태스크를 배울수록(ID 1→2→3→4), M2(4)의 En↔︎Fr 간 번역 성능이 조금씩 늘어나며, 모든 번역 태스크를 추가 학습한 M2(4)의 성능이 M2(5)에 버금간다는 점을 볼 수 있습니다. pivot(4)와 비교해서도, M2NMT가 거의 모든 제로샷 태스크에서 pivot NMT 대비 더 좋은 성능(빨간색 글씨)을 달성했음을 ([표 1-(1)])에서 확인했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-17-201217/005.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ 표 1 ] (1) 제로샷에서 pivot NMT와 M2NMT 성능 비교 (2) 새로운 언어(Fr)에 대한 번역 태스크를 추가한 모델의 성능 비교&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;AI Lab은 M2NMT가 생성한 문장 벡터가 언어에 독립적인 공간&lt;sup&gt;interlingual space&lt;/sup&gt;에 사상된다고 가정했습니다. 그 구조상 각 인코더는 모든 디코더가 해석할 수 있는 정보를 제공해야 하며, 각 디코더는 모든 인코더가 제공한 정보를 해석할 수 있어야 하기 때문입니다. 학습을 마친 모델에 새로 추가된 인코더와 디코더가 약간의 훈련을 거치기만 해도 각각 언어에 독립적인 표현력과 해석력을 갖췄음을 보여준 두번째 실험은 이 가정을 실험적으로 증명했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-17-201217/006.png&quot; width=&quot;62%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ 그림 5 ] M2MNT에서 생성되는 언어에 독립적인 특징 공간&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;sparse-and-decorrelated-representations-for-stable-zero-shot-nmt손보경-류성원2&quot;&gt;Sparse and Decorrelated Representations for Stable Zero-shot NMT(손보경, 류성원)&lt;a href=&quot;#fa91:rf:2&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:2&quot;&gt;[2]&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;1-1-mnmt의-제로샷-성능이-낮은-이유&quot;&gt;1-1 MNMT의 제로샷 성능이 낮은 이유&lt;/h2&gt;

&lt;p&gt;MNMT 성능 향상의 핵심 전략이 두 언어의 문장으로 구성된 대량의 병렬 말뭉치 확보에 있다고 해도 과언이 아닙니다. 하지만 한 언어쌍에 대해, 유명한 벤치마크 중 비교적 작은 규모인 160,000개&lt;sup&gt;&lt;a href=&quot;#fa91:fn:7&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:7&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; 수준의 병렬 말뭉치를 구축하는 일조차 하늘에 별따기만큼 쉽지가 않습니다. 이런 이유로 제로샷에서도 충분한 성능을 내는 모델 아키텍처에 관한 연구가 이전보다 더 활발하게 이뤄지고 있습니다.&lt;/p&gt;

&lt;p&gt;서론에서는 다국어 학습으로 인한 시너지 효과를 충분히 기대할 수 있으며, 모델 규모 대비 기대 이상의 성능을 내는 1-1 MNMT를 주로 활용한다는 점을 밝힌 바 있습니다. 1-1 MNMT는 어떤 언어로 번역할지를 안내하는 메타 토큰(token)을 입력해 원하는 언어의 문장을 생성합니다.&lt;/p&gt;

&lt;p&gt;하지만 1-1 MNMT를 이용한 제로샷 번역에서는 비영어 문장을 입력하면 토큰 종류와 관계없이 항상 영어 문장을 생성하려는 경향이 커지는 문제가 생깁니다. 이처럼 특정 결과만을 도출하려는 현상을 학계에서는 ‘얽힘&lt;sup&gt;entanglement&lt;/sup&gt;’이라고 표현합니다. 전세계적으로 가장 많이 쓰이는 언어인 영어로 작성된 문장이 반드시 포함된 병렬 말뭉치&lt;sup&gt;English-centric data&lt;/sup&gt;가 원인으로 보이지만, 이는 아직 수학적으로 증명된 바 없습니다.&lt;/p&gt;

&lt;p&gt;배치 크기&lt;sup&gt;batch size&lt;/sup&gt;, 드롭아웃&lt;sup&gt;dropout&lt;/sup&gt;, 가중치 초기화&lt;sup&gt;weight Initialization&lt;/sup&gt;와 같은 초매개변수&lt;sup&gt;hyperparameter&lt;/sup&gt; 조정은 얽힘 문제 해결에 도움이 되지 않았습니다. 제로샷 번역의 성능은 초매개변수 값에 매우 민감하게 반응하기 때문입니다. 그렇다고 이 값을 고정해버리면 감독학습에서의 번역 성능이 되려 낮아질 위험도 높아집니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-1-mnmt의-구조적-한계를-극복하려는-시도&quot;&gt;1-1 MNMT의 구조적 한계를 극복하려는 시도&lt;/h2&gt;

&lt;p&gt;이에 빔 탐색&lt;sup&gt;beam search&lt;/sup&gt;으로 다음에 올 단어를 예측할 때, 목표로 하는 언어에 해당하는 어휘만 남겨두고 나머지는 걸러내&lt;a href=&quot;#fa91:rf:7&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:7&quot;&gt;[7]&lt;/a&gt; 얽힘 문제를 해결하려는 시도가 있었습니다. 빔의 모든 탐색 단계에서 의도하는 언어의 단어를 선택해야 그 다음에도 목표로 하는 언어의 단어가 생성된다는 아이디어로부터 착안됐습니다.&lt;/p&gt;

&lt;p&gt;또다른 시도는 역번역&lt;sup&gt;back translation&lt;/sup&gt;&lt;a href=&quot;#fa91:rf:8&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:8&quot;&gt;[8]&lt;/a&gt;&lt;sup&gt;&lt;a href=&quot;#fa91:fn:8&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:8&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;을 이용한 데이터 어그먼테이션&lt;sup&gt;augmentation&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;#fa91:fn:9&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:9&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;입니다. 시중에서 쉽게 구할 수 있는 비영어 단일 말뭉치를 모델에 입력해 생성한 (비영어, 비영어’) 병렬 말뭉치로 데이터 균형을 맞춰서 모델을 재훈련한다면, 영어 문장을 내뱉는 경향을 줄일 수 있다고 본 거죠. 다만, 모든 언어쌍에 대해 합성 데이터셋을 생성하는 데 드는 계산량이 큰 만큼 높은 비용이 발생합니다.&lt;/p&gt;

&lt;p&gt;최근에는 동일한 의미를 가진 영어 문장 벡터와 비영어 문장 벡터가 언어에 독립적인 벡터를 생성할 수 있도록 인코더를 정규화하는 기법&lt;sup&gt;regularization&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;#fa91:fn:10&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:10&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;을 다룬 연구가 제안됐습니다. 이는 동일한 의미를 나타내는 비영어 문장과 영어 문장을 표현하는 두 벡터를 서로 구분할 수 없을 정도로 유사하다면, 디코더가 비영어 문장을 영어로 번역하려는 판단을 쉽게 내리지 못하리라는 가정을 전제로 합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ai-lab의-제안-slni를-이용한-인코더-정규화&quot;&gt;AI Lab의 제안, ‘SLNI를 이용한 인코더 정규화’&lt;/h2&gt;

&lt;p&gt;지속 학습&lt;sup&gt;continual learning&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;#fa91:fn:11&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:11&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;에서는 새로운 태스크를 가르칠 때 기존 태스크에서 배운 지식을 잊어버리는 ‘파괴적 망각&lt;sup&gt;catastrophic forgetting&lt;/sup&gt;‘이라는 현상이 발생합니다. 기존 태스크에 이용된 뉴런이 활성화되며, 관련된 매개변수 값이 재조정되기 때문입니다.&lt;/p&gt;

&lt;p&gt;SLNI&lt;a href=&quot;#fa91:rf:9&quot; class=&quot;reference&quot; id=&quot;fa91:rf-back:9&quot;&gt;[9]&lt;/a&gt;&lt;sup&gt;Sparse coding through Local Neural Inhibition&lt;/sup&gt;은 이같은 문제를 해결하기 위한 정규화 기법입니다. 연구진은 활성 상태에 있는 뉴런이 인접한 다른 뉴런의 활성화를 억제하는 ‘측면 억제&lt;sup&gt;lateral inhibition&lt;/sup&gt;’에서 아이디어를 얻었습니다. 기존 태스크에서 활성화된 중요한 뉴런값을 바꾸는 대신, 활성화되지 않은 뉴런을 새로운 태스크에 사용한다면 망각을 줄일 수 있다고 본거죠.&lt;/p&gt;

&lt;p&gt;SLNI는 서로 무관하면서도 희소한 특징을 갖춘 벡터&lt;sup&gt;sparse and decorrelated representation&lt;/sup&gt;를 만들면 측면 억제 효과를 얻을 수 있다고 설명합니다. 이에 SLNI는 국소적으로 인접한 두 차원의 값에 점진적으로 가우시안&lt;sup&gt;gaussian&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;#fa91:fn:12&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:12&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; 패널티를 줍니다.&lt;/p&gt;

&lt;p&gt;AI Lab은 이 SLNI가 파괴적 망각 현상의 완화 외에도 활용처가 있으리라 판단, Transformer의 인코더를 구성하는 하위층&lt;sup&gt;sublayer&lt;/sup&gt;에 SLNI를 적용했습니다. 그 결과, AI Lab은 바로 위에서 언급한 특징을 갖춘 벡터를 생성하는 인코더가 훈련 조건의 변화에 강건한 제로샷 모델을 만드는 데 도움이 됐음을 발견했습니다. 다음 실험 결과 항목에서 이 내용을 보다 자세히 설명하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;실험-결과-1&quot;&gt;실험 결과&lt;/h2&gt;

&lt;p&gt;AI Lab은 번역 모델로 Transformer를, 훈련 데이터셋으로는 IWSLT2017를 활용했습니다. 이 데이터셋은 한쪽이 영어, 다른 한쪽은 독일어(De), 이탈리아어(It), 네덜란드어(NI), 로마니아어(Ro)로 구성된 병렬 말뭉치입니다. SLNI를 적용한 번역 모델이 다양한 훈련 조건에도 안정적으로 문장을 합성하는지를 살펴보고자 4가지 실험 환경을 설정해 모델의 성능을 비교했습니다([표 2]).&lt;/p&gt;

&lt;table style=&quot;border-collapse: collapse; width: 100%;&quot; border=&quot;1&quot;&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td style=&quot;width:20%; text-align:center; font-weight:bold&quot;&gt;Default&lt;/td&gt;
    &lt;td style=&quot;width:80%;&quot;&gt;최대 2400 토큰/번역 방향 당, 0.2 드롭아웃&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center; font-weight:bold&quot;&gt;AttDrop&lt;/td&gt;
    &lt;td&gt;attention dropout&lt;sup&gt;&lt;a href=&quot;#fa91:fn:13&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:13&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; * 0.1 + activation dropout&lt;sup&gt;&lt;a href=&quot;#fa91:fn:14&quot; class=&quot;footnote&quot; id=&quot;fa91:fn-back:14&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; * 0.1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center; font-weight:bold&quot;&gt;LargeBatch&lt;/td&gt;
    &lt;td&gt;최대 9600 토큰/번역 방향 당&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center; font-weight:bold&quot;&gt;Compound&lt;/td&gt;
    &lt;td&gt;AttDrop + LargeBatch&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;[ 표 2 ]1-1 MNMT 번역 모델의 인코더에 SLNI를 적용하면, 여러 실험 조건에서 안정적으로 제로샷 성능을 달성하는지를 확인하는 실험을 진행했다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;AI Lab은 SLNI를 적용한 번역 모델이 감독학습에서의 성능은 거의 그대로 유지하며, 특히 다양한 훈련 조건에서 안정적인 제로샷 성능을 확보할 수 있음을 확인했습니다. 이는 데이터 후처리&lt;sup&gt;post-processing&lt;/sup&gt;나 사전 훈련, 데이터 어그먼테이션 없이 간단한 조치만으로도 만들어낸 성과라 큰 의의가 있다고 평가해볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-17-201217/007.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ 그림 6 ] 감독학습(왼쪽)과 제로샷 태스크를 수행할 때 SLNI 기법의 효과를 나타내는 그래프&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;AI Lab은 1-1 MNMT의 인코더를 정규화하면 모든 언어의 문장 벡터가 사상된 공간의 형태가 서로 유사한지&lt;sup&gt;space similarity&lt;/sup&gt;, 동일한 의미를 가진 영어 문장 벡터와 비영어 문장 문장 벡터가 비슷한 표현을 갖추는지&lt;sup&gt;instance similarity&lt;/sup&gt;를 확인하는 실험도 진행했습니다. 만약 SLNI를 적용한 번역 모델이 위 두 유사성을 갖춘다면, SLNI를 적용하지 않은 버전(naive)와 비교해 더 높은 숫자를 갖춰야 합니다. 하지만 실험 결과, 두 버전 간 숫자간 격차가 크지 않음을 확인했습니다. 즉, 이 실험은 SLNI가 기존 인코더 정규화 기법을 다룬 연구에서 전제한 내용과는 다른 원리로 얽힘 문제를 해결했음을 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-17-201217/008.png&quot; width=&quot;90%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ 표 3 ] 1-1MNMT에서 인코더를 정규화하는 접근 방식이 언어에 독립적인 특징 벡터를 생성한다는 걸 전제로 하지 않음을 실험적으로 증명했다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;향후-계획&quot;&gt;향후 계획&lt;/h1&gt;

&lt;p&gt;AI Lab은 M2NMT가 생성한 언어에 독립적인 특징 공간의 여러 효용을 검증하는 연구는 물론, 제로샷 태스크에서 1-1 MNMT가 훈련 조건 변화에 취약한 원인과 새로 제안한 기법의 문제 해결 원리에 관한 새로운 탐색을 해나갈 계획입니다. 그 뿐만 아니라 도메인에 특화된 번역 모델과 지속 학습에서 안정적으로 성능을 내는 기법에 대한 연구도 진행할 계획입니다. 앞으로도 자사 기술력을 집약한 카카오 i 번역 엔진에 대한 많은 애정과 관심 부탁드립니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;참고-문헌&quot;&gt;참고 문헌&lt;/h1&gt;

&lt;p&gt;&lt;a id=&quot;fa91:rf:1&quot; class=&quot;referencebody&quot;&gt;&lt;a href=&quot;#fa91:rf-back:1&quot; class=&quot;backlink&quot;&gt;[1]&lt;/a&gt;  &lt;a href=&quot;https://www.aclweb.org/anthology/2020.emnlp-main.476/&quot;&gt;Revisiting Modularized Multilingual NMT to Meet Industrial Demands&lt;/a&gt; (2020) by Sungwon Lyu, Bokyung Son, Kichang Yang, Jaekyoung Bae in EMNLP &lt;/a&gt;&lt;br /&gt;
&lt;a id=&quot;fa91:rf:2&quot; class=&quot;referencebody&quot;&gt;&lt;a href=&quot;#fa91:rf-back:2&quot; class=&quot;backlink&quot;&gt;[2]&lt;/a&gt;  &lt;a href=&quot;https://www.aclweb.org/anthology/2020.findings-emnlp.205/&quot;&gt;Sparse and Decorrelated Representations for Stable Zero-shot NMT&lt;/a&gt; (2020) by Bokyung Son, Sungwon Lyu in EMNLP &lt;/a&gt;&lt;br /&gt;
&lt;a id=&quot;fa91:rf:3&quot; class=&quot;referencebody&quot;&gt;&lt;a href=&quot;#fa91:rf-back:3&quot; class=&quot;backlink&quot;&gt;[3]&lt;/a&gt;  &lt;a href=&quot;http://kiss.kstudy.com/public/public2-article.asp?key=50905527&quot;&gt;일상생활 속으로 들어온 기계 번역&lt;/a&gt; (2017) by 김준석 in 국립국어원 &lt;/a&gt;&lt;br /&gt;
&lt;a id=&quot;fa91:rf:4&quot; class=&quot;referencebody&quot;&gt;&lt;a href=&quot;#fa91:rf-back:4&quot; class=&quot;backlink&quot;&gt;[4]&lt;/a&gt;  &lt;a href=&quot;https://www.aclweb.org/anthology/N16-1101/&quot;&gt;Multi-way, multilingual neural machine translation with a shared attention mechanism&lt;/a&gt; (2016) by Orhan Firat, Kyunghyun Cho, Yoshua Bengio in ACL &lt;/a&gt;&lt;br /&gt;
&lt;a id=&quot;fa91:rf:5&quot; class=&quot;referencebody&quot;&gt;&lt;a href=&quot;#fa91:rf-back:5&quot; class=&quot;backlink&quot;&gt;[5]&lt;/a&gt;  &lt;a href=&quot;https://www.aclweb.org/anthology/Q17-1024/&quot;&gt;Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation Melvin Johnson&lt;/a&gt; (2017) by Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean in ACL &lt;/a&gt;&lt;br /&gt;
&lt;a id=&quot;fa91:rf:6&quot; class=&quot;referencebody&quot;&gt;&lt;a href=&quot;#fa91:rf-back:6&quot; class=&quot;backlink&quot;&gt;[6]&lt;/a&gt;  &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.148/&quot;&gt;Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation&lt;/a&gt; (2020) by Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich in ACL &lt;/a&gt;&lt;br /&gt;
&lt;a id=&quot;fa91:rf:7&quot; class=&quot;referencebody&quot;&gt;&lt;a href=&quot;#fa91:rf-back:7&quot; class=&quot;backlink&quot;&gt;[7]&lt;/a&gt;  &lt;a href=&quot;https://arxiv.org/abs/1711.07893&quot;&gt;Effective strategies in zero-shot neural machine translation&lt;/a&gt; (2017) by Thanh-Le Ha, Jan Niehues, Alexander Waibel in Arxiv &lt;/a&gt;&lt;br /&gt;
&lt;a id=&quot;fa91:rf:8&quot; class=&quot;referencebody&quot;&gt;&lt;a href=&quot;#fa91:rf-back:8&quot; class=&quot;backlink&quot;&gt;[8]&lt;/a&gt;  &lt;a href=&quot;https://www.aclweb.org/anthology/P19-1121/&quot;&gt;Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations&lt;/a&gt; (2019) by Jiatao Gu, Yong Wang, Kyunghyun Cho, Victor O.K. Li in ACL &lt;/a&gt;&lt;br /&gt;
&lt;a id=&quot;fa91:rf:9&quot; class=&quot;referencebody&quot;&gt;&lt;a href=&quot;#fa91:rf-back:9&quot; class=&quot;backlink&quot;&gt;[9]&lt;/a&gt;  &lt;a href=&quot;https://research.fb.com/publications/selfless-sequential-learning/&quot;&gt;Selfless Sequential Learning&lt;/a&gt; (2019) by Rahaf Aljundi, Marcus Rohrbach, Tinne Tuytelaars  in ICLR &lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;각주&quot;&gt;각주&lt;/h1&gt;

&lt;ol class=&quot;footnotelist&quot;&gt;
&lt;li id=&quot;fa91:fn:1&quot; class=&quot;footnotebody&quot; value=&quot;1&quot;&gt;&lt;p&gt; 먼저 단위정보 시퀀스를 인코더에 입력한다. 인코더는 이를 분석해 고정 길이의 벡터 표현&lt;sup&gt;vector representation&lt;/sup&gt;을 추정한다. 디코더는 이 벡터를 활용해 또 다른 단위정보의 시퀀스를 생성한다.&lt;a href=&quot;#fa91:fn-back:1&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:2&quot; class=&quot;footnotebody&quot; value=&quot;2&quot;&gt;&lt;p&gt; 이를 테면, 한국어→영어, 한국어→일본어 방향의 번역에서는 하나의 한국어 인코더를 사용한다. 반대로, 영어→한국어, 일본어→한국어 번역에서도 하나의 한국어 디코더만 사용한다.&lt;a href=&quot;#fa91:fn-back:2&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:3&quot; class=&quot;footnotebody&quot; value=&quot;3&quot;&gt;&lt;p&gt; 모든 학습 데이터에 대해 오차를 최소화하는 가중치 값을 찾은 상태&lt;a href=&quot;#fa91:fn-back:3&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:4&quot; class=&quot;footnotebody&quot; value=&quot;4&quot;&gt;&lt;p&gt; 기계 번역을 위한 seq2seq 구조의 모델의 RNN 셀에서 특정 시퀀스를 디코딩할 때 관련 인코딩 결과값을 참조할 수 있게 하는 보조적인 역할을 하는 데 처음 사용됐다. 이후 구글이 Transformer를 통해 어텐션만으로도 문장을 모델링하는 셀프어텐션&lt;sup&gt;self-attention&lt;/sup&gt; 기법을 선보였다.&lt;a href=&quot;#fa91:fn-back:4&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:5&quot; class=&quot;footnotebody&quot; value=&quot;5&quot;&gt;&lt;p&gt; 정답&lt;sup&gt;label&lt;/sup&gt;이 주어진 데이터셋으로 훈련하는 방식&lt;a href=&quot;#fa91:fn-back:5&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:6&quot; class=&quot;footnotebody&quot; value=&quot;6&quot;&gt;&lt;p&gt; 새로운 언어가 포함된 방향의 번역 태스크의 훈련은 기존 언어별 인코더와 디코더의 가중치를 고정한 상태에서 진행한다.&lt;a href=&quot;#fa91:fn-back:6&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:7&quot; class=&quot;footnotebody&quot; value=&quot;7&quot;&gt;&lt;p&gt; IWSLT14(En-De)&lt;a href=&quot;#fa91:fn-back:7&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:8&quot; class=&quot;footnotebody&quot; value=&quot;8&quot;&gt;&lt;p&gt; 기계가 생성한 문장을 다시 학습에 사용한다는 특성 때문에 역번역이라는 이름이 붙여진 거로 보인다.&lt;a href=&quot;#fa91:fn-back:8&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:9&quot; class=&quot;footnotebody&quot; value=&quot;9&quot;&gt;&lt;p&gt; 이미지를 좌우로 뒤집거나(flipping) 자르는(cropping) 등 데이터에 인위적인 변화를 가하는 방법론&lt;a href=&quot;#fa91:fn-back:9&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:10&quot; class=&quot;footnotebody&quot; value=&quot;10&quot;&gt;&lt;p&gt; 설명력이 높으면서도 그 구조나 표현이 간단한 상태를 이르는 말&lt;a href=&quot;#fa91:fn-back:10&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:11&quot; class=&quot;footnotebody&quot; value=&quot;11&quot;&gt;&lt;p&gt; 학습 중간에 또는 기존 주어진 데이터셋의 학습이 끝나고 난 상황에서 새로운 데이터나 새로운 범주의 데이터가 입력해 모델을 업데이트하는 학습 방식&lt;a href=&quot;#fa91:fn-back:11&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:12&quot; class=&quot;footnotebody&quot; value=&quot;12&quot;&gt;&lt;p&gt; 평균을 중심으로 좌우 대칭의 종모양을 갖는 확률분포를 가리킨다. 여기서는 이 가우시안 분포 함수에 근사해 생성한 패널티 함수를 가리킨다.&lt;a href=&quot;#fa91:fn-back:12&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:13&quot; class=&quot;footnotebody&quot; value=&quot;13&quot;&gt;&lt;p&gt; multihead attention에 주는 dropout&lt;a href=&quot;#fa91:fn-back:13&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fa91:fn:14&quot; class=&quot;footnotebody&quot; value=&quot;14&quot;&gt;&lt;p&gt; 2-layer feedforward 중간에 있는 relu activation 뒤의 dropout&lt;a href=&quot;#fa91:fn-back:14&quot; class=&quot;backlink&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>samantha:작성,편집</name></author><category term="deepdive" /><category term="translation" /><category term="MNMT" /><category term="EMNLP" /><summary type="html">카카오엔터프라이즈 AI Lab(이하 AI Lab)이 낸 3편의 논문이 EMNLPEmpirical Methods in Natural Language Processing에 게재 승인됐습니다. 자연어처리에서 경험적 방법론을 다루는 이 학회는 ACLAssociation for Computational Linguistics, NAACLNORTH American Chapter of the ACL과 함께 전산언어학 분야에서는 인지도가 높습니다. 올해에는 총 3,677편의 논문 중 754편이 통과됐습니다.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/000.jpg" /><media:content medium="image" url="http://0.0.0.0:4000/000.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search</title><link href="http://0.0.0.0:4000/papers/neurips2020-glow-tts" rel="alternate" type="text/html" title="Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search" /><published>2020-12-15T00:00:00-06:00</published><updated>2020-12-15T00:00:00-06:00</updated><id>http://0.0.0.0:4000/papers/neurips2020-glow-tts</id><content type="html" xml:base="http://0.0.0.0:4000/papers/neurips2020-glow-tts">&lt;p&gt;FastSpeech와 ParaNet 같은 최신 TTS(음성합성) 모델은 발화를 병렬적으로 합성&lt;sup&gt;non-Autoregressive&lt;/sup&gt;해 그 속도를 높인  새로운 보코더&lt;sup&gt;vocoder&lt;/sup&gt;를 제안했습니다. 하지만 이런 병렬 모델이 텍스트를 구성하는 음소 순서대로 오디오를 정렬하기 위해서는 따로 훈련된 Autoregressive 모델의 지원이 반드시 필요합니다.&lt;/p&gt;

&lt;p&gt;이에 카카오엔터프라이즈는 정렬 모델을 따로 구축하지 않고도 이를 보다 정확하게 추정하는 새로운 TTS 모델인 Glow-TTS를 제안했습니다. Glow-TTS는 플로우 기반 생성 모델&lt;sup&gt;flow-based generative models&lt;/sup&gt;과 동적 프로그래밍&lt;sup&gt;dynamic programming&lt;/sup&gt;의 속성을 활용해 입력된 텍스트 순서에 따라 발화를 차례대로 정렬&lt;sup&gt;monotonic alignment&lt;/sup&gt;합니다.&lt;/p&gt;

&lt;p&gt;그 결과, 매우 긴 텍스트로 빠르게 합성함은 물론, 서로 다른 강세와 억양을 갖춘 발화를 생성할 수 있습니다. 자체 실험에서 Glow-TTS는 autoregressive 모델인 Tacotron 2과 비교해 비슷한 품질의 음성을 약 15배 더 빠르게 생성했습니다. 또한 Glow-TTS가 다화자 음성합성 태스크에도 쉽게 적용될 수 있음을 확인했습니다.&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈는 이번 연구로 얻은 기술력과 경험을 바탕으로 E2E&lt;sup&gt;end-to-end&lt;/sup&gt; TTS 기술을 고도화할 계획입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overall-architecture&quot;&gt;Overall Architecture&lt;/h1&gt;

&lt;p&gt;Inspired by the fact that a human reads out text in order, without skipping any words, we design Glow-TTS to generate a mel-spectrogram conditioned on a monotonic and non-skipping alignment between text and speech representations.&lt;/p&gt;

&lt;p&gt;By combining the properties of flows and dynamic programming, Glow-TTS efficiently searches for the most probable monotonic alignment between text and the latent representation of speech. The proposed model is directly trained to maximize the log-likelihood of speech with the alignment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-15-NeurIPS-Glow-TTS/001.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ Figure 1 ] Training and inference procedures of Glow-TTS&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;We vary the standard deviation (i.e., temperature T ) of the prior distribution at inference; Glow-TTS shows the best performance at the temperature of 0.333. For any temperature, it shows comparable performance to Tacotron 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-15-NeurIPS-Glow-TTS/002.png&quot; width=&quot;60%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Table 1 ] The Mean Opinion Score (MOS) of single speaker TTS models with 95% confidence intervals.
On average, Glow-TTS shows a 15.7 times faster synthesis speed than Tacotron 2. The CER(character error rate) of Tacotron 2 starts to grow when the length of input characters exceeds about 260. On the other hand, even though our model has not seen such long texts during training, it shows robustness to long texts.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-15-NeurIPS-Glow-TTS/003.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ Figure 2 ] Comparison of inference time and length robustness&lt;/em&gt;&lt;/p&gt;</content><author><name>jay:카카오엔터프라이즈</name></author><category term="papers" /><category term="TTS" /><summary type="html">FastSpeech와 ParaNet 같은 최신 TTS(음성합성) 모델은 발화를 병렬적으로 합성non-Autoregressive해 그 속도를 높인 새로운 보코더vocoder를 제안했습니다. 하지만 이런 병렬 모델이 텍스트를 구성하는 음소 순서대로 오디오를 정렬하기 위해서는 따로 훈련된 Autoregressive 모델의 지원이 반드시 필요합니다.</summary></entry><entry><title type="html">HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</title><link href="http://0.0.0.0:4000/papers/neurips2020-hifi-gan" rel="alternate" type="text/html" title="HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis" /><published>2020-12-15T00:00:00-06:00</published><updated>2020-12-15T00:00:00-06:00</updated><id>http://0.0.0.0:4000/papers/neurips2020-hifi-gan</id><content type="html" xml:base="http://0.0.0.0:4000/papers/neurips2020-hifi-gan">&lt;p&gt;최근 음성합성 연구에서는 GAN&lt;sup&gt;generative adversarial networks&lt;/sup&gt; 구조를 활용해 보코더&lt;sup&gt;vocoder&lt;/sup&gt;의 음성 합성 속도와 메모리 효율을 높이는 시도가 있었습니다. 하지만 이런 방식의 보코더가 합성한 음성의 품질은 Autoregressive 모델이나 플로우 기반의 생성 모델&lt;sup&gt;flow-based generative model&lt;/sup&gt;에 미치지 못했습니다.&lt;/p&gt;

&lt;p&gt;이에 카카오엔터프라이즈는 음성 오디오의 주기적 신호를 구별해내는 방식으로 기존 제안된 모델보다 월등히 좋은 품질의 오디오를 빠르게 합성하는 HiFi-GAN을 제안했습니다.&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈는 메모리 효율성 및 속도와 관련된 초매개변수&lt;sup&gt;hyperparameter&lt;/sup&gt;를 조정하고, 이 값을 조합한 3가지 버전의 세부 모델로 실험을 진행했습니다. 첫 번째 버전은 인간과 비슷한 수준의 고품질 오디오를 실시간의 167.9배 속도로 합성합니다. 두번째 버전은 비교 모델 중 가장 적은 매개변수를 사용하면서도 가장 좋은 음질을 생성합니다. 세 번째 버전은 GPU에서 실시간의 1,186.8배, CPU에서 13.4배 더 빠르게 기존 모델과 비슷한 품질의 오디오를 합성합니다.&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈는 이번 연구로 얻은 기술력과 경험을 바탕으로 음질 개선, 맥락에 따라 다양한 스타일의 발화 생성 등 다양한 태스크를 수행하는 오디오 합성 기술을 연구할 계획입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Several recent studies on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this study, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real time on CPU with comparable quality to an autoregressive counterpart.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overall-architecture&quot;&gt;Overall Architecture&lt;/h1&gt;

&lt;p&gt;HiFi-GAN consists of one generator and two discriminators: multi-scale and multi-period discrimina- tors. The generator and discriminators are trained adversarially, along with two additional losses for improving training stability and model performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-15-NeurIPS-HiFi-GAN/001.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Figure 1 ] The generator upsamples mel-spectrograms up to \(\vert k_{u} \vert\) times to match the temporal resolution of raw waveforms. A MRF module adds features from \(\vert k_{r} \vert\) residual blocks of different kernel sizes and dilation rates. Lastly, the \(n\)-th residual block with kernel size \(k_{r}[n]\) and dilation rates \(D_{r}[n]\) in a MRF module is depicted.
&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-15-NeurIPS-HiFi-GAN/002.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Figure 2 ] (a) The second sub-discriminator of the MSD. (b) The second sub-discriminator of the MPD with period 3.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;To evaluate the performance of our models in terms of both quality and speed, we performed the MOS test and the speed measurement. For easy comparison of audio quality, synthesis speed and model size, the results are compiled and presented in Table1. Remarkably, all variations of HiFi-GAN scored higher than the other model. Moreover, it shows a significant improvement in terms of synthesis speed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-15-NeurIPS-HiFi-GAN/003.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Table 1 ] Comparison of the MOS and the synthesis speed. Speed of n kHz means that the model can generate n × 1000 raw audio samples per second. The numbers in () mean the speed compared to real time.&lt;/em&gt;&lt;/p&gt;</content><author><name>henry:카카오엔터프라이즈</name></author><category term="papers" /><category term="speech synthesis" /><summary type="html">최근 음성합성 연구에서는 GANgenerative adversarial networks 구조를 활용해 보코더vocoder의 음성 합성 속도와 메모리 효율을 높이는 시도가 있었습니다. 하지만 이런 방식의 보코더가 합성한 음성의 품질은 Autoregressive 모델이나 플로우 기반의 생성 모델flow-based generative model에 미치지 못했습니다.</summary></entry><entry><title type="html">Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer</title><link href="http://0.0.0.0:4000/papers/inlg2020-stable-style-transformer" rel="alternate" type="text/html" title="Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer" /><published>2020-12-15T00:00:00-06:00</published><updated>2020-12-15T00:00:00-06:00</updated><id>http://0.0.0.0:4000/papers/inlg2020-stable-style-transformer</id><content type="html" xml:base="http://0.0.0.0:4000/papers/inlg2020-stable-style-transformer">&lt;p&gt;카카오엔터프라이즈는 비병렬 데이터셋을 이용해 자연스러운 문장을 생성하는 새로운 텍스트 스타일 변환&lt;sup&gt;text style transfer&lt;/sup&gt; 모델인 SST&lt;sup&gt;Stable Style Transformer&lt;/sup&gt;를 제안했습니다. 텍스트 스타일 변환은 입력 문장의 내용&lt;sup&gt;content&lt;/sup&gt;은 보전하면서, 문장의 속성&lt;sup&gt;attribute&lt;/sup&gt;에 해당하는 값을 바꾸는 태스크를 가리킵니다.&lt;/p&gt;

&lt;p&gt;SST의 스타일 변환은 두 단계를 걸쳐 진행됩니다. 첫 번째, 분류기를 통해 문장에서 속성을 표현하는 부분을 가리키는 토큰을 삭제합니다. 두 번째, 인코더와 디코더로 속성을 제외한 나머지 문장 토큰에 변환하려는 스타일을 합쳐서 문장을 생성합니다. 그 결과, 자동화된 평가 척도에서 문장이 안정적으로 합성됨을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈는 텍스트 스타일 변환뿐만 아니라 사람의 감정이나 대화 상황을 이해하는 공감적 대화&lt;sup&gt;empathetic conversation&lt;/sup&gt;, 데이터를 문장으로 표현하기&lt;sup&gt;data-to-text&lt;/sup&gt;, 페르소나 채팅&lt;sup&gt;persona chat&lt;/sup&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;과 같이 원하는 조건에 따라 문장을 생성하는 자연어 생성에 관한 다양한 연구를 진행할 계획입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Text style transfer is the task that generates a sentence by preserving the content of the input sentence and transferring the style. Most existing studies are progressing on non-parallel datasets because parallel datasets are limited and hard to construct. In this work, we introduce a method that follows two stages in non-parallel datasets. The first stage is to delete attribute markers of a sentence directly through a classifier. The second stage is to generate a transferred sentence by combining the content tokens and the target style. We experiment on two benchmark datasets and evaluate context, style, fluency, and semantic. It is difficult to select the best system using only these automatic metrics, but it is possible to select stable systems. We consider only robust systems in all automatic evaluation metrics to be the minimum conditions that can be used in real applications. Many previous systems are difficult to use in certain situations because performance is significantly lower in several evaluation metrics. However, our system is stable in all automatic evaluation metrics and has results comparable to other models. Also, we compare the performance results of our system and the unstable system through human evaluation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overall-architecture&quot;&gt;Overall Architecture&lt;/h1&gt;

&lt;p&gt;Our approach consists of two stages: Delete and Generate framework in Fig. 1. The first stage is the Delete process with a pre-trained style classifier. The pre-trained style classifier finds and deletes tokens that contain a lot of style attributes. The second stage is encoding the content tokens and combining them with a target style to generate a sentence. Both the encoder and the decoder have the Transformer structure, which is better than RNN and robust to long dependency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-15-INLG-Stable-Style-Transformer/001.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ Figure 1 ] The proposed model framework consists of the Delete and Generate process.
Experiments&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We test our model on two datasets, YELP and AMAZON. The Yelp dataset is for business reviews, and the Amazon dataset is product reviews. Both datasets are labeled negative and positive and statistics are shown in Table 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-15-INLG-Stable-Style-Transformer/002.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;[ Table 1 ] (Sentiment) Dataset statistics&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We show that filtering out unstable systems through human evaluation is expensive, so selecting a stable system through automatic evaluation can be helpful. The proposed direct and model-agnostic deletion method allows the classifier to intuitively delete attribute markers and easily handle the trade-off of content and style.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-12-15-INLG-Stable-Style-Transformer/003.png&quot; width=&quot;&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[ Table 2 ] Automatic evaluation results of the Yelp dataset (s: self, h: human, G: geometric mean, f: fine-tuned, p: pre-trained). The red indicates that the evaluation score is significantly worse than other systems. Our model is referred to as SST(α, β). The bold black indicates the better performance of our systems for the four metrics that determine it is a stable system.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;footnote&quot;&gt;footnote&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;시스템 설계자가 미리 정의한 고유 페르소나(인격)를 가진 가상의 상대와 대화를 나누는 태스크 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>rung:카카오엔터프라이즈</name></author><category term="papers" /><category term="Style Transfer" /><summary type="html">카카오엔터프라이즈는 비병렬 데이터셋을 이용해 자연스러운 문장을 생성하는 새로운 텍스트 스타일 변환text style transfer 모델인 SSTStable Style Transformer를 제안했습니다. 텍스트 스타일 변환은 입력 문장의 내용content은 보전하면서, 문장의 속성attribute에 해당하는 값을 바꾸는 태스크를 가리킵니다.</summary></entry></feed>