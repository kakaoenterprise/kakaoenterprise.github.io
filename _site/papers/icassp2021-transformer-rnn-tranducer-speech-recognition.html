<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Multitask Learning and Joint Optimization For Transformer-Rnn-Tranducer Speech Recognition | Kakao Enterprise AI Research</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Multitask Learning and Joint Optimization For Transformer-Rnn-Tranducer Speech Recognition" />
<meta name="author" content="jeffrey:카카오엔터프라이즈" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Abstract" />
<meta property="og:description" content="Abstract" />
<link rel="canonical" href="http://0.0.0.0:4000/papers/icassp2021-transformer-rnn-tranducer-speech-recognition" />
<meta property="og:url" content="http://0.0.0.0:4000/papers/icassp2021-transformer-rnn-tranducer-speech-recognition" />
<meta property="og:site_name" content="Kakao Enterprise AI Research" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-13T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Multitask Learning and Joint Optimization For Transformer-Rnn-Tranducer Speech Recognition" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"jeffrey:카카오엔터프라이즈"},"headline":"Multitask Learning and Joint Optimization For Transformer-Rnn-Tranducer Speech Recognition","dateModified":"2021-06-13T00:00:00-05:00","description":"Abstract","datePublished":"2021-06-13T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/papers/icassp2021-transformer-rnn-tranducer-speech-recognition"},"url":"http://0.0.0.0:4000/papers/icassp2021-transformer-rnn-tranducer-speech-recognition","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
  <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nanum+Gothic:400,700,800&amp;subset=korean">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">

  <!-- Icon -->
  <script src="https://kit.fontawesome.com/29661d1774.js" crossorigin="anonymous"></script>

  <!-- RSS -->
  <link type="application/atom+xml" rel="alternate" href="http://0.0.0.0:4000/feed.xml" title="Kakao Enterprise AI Research" />

  <!-- Google Analytics-->
  

  <!-- katex -->
  
</head>

  <body>
    <nav class="nav">
  <div class="nav-container">
    <a href="/">
      <h2 class="nav-title">Kakao Enterprise AI Research</h2>
    </a>
    <ul>
      <li><a href="/papers">Papers</a></li>
    </ul>
  </div>
</nav>

    <main>
      <article class="post">
  <header class="post-header">
    <h4 class="catalogue-research-area">SPEECH/AUDIO</h4>
    <h1 class="post-title">Multitask Learning and Joint Optimization For Transformer-Rnn-Tranducer Speech Recognition</h1>

    

    

    전재진(카카오엔터프라이즈), 김의성(카카오엔터프라이즈)

    <h4>
        
          International Conference on Acoustics, Speech, and Signal Processing (ICASSP)
        
    </h4>

    <h4>2021-06-13</h4>

    
  </header>

  <div class="post-line"></div>

  <div class="post-body">
    <h1 id="abstract">Abstract</h1>

<p>Recently, several types of end-to-end speech recognition methods named transformer-transducer were introduced. According to those kinds of methods, transcription networks are generally modeled by transformer-based neural networks, while prediction networks could be modeled by either transformers or recurrent neural networks (RNN). This paper explores multitask learning, joint optimization, and joint decoding methods for transformer-RNN-transducer systems. Our proposed methods have the main advantage in that the model can maintain information on the large text corpus. We prove their effectiveness by performing experiments utilizing the well-known ESPNET toolkit for the widely used Librispeech datasets. We also show that the proposed meth- ods can reduce word error rate (WER) by 16.6 % and 13.3 % for test-clean and test-other datasets, respectively, with- out changing the overall model structure nor exploiting an external LM.</p>

  </div>

  <div class="post-line"></div>

  <div class="post-tag-box-container">
    
  </div>
</article>
<div class="pagination">
    <a onclick="window.history.back()" class="left arrow" style="cursor: pointer;">&#8592; 목록으로</a>
</div>

    </main>
    <footer>
  <a class="footer-link" href="https://github.com/kakaoenterprise" target="_blank">
    <img src="/assets/GitHub-Mark.png" alt="Kakao Enterprise GitHub" />GitHub</a>
  <br/>
  <a class="footer-copyright">Copyright © Kakao Enterprise All rights reserved.</a>

</footer>

  </body>
</html>
