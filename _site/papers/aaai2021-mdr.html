<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Multi-level Distance Regularization for Deep Metric Learning | Kakao Enterprise AI Research</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Multi-level Distance Regularization for Deep Metric Learning" />
<meta name="author" content="aiden:카카오엔터프라이즈" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="딥러닝 기반 거리 학습에 적합한 새로운 정규화 기법 ‘MDR’ 제안" />
<meta property="og:description" content="딥러닝 기반 거리 학습에 적합한 새로운 정규화 기법 ‘MDR’ 제안" />
<link rel="canonical" href="https://pages.github.kakaocorp.com/papers/aaai2021-mdr" />
<meta property="og:url" content="https://pages.github.kakaocorp.com/papers/aaai2021-mdr" />
<meta property="og:site_name" content="Kakao Enterprise AI Research" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-02T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Multi-level Distance Regularization for Deep Metric Learning" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"aiden:카카오엔터프라이즈"},"description":"딥러닝 기반 거리 학습에 적합한 새로운 정규화 기법 ‘MDR’ 제안","@type":"BlogPosting","headline":"Multi-level Distance Regularization for Deep Metric Learning","dateModified":"2021-02-02T00:00:00-06:00","datePublished":"2021-02-02T00:00:00-06:00","url":"https://pages.github.kakaocorp.com/papers/aaai2021-mdr","mainEntityOfPage":{"@type":"WebPage","@id":"https://pages.github.kakaocorp.com/papers/aaai2021-mdr"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
  <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nanum+Gothic:400,700,800&amp;subset=korean">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">

  <!-- Icon -->
  <script src="https://kit.fontawesome.com/29661d1774.js" crossorigin="anonymous"></script>

  <!-- RSS -->
  <link type="application/atom+xml" rel="alternate" href="https://pages.github.kakaocorp.com/feed.xml" title="Kakao Enterprise AI Research" />

  <!-- Google Analytics-->
  

  <!-- katex -->
  
</head>

  <body>
    <nav class="nav">
  <div class="nav-container">
    <a href="/">
      <h2 class="nav-title">Kakao Enterprise AI Research</h2>
    </a>
    <ul>
      <li><a href="/papers">Papers</a></li>
    </ul>
  </div>
</nav>

    <main>
      <article class="post">
  <header class="post-header">
    <h4 class="catalogue-research-area">COMPUTER VISION</h4>
    <h1 class="post-title">Multi-level Distance Regularization for Deep Metric Learning</h1>

    

    

    김용현(카카오엔터프라이즈), 박원표(카카오)

    <h4>
        
          Association for Advancement of AI (AAAI)
        
    </h4>

    <h4>2021-02-02</h4>

    
      <div class="link-button-group">
        
          <a href="https://arxiv.org/pdf/2102.04223.pdf" target="_blank">
            <button class="link-button">
              <i class="far fa-file-alt"></i> Paper
            </button>
          </a>
        

        
          <a href="https://github.com/kakaoenterprise/AAAI2021_MDR" target="_blank">
            <button class="link-button">
              <i class="fab fa-github"></i> Code
            </button>
          </a>
        

        
      </div>
    
  </header>

  <div class="post-line"></div>

  <div class="post-body">
    <p>딥러닝 기반 거리 학습(DML<sup>Deep Metric Learning</sup>)은 주어진 두 데이터 간 의미적인 거리<sup>pairwise distance</sup>를 학습하는 방법입니다. 특히 입력 이미지와 유사한 이미지를 찾는 데 효과적인 이 기술은 이미지 검색<sup>image retrieval</sup>, 상품 추천, 얼굴 인식<sup>face recognition</sup>과 같은 태스크에서 핵심적으로 활용되고 있습니다.</p>

<p>선행 연구에서는 같은 범주의 이미지 간 거리는 더 가깝게, 범주가 다른 이미지의 거리는 더 멀게 만드는 손실 함수<sup>loss function</sup> 재정의에 집중했습니다. 다만 거리가 아닌 임베딩 벡터<sup>embedding vector</sup>를 제약하는 L2 정규화<sup>L2 normalization</sup><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>로는 직접적인 거리 정규화가 어려워서 모델이 쉽게 과적합<sup>overfitting</sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>되는 문제가 있었습니다.</p>

<p>이에 공동 연구팀은 DML에 더 적합한 새로운 거리 정규화 기법인 MDR<sup>Multi-level Distance Regularization</sup>을 제안했습니다. MDR은 데이터 간의 거리 분포를 여러 부분으로 나눈 뒤 각 부분을 대표하는 레벨<sup>level</sup>을 학습하고, 각 레벨에 속하는 두 쌍의 데이터 간 거리를 정규화합니다. MDR는 기존 손실 함수와 함께(jointly) 동작하며 쉬운 샘플<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>이 학습에 영향을 주지 못하거나 어려운 샘플<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>이 학습을 지배하는 현상을 방지합니다.</p>

<p>트리플릿 손실 함수<sup>Triplet loss function</sup>에 MDR을 적용해본 결과, 주요 벤치마크 데이터셋에서 유의미한 성능 향상을 확인할 수 있었습니다.</p>

<p>카카오엔터프라이즈는 이번 연구로 얻은 기술력과 경험을 바탕으로 상품 추천, 얼굴 인식 등 관련 기술을 고도화할 계획입니다.</p>

<p><br /></p>

<p class="tech-ground">☛ Tech Ground 데모 페이지 바로 가기 : <b><a href="https://labs.kakaoi.ai/facedetection">얼굴 검출</a></b> 데모</p>

<p><br /></p>

<h1 id="overall-architecture">Overall Architecture</h1>

<p>At first, MDR normalizes pairwise distances among the embedding vectors of a mini-batch, with their mean and standard deviation to obtain the objective degree of similarity between a pair by considering overall distribution. MDR defines the multiple levels that represent various degrees of similarity for pairwise distances, and the levels and the be- longing distances are trained to approach each other (Figure 1b). A conventional loss function of DML struggles to optimize a model by overcoming the disturbance from the proposed regularization. Therefore, the learning process succeeds in learning a model with a better generalization ability.</p>

<p><img src="/assets/img/2021-02-02-AAAI-mdr/001.png" width="" align="" /></p>

<p><em>[ Figure 1 ] Conceptual comparison between the conventional learning scheme and our learning scheme. (a) illustrates the triplet learning, which is one of the representative conventional learning. It increases the relative difference between distances of a positive pair and that of a negative pair more than margin m. (b) illustrates our learning combined with the triplet learning. It has multiple levels with disjoint intervals to reflect various degrees of similarity between pairs. It disturbs the learning procedure to construct an efficient embedding space by preventing the pairwise distances from deviating from its belonging level.</em></p>

<p><img src="/assets/img/2021-02-02-AAAI-mdr/002.png" width="" align="" /></p>

<p><em>[ Figure 2 ] Learning procedure of the proposed MDR. The embedding network generates embedding vectors from given images. Our MDR computes a matrix of pairwise distances for the embedding vectors, and then, the distances are normalized after vectorization. In our learning scheme, a model is trained by simultaneously optimizing the conventional metric learning loss such as Triplet loss (Schroff, Kalenichenko, and Philbin 2015) and the proposed loss, which regularizes the normalized pairwise distances with multiple levels.</em></p>

<p><br /></p>

<h1 id="experiments">Experiments</h1>

<p>We employ the four standard datasets of deep metric learning for evaluations: CUB-200-2011 (CUB-200), Cars-196, Stanford Online Product (SOP) and In-Shop Clothes Retrieval (In-Shop). Our method significantly outperforms the other state-of-the-art methods in all recall criteria for all datasets.</p>

<p>We prove the effectiveness of MDR by showing the improvements that greatly exceed the existing methods, and by extensively performing the ablation studies of its behaviors. By applying our MDR, many methods can be significantly improved without any extra burdens at inference time.</p>

<p>Moreover, our extensive ablation studies show that MDR can be adopted to any backbone networks and any distance-based loss functions to improve the performance of a model.</p>

<p><img src="/assets/img/2021-02-02-AAAI-mdr/003.png" width="" align="" /></p>

<p><img src="/assets/img/2021-02-02-AAAI-mdr/004.png" width="" align="" /></p>

<p><em>[ Table 1 ] Recall@K comparison with state-of-the-art methods. The baseline methods and MDR are grouped in the gray-colored rows. † indicates that the model is trained and tested with large images of 256 × 256 following the setting of (Jacob et al. 2019). We round reported values to the first decimal place.</em></p>

<p><img src="/assets/img/2021-02-02-AAAI-mdr/005.png" width="85%" align="" /></p>

<p><em>[ Figure 3 ] Class centers in the embedding space of two models trained without MDR (Triplet &amp; Triplet+L2 Norm) and one model trained with MDR (Triplet+MDR). We visualize using t-SNE on CUB-200.</em></p>

<p><img src="/assets/img/2021-02-02-AAAI-mdr/006.png" width="70%" align="" /></p>

<p><em>[ Table 2 ] Recall@1 comparison with various backbone net- works, loss functions, and level configurations. The models of (a) are trained with Triplet loss. The models of (b) use IBN as the backbone network. In (a) and (b), a column with ✓ indicates that the models are trained with MDR.</em> </p>

<p><img src="/assets/img/2021-02-02-AAAI-mdr/007.png" width="50%" align="" /></p>

<p><img src="/assets/img/2021-02-02-AAAI-mdr/008.png" width="55%" align="" /></p>

<p><em>[ Figure 4 ] (a) compares the three methods on various dimensionalities of the embedding vector on CUB-200 and Cars- 196. (b) shows the learning curves of the three methods for the training and test set on CUB-200.</em></p>

<p><br /></p>

<h3 id="footnote">footnote</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>정규화는 모델의 설명력을 높이면서도 복잡도를 줄이는 기법을 가리킨다. 정규화 방법론 중 하나인 L2 정규화는 학습 데이터에 따라 특정 가중치의 값이 지나치게 커지는 걸 방지하도록 한다. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>훈련 데이터에 대한 손실(함수)값이 감소한다고 해서 학습하지 않은 새로운 데이터에서도 같은 정확도를 낸다는 보장은 없다. 이처럼 훈련 데이터에 대해서만 좋은 결과를 내는 현상을 과적합이라고 표현한다. 훈련 데이터에 포함된 노이즈<sup>noise</sup>마저 버린 상태로, 훈련 데이터가 적을수록 과적합 정도는 심해진다. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>손실값이 작아서 실제 가중치 업데이트에 미비한 영향을 미치는 샘플 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>손실값이 커서 가중치 업데이트에서 지배적인 영향을 미치는 샘플 <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  <div class="post-line"></div>

  <div class="post-tag-box-container">
    
      <div class="post-tag-box">#deep metric learning</div>
    
      <div class="post-tag-box">#regularization</div>
    
  </div>
</article>
<div class="pagination">
    <a onclick="window.history.back()" class="left arrow" style="cursor: pointer;">&#8592; 목록으로</a>
</div>

    </main>
    <footer>
  <a class="footer-link" href="https://github.com/kakaoenterprise" target="_blank">
    <img src="/assets/GitHub-Mark.png" alt="Kakao Enterprise GitHub" />GitHub</a>
  <br/>
  <a class="footer-copyright">Copyright © Kakao Enterprise All rights reserved.</a>

</footer>

  </body>
</html>
