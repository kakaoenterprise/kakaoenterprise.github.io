---
layout: post
research-area: NLP
title: "Revisiting modularized multilingual NMT to meet industrial demands"
slug: emnlp2020-m2nmt
description: 다국어 번역 모델 아키텍처인 'M2NMT'의 재발견
published-date: 2020-11-16
publisher: EMNLP
publisher-fullname : Empirical Methods in Natural Language Processing (EMNLP)
authors:
  - james:카카오엔터프라이즈
  - meta:카카오엔터프라이즈
  - kevin:카카오엔터프라이즈
  - storm:카카오엔터프라이즈
paper: https://www.aclweb.org/anthology/2020.emnlp-main.476.pdf
code:
deepdive: 201217
tag:
  - NMT
---
카카오엔터프라이즈는 M2NMT<sup>Modularized Multilingual Neural Machine Translation Model</sup>[^1]의 가치를 재발견했습니다. 여러 번역 조건에서 성능 저하를 보이는 1-1 MNMT[^2]와는 달리, M2NMT에서는 성능이 유지됐습니다. 새로 추가된 언어를 포함한 태스크에서 NMT[^3]보다 좋은 성능을 보일 수 있음을 확인하였습니다. 또한, 제로샷<sup>zero-shot</sup>[^4] 에서 피벗 방식<sup>pivot translation</sup>[^5] 보다 더 좋은 성능을 달성했습니다. 카카오엔터프라이즈는 다국어 학습으로 인한 시너지<sup>data-diversification, and regularization effect</sup>가 주된 요인이라 분석했습니다.

카카오엔터프라이즈는 향후 이 모델이 생성한 '언어에 독립적인 특징 공간<sup>interlingual space</sup>'의 여러 효용을 검증하는 연구를 진행할 계획입니다.

<br/>

# Abstract

The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bot- tleneck and low maintainability hinders its extensive adoption in industries. In this study, we revisit the multilingual neural machine translation model that only shares modules among the same languages (M2) as a practical alternative to 1-1 to satisfy industrial requirements. Through comprehensive experiments, we identify the benefits of multi-way training and demonstrate that the M2 can enjoy these benefits without suffering from the capacity bottleneck. Furthermore, the interlingual space of the M2 allows convenient modification of the model. By leveraging trained modules, we find that incrementally added modules exhibit better performance than singly trained models. The zero-shot performance of the added modules is even comparable to supervised models. Our findings suggest that the M2 can be a competent candidate for multilingual translation in industries.

<br/>

# Experiments

By extensively comparing the single models, 1-1 model, and M2 in varying conditions, we find that the M2 can benefit from multi-way training through data-diversification and regularization while suffering less from capacity bottlenecks.

{% include image.html name="001.png" width="" align="center" %}
<em>[ Table 1 ] Averaged SacreBLEU test scores of single models, 1-1, and M2 trained using a balanced dataset of different configurations.  M2M  indicates the training of full many-to-many directions among languages (12 directions), whereas  JM2M  represents the training of directions that only include English on one side(6 directions). * indicates that the score is averaged only on English-centric.</em>

<br/>

-----

### footnote

[^1]: 다국어 번역을 위해 모든 언어마다 인코더와 디코더를 둔 아키텍처

[^2]: 다국어 번역을 위해 한쌍의 인코더와 디코더를 둔 아키텍처

[^3]: 두 언어간 번역을 위해 한쌍의 인코더와 디코더를 둔 아키텍처

[^4]: 학습되지 않은 방향의 번역

[^5]: 두 언어쌍에 대한 병렬 말뭉치(parallel corpus)를 구하기가 상대적으로 어려운 상황적 특성을 반영해, 전세계적으로 가장 많이 쓰이는 언어인 ‘영어’로 두번 번역한다. 예를 들어, 한국어↔︎중국어 번역 시 한국어↔︎영어로, 영어↔︎중국어로 번역한다.
