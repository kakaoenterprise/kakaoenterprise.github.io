<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://kakaoenterprise.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kakaoenterprise.github.io/" rel="alternate" type="text/html" /><updated>2021-06-17T00:59:17-05:00</updated><id>https://kakaoenterprise.github.io/feed.xml</id><title type="html">Kakao Enterprise AI Research</title><subtitle>카카오엔터프라이즈 연구 성과를 공개하는 리서치 플랫폼</subtitle><author><name>카카오엔터프라이즈</name></author><entry><title type="html">UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation</title><link href="https://kakaoenterprise.github.io/papers/interspeech2021-univnet" rel="alternate" type="text/html" title="UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation" /><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech2021-univnet</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech2021-univnet">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.&lt;/p&gt;</content><author><name>taylor:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">SE-Conformer: Time-Domain Speech Enhancement using Conformer</title><link href="https://kakaoenterprise.github.io/papers/interspeech2021-se-conformer" rel="alternate" type="text/html" title="SE-Conformer: Time-Domain Speech Enhancement using Conformer" /><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech2021-se-conformer</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech2021-se-conformer">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Convolution-augmented transformer(Conformer) has recently shown competitive results in speech-domain applications, such as automatic speech recognition, continuous speech separation, and sound event detection. Conformer can capture both the short and long-term temporal sequence information by attending to the whole sequence at once with multi-head self-attention and convolutional neural network. However, the effectiveness of conformer in speech enhancement has not been demonstrated. In this paper, we propose an end-to-end speech enhancement architecture(SE-Conformer), incorporating a convolutional encoder–decoder and conformer, designed to be directly applied to the time-domain signal. We performed evaluations on both the VoiceBank-DEMAND Corpus(VCTK) and Librispeech datasets in terms of objective speech quality metrics. The experimental results show that the proposed model outperforms other competitive baselines in speech enhancement performance.&lt;/p&gt;</content><author><name>chris:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Auxiliary Sequence Labeling Tasks for Disfluency Detection</title><link href="https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection" rel="alternate" type="text/html" title="Auxiliary Sequence Labeling Tasks for Disfluency Detection" /><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Detecting disfluencies in spontaneous speech is an important preprocessing step in natural language processing and speech recognition applications. Existing works for disfluency detection have focused on designing a single objective only for disfluency detection, while auxiliary objectives utilizing linguistic information of a word such as named entity or part-of-speech information can be effective. In this paper, we focus on detecting disfluencies on spoken transcripts and propose a method utilizing named entity recognition(NER) and part-of-speech(POS) as auxiliary sequence labeling(SL) tasks for disfluency detection. First, we investigate cases that utilizing linguistic information of a word can prevent mispredicting important words and can be helpful for the correct detection of disfluencies. Second, we show that training a disfluency detection model with auxiliary SL tasks can improve its F-score in disfluency detection. Then, we analyze which auxiliary SL tasks are influential depending on baseline models. Experimental results on the widely used English Switchboard dataset show that our method outperforms the previous state-of-the-art in disfluency detection.&lt;/p&gt;</content><author><name>이동엽:카카오</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Learning to Walk across Time for Interpretable Temporal Knowledge Graph Completion</title><link href="https://kakaoenterprise.github.io/papers/sigkdd-t-gap" rel="alternate" type="text/html" title="Learning to Walk across Time for Interpretable Temporal Knowledge Graph Completion" /><published>2021-08-14T00:00:00-05:00</published><updated>2021-08-14T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/sigkdd-t-gap</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/sigkdd-t-gap">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Static knowledge graphs(KGs), despite their wide usage in relational reasoning and downstream tasks, fall short of realistic modeling of knowledge and facts that are only temporarily valid. Compared to static knowledge graphs, temporal knowledge graphs(TKGs) inherently reflect the transient nature of real-world knowledge. Naturally, automatic TKG completion has drawn much research interests for a more realistic modeling of relational reasoning. However, most of the existing models for TKG completion extend static KG embeddings that do not fully exploit TKG structure, thus lacking in 1) accounting for temporally relevant events already residing in the local neighborhood of a query, and 2) path-based inference that facilitates multi-hop reasoning and better interpretability. In this paper, we propose T-GAP, a novel model for TKG completion that maximally utilizes both temporal information and graph structure in its encoder and decoder. T-GAP encodes query-specific substructure of TKG by focusing on the temporal displacement between each event and the query timestamp, and performs path-based inference by propagating attention through the graph. Our empirical experiments demonstrate that T-GAP not only achieves superior performance against state-of-the-art baselines, but also competently generalizes to queries with unseen timestamps. Through extensive qualitative analyses, we also show that T-GAP enjoys transparent interpretability, and follows human intuition in its reasoning process.&lt;/p&gt;</content><author><name>hoony:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">OutFlip: Generating Examples for Unknown Intent Detection with Natural Language Attack</title><link href="https://kakaoenterprise.github.io/papers/acl-ijcnlp2021-outflip" rel="alternate" type="text/html" title="OutFlip: Generating Examples for Unknown Intent Detection with Natural Language Attack" /><published>2021-08-01T00:00:00-05:00</published><updated>2021-08-01T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/acl-ijcnlp2021-outflip</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/acl-ijcnlp2021-outflip">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Out-of-domain (OOD) input detection is vital in a task-oriented dialogue system since the acceptance of unsupported inputs could lead to an incorrect response of the system. This paper proposes OutFlip, a method to generate out-of-domain samples using only in-domain training dataset automatically. A white-box natural language attack method HotFlip is revised to generate out-of-domain samples instead of adversarial examples. Our evaluation results showed that integrating OutFlip-generated out-of-domain samples into the training dataset could significantly improve an intent classification model’s out-of-domain detection performance.&lt;/p&gt;</content><author><name>heuristic:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Deep Context- and Relation-Aware Learning for Aspect-based Sentiment Analysis</title><link href="https://kakaoenterprise.github.io/papers/acl-ijcnlp2021-dcran" rel="alternate" type="text/html" title="Deep Context- and Relation-Aware Learning for Aspect-based Sentiment Analysis" /><published>2021-08-01T00:00:00-05:00</published><updated>2021-08-01T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/acl-ijcnlp2021-dcran</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/acl-ijcnlp2021-dcran">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Existing works for aspect-based sentiment analysis (ABSA) have adopted a unified approach, which allows the interactive relations among subtasks. However, we observe that these methods tend to predict polarities based on the literal meaning of aspect and opinion terms and mainly consider relations implicitly among subtasks at the word level. In addition, identifying multiple aspect–opinion pairs with their polarities is much more challenging. Therefore, a comprehensive understanding of contextual information w.r.t. the aspect and opinion is further required in ABSA. In this paper, we propose Deep Contextualized Relation-Aware Network (DCRAN), which allows interactive relations among subtasks with deep contextual information based on two modules (i.e., Aspect and Opinion Propagation and Explicit Self-Supervised Strategies). Especially, we design novel self-supervised strategies for ABSA, which have strengths in dealing with multiple aspects. Experimental results show that DCRAN significantly outperforms previous state-of-the-art methods by large margins on three widely used benchmarks.&lt;/p&gt;</content><author><name>오신혁:넷마블</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</title><link href="https://kakaoenterprise.github.io/papers/icml2021-e2e-tts" rel="alternate" type="text/html" title="Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech" /><published>2021-07-18T00:00:00-05:00</published><updated>2021-07-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/icml2021-e2e-tts</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icml2021-e2e-tts">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.&lt;/p&gt;</content><author><name>jay:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</title><link href="https://kakaoenterprise.github.io/papers/icml2021-vilt" rel="alternate" type="text/html" title="ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision" /><published>2021-07-18T00:00:00-05:00</published><updated>2021-07-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/icml2021-vilt</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icml2021-vilt">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Vision-and-Language Pretraining(VLP) has improved performance on various joint vision-and language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual encoder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-andLanguage Transformer (ViLT), monolithic in the sense that processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to 60 times faster than previous VLP models, yet with competitive or better downstream task performance.&lt;/p&gt;</content><author><name>김원재:카카오</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">U-Convolution Based Residual Echo Suppression With Multiple Encoders</title><link href="https://kakaoenterprise.github.io/papers/icassp2021-u-convolution-based-residual-eco-suppression" rel="alternate" type="text/html" title="U-Convolution Based Residual Echo Suppression With Multiple Encoders" /><published>2021-06-13T00:00:00-05:00</published><updated>2021-06-13T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/icassp2021-u-convolution-based-residual-eco-suppression</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icassp2021-u-convolution-based-residual-eco-suppression">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;In this paper, we propose an efficient end-to-end neural network that can estimate near-end speech using a U- convolution block by exploiting various signals to achieve residual echo suppression (RES). Specifically, the proposed model employs multiple encoders and an integration block to utilize complete signal information in an acoustic echo can- cellation system and also applies the U-convolution blocks to separate near-end speech efficiently. The proposed network affords an improvement in the perceptual evaluation of speech quality (PESQ) and the short-time objective intelligi- bility (STOI), as compared to baselines, in scenarios involving smart audio devices. The experimental results show that the proposed method outperforms the baselines for various types of mismatched background noise and environmental reverberation, while requiring low computational resources.&lt;/p&gt;</content><author><name>chris:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Multitask Learning and Joint Optimization For Transformer-Rnn-Tranducer Speech Recognition</title><link href="https://kakaoenterprise.github.io/papers/icassp2021-transformer-rnn-tranducer-speech-recognition" rel="alternate" type="text/html" title="Multitask Learning and Joint Optimization For Transformer-Rnn-Tranducer Speech Recognition" /><published>2021-06-13T00:00:00-05:00</published><updated>2021-06-13T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/icassp2021-transformer-rnn-tranducer-speech-recognition</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icassp2021-transformer-rnn-tranducer-speech-recognition">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Recently, several types of end-to-end speech recognition methods named transformer-transducer were introduced. According to those kinds of methods, transcription networks are generally modeled by transformer-based neural networks, while prediction networks could be modeled by either transformers or recurrent neural networks (RNN). This paper explores multitask learning, joint optimization, and joint decoding methods for transformer-RNN-transducer systems. Our proposed methods have the main advantage in that the model can maintain information on the large text corpus. We prove their effectiveness by performing experiments utilizing the well-known ESPNET toolkit for the widely used Librispeech datasets. We also show that the proposed meth- ods can reduce word error rate (WER) by 16.6 % and 13.3 % for test-clean and test-other datasets, respectively, with- out changing the overall model structure nor exploiting an external LM.&lt;/p&gt;</content><author><name>jeffrey:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry></feed>