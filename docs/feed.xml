<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kakaoenterprise.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kakaoenterprise.github.io/" rel="alternate" type="text/html" /><updated>2022-10-24T22:44:06-05:00</updated><id>https://kakaoenterprise.github.io/feed.xml</id><title type="html">카카오엔터프라이즈 AI Research</title><subtitle>카카오엔터프라이즈 AI Lab에서 발표한 AI 논문과 연구 성과를 소개합니다.</subtitle><author><name>카카오엔터프라이즈</name></author><entry><title type="html">Efficient Skeleton-Based Action Recognition via Joint-Mapping strategies</title><link href="https://kakaoenterprise.github.io/papers/wacv-action-recognition" rel="alternate" type="text/html" title="Efficient Skeleton-Based Action Recognition via Joint-Mapping strategies" /><published>2023-01-03T00:00:00-06:00</published><updated>2023-01-03T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/wacv-action-recognition</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/wacv-action-recognition"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Graph convolutional networks (GCNs) have brought remarkable progress in skeleton-based action recognition. However, high computational cost and large model size make models difficult to be applied in real-world embedded system. Specifically, GCN that is applied in automated surveillance system pre-require models such as pedestrian detection and human pose estimation. Therefore, each model should be computationally lightweight and whole process should be operated in real-time. In this paper, we propose two different joint-mapping modules to reduce the number of joint representations, alleviating a total computational cost and model size. Our models achieve better accuracy-latency trade-off compared to previous state-of-the-arts on two datasets, namely NTU RGB+D and NTU RGB+D 120, demonstrating the suitability for practical applications. Furthermore, we measure the latency of the models by using TensorRT framework to compare the models from a practical perspective.</p>]]></content><author><name>marcus:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Normalizing Mutual Information for Robust Adaptive Training for Translation</title><link href="https://kakaoenterprise.github.io/papers/emnlp-translation" rel="alternate" type="text/html" title="Normalizing Mutual Information for Robust Adaptive Training for Translation" /><published>2022-12-07T00:00:00-06:00</published><updated>2022-12-07T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/emnlp-translation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/emnlp-translation"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Despite the success of neural machine translation models, tensions between fluency of optimizing target language modeling and sourcefaithfulness remain as challenges. Previously, Conditional Bilingual Mutual Information (CBMI), a scoring metric for the importance of target sentences and tokens, was proposed to encourage fluent and faithful translations. The score is obtained by combining the probability from the translation model and the target language model, which is then used to assign different weights to losses from sentences and tokens. Meanwhile, we argue this metric is not properly normalized, for which we propose Normalized Pointwise Mutual Information (NPMI). NPMI utilizes an additional language model on source language to approximate the joint likelihood of source-target pair and the likelihood of the source, which is then used for normalizing the score. We showed that NPMI better captures the dependence between source-target and that NPMI-based token-level adaptive training brings improvements over baselines with empirical results from En-De, De-En, and En-Ro translation tasks.</p>]]></content><author><name>이영원:서울대</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">LittleBird: Efficient Faster &amp;amp; Longer Transformer for Question Answering</title><link href="https://kakaoenterprise.github.io/papers/emnlp-littlebird" rel="alternate" type="text/html" title="LittleBird: Efficient Faster &amp;amp; Longer Transformer for Question Answering" /><published>2022-12-07T00:00:00-06:00</published><updated>2022-12-07T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/emnlp-littlebird</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/emnlp-littlebird"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism. Longformer, ETC and BigBird addressed this issue and effectively solved the quadratic dependency problem. However we find that these models are not sufficient, and propose LittleBird, a novel model based on BigBird with improved speed and memory footprint while maintaining accuracy. In particular, we devise a more flexible and efficient position representation method based on Attention with Linear Biases (ALiBi). We also show that replacing the method of global information represented in the BigBird with pack and unpack attention is more effective. The proposed model can work on long inputs even after being pre-trained on short inputs, and can be trained efficiently reusing existing pretrained language model for short inputs. This is a significant benefit for low-resource languages where large amounts of long text data are difficult to obtain. As a result, our experiments show that LittleBird works very well in a variety of languages, achieving high performance in question answering tasks, particularly in KorQuAD2.0, Korean Question Answering Dataset for long paragraphs.</p>]]></content><author><name>phil:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets</title><link href="https://kakaoenterprise.github.io/papers/emnlp-apeach" rel="alternate" type="text/html" title="APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets" /><published>2022-12-07T00:00:00-06:00</published><updated>2022-12-07T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/emnlp-apeach</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/emnlp-apeach"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>In hate speech detection, developing training and evaluation datasets across various domains is the critical issue. Whereas, major approaches crawl social media texts and hire crowd-workers to annotate the data. Following this convention often restricts the scope of pejorative expressions to a single domain lacking generalization. Sometimes domain overlap between training corpus and evaluation set overestimate the prediction performance when pretraining language models on low-data language. To alleviate these problems in Korean, we propose APEACH that asks unspecified users to generate hate speech examples followed by minimal post-labeling. We find that APEACH can collect useful datasets that are less sensitive to the lexical overlaps between the pretraining corpus and the evaluation set, thereby properly measuring the model performance.</p>]]></content><author><name>양기창:카카오, 카카오엔터프라이즈, 숭실대</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods</title><link href="https://kakaoenterprise.github.io/papers/coling-multi-context-retrieval" rel="alternate" type="text/html" title="Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods" /><published>2022-10-16T00:00:00-05:00</published><updated>2022-10-16T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/Coling-multi-context-retrieval</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/coling-multi-context-retrieval"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Persona and Knowledge dual context opendomain chat is a novel dialogue generation task introduced recently (Jang et al., 2021). While Persona and Knowledge is each interesting context of open-domain dialogue, the combination of both has not been well studied. We tackle Persona-Knowledge identification and response generation tasks in this paper. We design an informed data augmentation strategy that is compatible with neural Q&amp;A retrieval models. With the augmented data, we perform permutative Persona-Knowledge evaluation and successive Persona search fine-tuning. Furthermore, we perform dialogue generation with various decoding techniques and illustrate crucial elements. We achieve SOTA across official metrics with 93.99% Grounding accuracy average and 23.62 SacreBLEU score.</p>]]></content><author><name>오민식:Alexa AI</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers</title><link href="https://kakaoenterprise.github.io/papers/interspeech-rnn-t" rel="alternate" type="text/html" title="Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-rnn-t</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-rnn-t"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Recurrent neural network transducer (RNN-T) is an end-to-end speech recognition framework converting input acoustic frames into a character sequence. The state-of-the-art encoder network for RNN-T is the Conformer, which can effectively model the local-global context information via its convolution and self-attention layers. Although Conformer RNN-T has shown outstanding performance, most studies have been verified in the setting where the train and test data are drawn from the same domain. The domain mismatch problem for Conformer RNN-T has not been intensively investigated yet, which is an important issue for the product-level speech recognition system. In this study, we identified that fully connected self-attention layers in the Conformer caused high deletion errors, specifically in the long-form out-domain utterances. To address this problem, we introduce sparse self-attention layers for Conformer-based encoder networks, which can exploit local and generalized global information by pruning most of the in-domain fitted global connections. Also, we propose a state reset method for the generalization of the prediction network to cope with long-form utterances. Applying proposed methods to an out-domain test, we obtained 27.6% relative character error rate (CER) reduction compared to the fully connected self-attention layer-based Conformers.</p>]]></content><author><name>김준태:SK텔레콤</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning</title><link href="https://kakaoenterprise.github.io/papers/interspeech-pronunciation" rel="alternate" type="text/html" title="Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-pronunciation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-pronunciation"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Self-supervised learning (SSL) approaches such as wav2vec 2.0 and HuBERT models have shown promising results in various downstream tasks in the speech community. In particular, speech representations learned by SSL models have been shown to be effective for encoding various speech-related characteristics. In this context, we propose a novel automatic pronunciation assessment method based on SSL models. First, the proposed method fine-tunes the pre-trained SSL models with connectionist temporal classification to adapt the English pronunciation of English-as-a-second-language (ESL) learners in a data environment. Then, the layer-wise contextual representations are extracted from all across the transformer layers of the SSL models. Finally, the automatic pronunciation score is estimated using bidirectional long short-term memory with the layer-wise contextual representations and the corresponding text. We show that the proposed SSL model-based methods outperform the baselines, in terms of the Pearson correlation coefficient, on datasets of Korean ESL learner children and Speechocean762. Furthermore, we analyze how different representations of transformer layers in the SSL model affect the performance of the pronunciation assessment task.</p>]]></content><author><name>chris:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation</title><link href="https://kakaoenterprise.github.io/papers/interspeech-emotion-recognition" rel="alternate" type="text/html" title="The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-emotion-recognition</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-emotion-recognition"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>In emotion recognition in conversation (ERC), the emotion of the current utterance is predicted by considering the previous context, which can be utilized in many natural language processing tasks. Although multiple emotions can coexist in a given sentence, most previous approaches take the perspective of a classification task to predict only a given label. However, it is expensive and difficult to label the emotion of a sentence with confidence or multi-label. In this paper, we automatically construct a grayscale label considering the correlation between emotions and use it for learning. That is, instead of using a given label as a one-hot encoding, we construct a grayscale label by measuring scores for different emotions. We introduce several methods for constructing grayscale labels and confirm that each method improves the emotion recognition performance. Our method is simple, effective, and universally applicable to previous systems. The experiments show a significant improvement in the performance of baselines.</p>]]></content><author><name>rung:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech</title><link href="https://kakaoenterprise.github.io/papers/interspeech-jets" rel="alternate" type="text/html" title="JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-JETS</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-jets"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>In neural text-to-speech (TTS), two-stage system or a cascade of separately learned models have shown synthesis quality close to human speech. For example, FastSpeech2 transforms an input text to a mel-spectrogram and then HiFi-GAN generates a raw waveform from a mel-spectogram where they are called an acoustic feature generator and a neural vocoder respectively. However, their training pipeline is somewhat cumbersome in that it requires a fine-tuning and an accurate speech-text alignment for optimal performance. In this work, we present end-to-end text-to-speech (E2E-TTS) model which has a simplified training pipeline and outperforms a cascade of separately learned models. Specifically, our proposed model is jointly trained FastSpeech2 and HiFi-GAN with an alignment module. Since there is no acoustic feature mismatch between training and inference, it does not requires fine-tuning. Furthermore, we remove dependency on an external speech-text alignment tool by adopting an alignment learning objective in our joint training framework. Experiments on LJSpeech corpus shows that the proposed model outperforms publicly available, state-of-the-art implementations of ESPNet2-TTS on subjective evaluation (MOS) and some objective evaluations.</p>]]></content><author><name>satoshi:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Proxyless Neural Architecture Adaptation at Once</title><link href="https://kakaoenterprise.github.io/papers/ieee-pnaa" rel="alternate" type="text/html" title="Proxyless Neural Architecture Adaptation at Once" /><published>2022-09-15T00:00:00-05:00</published><updated>2022-09-15T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/IEEE-PNAA</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/ieee-pnaa"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Recently, Neural Architecture Search (NAS) methods are introduced and show impressive performance on many benchmarks. Among those NAS studies, Neural Architecture Transformer (NAT) aims to adapt the given neural architecture to improve performance while maintaining computational costs. In the architecture adaptation task, we can utilize the known high-performance architectures, and the architecture adaptation results of NAT showed performance improvements on various architectures in their experiments. However, we verified that NAT lacks reproducibility through multiple trials of experiments. Moreover, it requires an additional architecture adaptation process before network weight training. In this paper, we propose proxyless neural architecture adaptation that is reproducible and efficient. The proposed method doesn’t need a proxy task for architecture adaptation. It directly improves the architecture during the conventional training process, and we can directly use the trained neural network. Moreover, the proposed method can be applied to both supervised learning and self-supervised learning. The proposed method shows stable performance improvements on various architectures and various datasets. Extensive experiments on two benchmark datasets, i.e ., CIFAR-10 and Tiny Imagenet, present that the proposed method definitely outperforms NAT and be applicable to various models and datasets.</p>]]></content><author><name>김도국:인하대</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry></feed>