<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kakaoenterprise.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kakaoenterprise.github.io/" rel="alternate" type="text/html" /><updated>2022-07-27T02:47:38-05:00</updated><id>https://kakaoenterprise.github.io/feed.xml</id><title type="html">카카오엔터프라이즈 AI Research</title><subtitle>카카오엔터프라이즈 AI Lab에서 발표한 AI 논문과 연구 성과를 소개합니다.</subtitle><author><name>카카오엔터프라이즈</name></author><entry><title type="html">Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers</title><link href="https://kakaoenterprise.github.io/papers/interspeech-rnn-t" rel="alternate" type="text/html" title="Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-rnn-t</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-rnn-t"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Recurrent neural network transducer (RNN-T) is an end-to-end speech recognition framework converting input acoustic frames into a character sequence. The state-of-the-art encoder network for RNN-T is the Conformer, which can effectively model the local-global context information via its convolution and self-attention layers. Although Conformer RNN-T has shown outstanding performance, most studies have been verified in the setting where the train and test data are drawn from the same domain. The domain mismatch problem for Conformer RNN-T has not been intensively investigated yet, which is an important issue for the product-level speech recognition system. In this study, we identified that fully connected self-attention layers in the Conformer caused high deletion errors, specifically in the long-form out-domain utterances. To address this problem, we introduce sparse self-attention layers for Conformer-based encoder networks, which can exploit local and generalized global information by pruning most of the in-domain fitted global connections. Also, we propose a state reset method for the generalization of the prediction network to cope with long-form utterances. Applying proposed methods to an out-domain test, we obtained 27.6% relative character error rate (CER) reduction compared to the fully connected self-attention layer-based Conformers.</p>]]></content><author><name>김준태:SK텔레콤</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning</title><link href="https://kakaoenterprise.github.io/papers/interspeech-pronunciation" rel="alternate" type="text/html" title="Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-pronunciation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-pronunciation"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Self-supervised learning (SSL) approaches such as wav2vec 2.0 and HuBERT models have shown promising results in various downstream tasks in the speech community. In particular, speech representations learned by SSL models have been shown to be effective for encoding various speech-related characteristics. In this context, we propose a novel automatic pronunciation assessment method based on SSL models. First, the proposed method fine-tunes the pre-trained SSL models with connectionist temporal classification to adapt the English pronunciation of English-as-a-second-language (ESL) learners in a data environment. Then, the layer-wise contextual representations are extracted from all across the transformer layers of the SSL models. Finally, the automatic pronunciation score is estimated using bidirectional long short-term memory with the layer-wise contextual representations and the corresponding text. We show that the proposed SSL model-based methods outperform the baselines, in terms of the Pearson correlation coefficient, on datasets of Korean ESL learner children and Speechocean762. Furthermore, we analyze how different representations of transformer layers in the SSL model affect the performance of the pronunciation assessment task.</p>]]></content><author><name>chris:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation</title><link href="https://kakaoenterprise.github.io/papers/interspeech-emotion-recognition" rel="alternate" type="text/html" title="The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-emotion-recognition</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-emotion-recognition"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>In emotion recognition in conversation (ERC), the emotion of the current utterance is predicted by considering the previous context, which can be utilized in many natural language processing tasks. Although multiple emotions can coexist in a given sentence, most previous approaches take the perspective of a classification task to predict only a given label. However, it is expensive and difficult to label the emotion of a sentence with confidence or multi-label. In this paper, we automatically construct a grayscale label considering the correlation between emotions and use it for learning. That is, instead of using a given label as a one-hot encoding, we construct a grayscale label by measuring scores for different emotions. We introduce several methods for constructing grayscale labels and confirm that each method improves the emotion recognition performance. Our method is simple, effective, and universally applicable to previous systems. The experiments show a significant improvement in the performance of baselines.</p>]]></content><author><name>rung:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech</title><link href="https://kakaoenterprise.github.io/papers/interspeech-jets" rel="alternate" type="text/html" title="JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-JETS</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-jets"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>In neural text-to-speech (TTS), two-stage system or a cascade of separately learned models have shown synthesis quality close to human speech. For example, FastSpeech2 transforms an input text to a mel-spectrogram and then HiFi-GAN generates a raw waveform from a mel-spectogram where they are called an acoustic feature generator and a neural vocoder respectively. However, their training pipeline is somewhat cumbersome in that it requires a fine-tuning and an accurate speech-text alignment for optimal performance. In this work, we present end-to-end text-to-speech (E2E-TTS) model which has a simplified training pipeline and outperforms a cascade of separately learned models. Specifically, our proposed model is jointly trained FastSpeech2 and HiFi-GAN with an alignment module. Since there is no acoustic feature mismatch between training and inference, it does not requires fine-tuning. Furthermore, we remove dependency on an external speech-text alignment tool by adopting an alignment learning objective in our joint training framework. Experiments on LJSpeech corpus shows that the proposed model outperforms publicly available, state-of-the-art implementations of ESPNet2-TTS on subjective evaluation (MOS) and some objective evaluations.</p>]]></content><author><name>satoshi:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Classification-based Multi-task Learning for Efficient Pose Estimation Network</title><link href="https://kakaoenterprise.github.io/papers/icpr-pose-estimation" rel="alternate" type="text/html" title="Classification-based Multi-task Learning for Efficient Pose Estimation Network" /><published>2022-08-21T00:00:00-05:00</published><updated>2022-08-21T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/ICPR-pose-estimation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icpr-pose-estimation"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Human pose estimation is an interesting and underlying topic in various fields such as action recognition and human-computer interaction. Although many methods have been developed recently, they are still far from perfect in accuracy and speed at a time. In this paper, we propose a Classification-based Pose Estimation Network with Multi-task Learning (CPENML) based on the low-resolution feature map to improve accuracy and inference time simultaneously. The proposed CPENML consists of two ideas. Firstly, novel proposed keypoint and offset estimation
tasks based on classification achieve better performance than regression. Secondly, the proposed Multi-Scale Network
(MSN) makes robust feature maps and balances the keypoint and offset tasks to maximize performance. To prove the effectiveness of the proposed method, we conduct ablation studies on the COCO dataset for proposed ideas. Compared to benchmarks, we demonstrate the superiority of our proposed method on COCO dataset in terms of inference time and accuracy.</p>]]></content><author><name>benjamin:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">ComDensE : Combined Dense Embedding of Relation-aware and Common Features for Knowledge Graph Completion</title><link href="https://kakaoenterprise.github.io/papers/icpr-comdense" rel="alternate" type="text/html" title="ComDensE : Combined Dense Embedding of Relation-aware and Common Features for Knowledge Graph Completion" /><published>2022-08-21T00:00:00-05:00</published><updated>2022-08-21T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/ICPR-ComDensE</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icpr-comdense"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Real-world knowledge graphs (KG) are mostly incomplete. The problem of recovering missing relations, called KG completion, has recently become an active research area. Knowledge graph (KG) embedding, a low-dimensional representation of entities and relations, is the crucial technique for KG completion. Convolutional neural networks in models such as ConvE, SACN, InteractE, and RGCN achieve recent successes. This paper takes a different architectural view and proposes ComDensE which combines relation-aware and common features using dense neural networks. In the relation-aware feature extraction, we attempt to create relational inductive bias by applying an encoding function specific to each relation. In the common feature extraction, we apply the common encoding function to all input embeddings. These encoding functions are implemented using dense layers in ComDensE. ComDensE achieves the state-of-the-art performance in the link prediction in terms of MRR, HIT@1 on FB15k-237 and HIT@1 on WN18RR compared to the previous baseline approaches. We conduct an extensive ablation study to examine the effects of the relation-aware layer and the common layer of the ComDensE. Experimental results illustrate that the combined dense architecture as implemented in ComDensE achieves the best performance.</p>]]></content><author><name>lucas:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base from Unstructured Text</title><link href="https://kakaoenterprise.github.io/papers/sigkdd-oasys" rel="alternate" type="text/html" title="OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base from Unstructured Text" /><published>2022-08-15T00:00:00-05:00</published><updated>2022-08-15T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/SIGKDD-OASYS</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/sigkdd-oasys"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>In recent years, creating and managing knowledge bases have become crucial to the retail product and enterprise domains. We present an automatic knowledge base construction system that mines data from documents. This system can generate training data during the training process without human intervention. Therefore, it is domain-agnostic trainable using only the target domain text corpus and a pre-defined knowledge base. This system is called OASYS and is the first system built with the Korean language in mind. In addition, we also have constructed a new human-annotated benchmark dataset of the Korean Wikipedia corpus paired with a Korean DBpedia to aid system evaluation. The system performance results on human-annotated benchmark test dataset are meaningful and show that the generated knowledge base from OASYS trained on only auto-generated data is useful. We provide both a human-annotated test dataset and an auto-generated dataset.</p>]]></content><author><name>lucas:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Connecting a Low Loss Subspace for Personalized Federated Learning</title><link href="https://kakaoenterprise.github.io/papers/sigkdd-federated-learning" rel="alternate" type="text/html" title="Connecting a Low Loss Subspace for Personalized Federated Learning" /><published>2022-08-14T00:00:00-05:00</published><updated>2022-08-14T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/SIGKDD-federated-learning</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/sigkdd-federated-learning"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Due to the curse of statistical heterogeneity across clients, adopting a personalized federated learning method has become an essential choice for the successful deployment of federated learning-based services. Among diverse branches of personalization techniques, a model mixture-based personalization method is preferred as each client has their own personalized model as a result of federated learning. It usually requires a local model and a federated model, but this approach is either limited to partial parameter exchange or requires additional local updates, each of which is helpless to novel clients and burdensome to the client’s computational capacity. As the existence of a connected subspace containing diverse low-loss solutions between two or more independent deep networks has been discovered, we combined this interesting property with the model mixture-based personalized federated learning method for improved performance of personalization. We proposed SuPerFed, a personalized federated learning method that induces an explicit connection between the optima of the local and the federated model in weight space for boosting each other. Through extensive experiments on several benchmark datasets, we demonstrated that our method achieves consistent gains in both personalization performance and robustness to problematic scenarios possible in realistic services.</p>]]></content><author><name>한석주:UNIST, 카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Paraphrasing via Ranking Many Candidates</title><link href="https://kakaoenterprise.github.io/papers/inlg-paraphrasing" rel="alternate" type="text/html" title="Paraphrasing via Ranking Many Candidates" /><published>2022-07-18T00:00:00-05:00</published><updated>2022-07-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/inlg-paraphrasing</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/inlg-paraphrasing"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>We present a simple and effective way to generate a variety of paraphrases and find a good quality paraphrase among them. As in previous studies, it is difficult to ensure that one generation method always generates the best paraphrase in various domains. Therefore, we focus on finding the best candidate from multiple candidates, rather than assuming that there is only one combination of generative models and decoding options. Our approach shows that it is easy to apply in various domains and has sufficiently good performance compared to previous methods. In addition, our approach can be used for data augmentation that extends the downstream corpus, showing that it can help improve performance in English and Korean datasets.</p>]]></content><author><name>rung:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">A Statistical Manifold Framework for Point Cloud Data</title><link href="https://kakaoenterprise.github.io/papers/icml-point-cloud-data" rel="alternate" type="text/html" title="A Statistical Manifold Framework for Point Cloud Data" /><published>2022-07-17T00:00:00-05:00</published><updated>2022-07-17T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/ICML-point-cloud-data</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icml-point-cloud-data"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Many problems in machine learning involve data sets in which each data point is a point cloud in R^D. A growing number of applications require a means of measuring not only distances between point clouds, but also angles, volumes, derivatives, and other more advanced concepts. To formulate and quantify these concepts in a coordinate-invariant way, we develop a Riemannian geometric framework for point cloud data. By interpreting each point in a point cloud as a sample drawn from some given underlying probability density, the space of point cloud data can be given the structure of a statistical manifold – each point on this manifold represents a point cloud – with the Fisher information metric acting as a natural Riemannian metric. Two autoencoder applications of our framework are presented: (i) smoothly deforming one 3D object into another via interpolation between the two corresponding point clouds; (ii) learning an optimal set of latent space coordinates for point cloud data that best preserves angles and distances, and thus produces a more discriminative representation space. Experiments with large-scale standard benchmark point cloud data show greatly improved classification accuracy vis-´a-vis existing methods.</p>]]></content><author><name>이용현:서울대</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry></feed>