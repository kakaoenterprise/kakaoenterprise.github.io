<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://kakaoenterprise.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kakaoenterprise.github.io/" rel="alternate" type="text/html" /><updated>2021-10-13T21:20:38-05:00</updated><id>https://kakaoenterprise.github.io/feed.xml</id><title type="html">카카오엔터프라이즈 AI Research</title><subtitle>카카오엔터프라이즈 AI Lab에서 발표한 AI 논문과 연구 성과를 소개합니다.</subtitle><author><name>카카오엔터프라이즈</name></author><entry><title type="html">SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustness</title><link href="https://kakaoenterprise.github.io/papers/neurips-smoothmix" rel="alternate" type="text/html" title="SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustness" /><published>2021-12-06T00:00:00-06:00</published><updated>2021-12-06T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/neurips-smoothmix</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/neurips-smoothmix">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Randomized smoothing is currently a state-of-the-art method to construct a certifiably robust classifier from neural networks against l2-adversarial perturbations. Under the paradigm, the robustness of a classifier is aligned with the prediction confidence, i.e., the higher confidence from a smoothed classifier implies the better robustness. This motivates us to rethink the fundamental trade-off between accuracy and robustness in terms of calibrating confidences of smoothed classifier. In this paper, we propose a simple training scheme, coined SmoothMix, to control the robustness of smoothed classifiers via self-mixup: it trains convex combinations of samples along the direction of adversarial perturbation for each input. The proposed procedure effectively identifies over-confident, near off-class samples as a cause of limited robustness in case of smoothed classifiers, and offers an intuitive way to adaptively set a new decision boundary between these samples for better robustness. Our experimental results demonstrate that the proposed method can significantly improve the certified l2-robustness of smoothed classifiers compared to existing state-of-the-art robust training methods.&lt;/p&gt;</content><author><name>정종현:카이스트</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Learning Debiased Representation via Disentangled Feature Augmentation</title><link href="https://kakaoenterprise.github.io/papers/neurips-learning-debiased-representation" rel="alternate" type="text/html" title="Learning Debiased Representation via Disentangled Feature Augmentation" /><published>2021-12-06T00:00:00-06:00</published><updated>2021-12-06T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/neurips-learning-debiased-representation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/neurips-learning-debiased-representation">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Image classification models tend to make decisions based on peripheral attributes of data items that have strong correlation with a target variable (i.e., dataset bias). These biased models suffer from the poor generalization capability when evaluated on unbiased datasets. Existing approaches for debiasing often identify and emphasize those samples with no such correlation (i.e., bias-conflicting) without defining the bias type in advance. However, such bias-conflicting samples are significantly scarce in biased datasets, limiting the debiasing capability of these approaches. This paper first presents an empirical analysis revealing that training with “diverse” bias-conflicting samples beyond a given training set is crucial for debiasing as well as the generalization capability. Based on this observation, we propose a novel feature-level data augmentation technique in order to synthesize diverse bias-conflicting samples. To this end, our method learns the disentangled representation of (1) the intrinsic attributes (i.e., those inherently defining a certain class) and (2) bias attributes (i.e., peripheral attributes causing the bias), from a large number of bias-aligned samples, the bias attributes of which have strong correlation with the target variable. Using the disentangled representation, we synthesize bias-conflicting samples that contain the diverse intrinsic attributes of bias-aligned samples by swapping their latent features. By utilizing these diversified bias-conflicting features during the training, our approach achieves superior classification accuracy and debiasing results against the existing baselines on both synthetic as well as real-world datasets.&lt;/p&gt;</content><author><name>이정수:카이스트</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Kakao Enterprise’s WMT21 Machine Translation using Terminologies Task Submission</title><link href="https://kakaoenterprise.github.io/papers/wmt21-terminology-translation" rel="alternate" type="text/html" title="Kakao Enterprise’s WMT21 Machine Translation using Terminologies Task Submission" /><published>2021-11-19T00:00:00-06:00</published><updated>2021-11-19T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/wmt21-terminology-translation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/wmt21-terminology-translation">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;This paper describes Kakao Enterprise’s submission to the WMT21 shared Machine Translation using Terminologies task. We integrate terminology constraints by pre-training with target lemma annotations and fine-tuning with exact target annotations utilizing the given terminology dataset. This approach yields a model that achieves outstanding results in terms of both translation quality and term consistency, ranking first based on COMET in the En→Fr language direction. Furthermore, we explore various methods such as back-translation, explicitly training terminologies as additional parallel data, and in-domain data selection.&lt;/p&gt;</content><author><name>juliette:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Capturing Speaker Incorrectness: Speaker-Focused Post-Correction for Abstractive Dialogue Summarization</title><link href="https://kakaoenterprise.github.io/papers/newsum-csi" rel="alternate" type="text/html" title="Capturing Speaker Incorrectness: Speaker-Focused Post-Correction for Abstractive Dialogue Summarization" /><published>2021-11-10T00:00:00-06:00</published><updated>2021-11-10T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/NewSum-CSI</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/newsum-csi">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;In this paper, we focus on improving the quality of the summary generated by neural abstractive dialogue summarization systems.&lt;/p&gt;

&lt;p&gt;Even though pre-trained language models generate well-constructed and promising results, it is still challenging to summarize the conversation of multiple participants since the summary should include a description of the overall situation and the actions of each speaker.&lt;/p&gt;

&lt;p&gt;This paper proposes self-supervised strategies for speaker-focused post-correction in abstractive dialogue summarization. Specifically, our model first discriminates which type of speaker correction is required in a draft summary and then generates a revised summary according to the required type.&lt;/p&gt;

&lt;p&gt;Experimental results show that our proposed method adequately corrects the draft summaries, and the revised summaries are significantly improved in both quantitative and qualitative evaluations.&lt;/p&gt;</content><author><name>이동엽:카카오</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model</title><link href="https://kakaoenterprise.github.io/papers/emnlp-evaluation-dataset-and-strategy" rel="alternate" type="text/html" title="An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model" /><published>2021-11-07T00:00:00-05:00</published><updated>2021-11-07T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/emnlp-evaluation-dataset-and-strategy</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/emnlp-evaluation-dataset-and-strategy">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Multi-turn response selection models have recently shown comparable performance to humans in several benchmark datasets. However, in the real environment, these models often have weaknesses, such as making incorrect predictions based heavily on superficial patterns without a comprehensive understanding of the context. For example, these models often give a high score to the wrong response candidate containing several keywords related to the context but using the inconsistent tense. In this study, we analyze the weaknesses of the open-domain Korean Multi-turn response selection models and publish an adversarial dataset to evaluate these weaknesses. We also suggest a strategy to build a robust model in this adversarial environment.&lt;/p&gt;</content><author><name>mat:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">AligNART: Non-autoregressive Neural Machine Translation by JointlyLearning to Estimate Alignment and Translate</title><link href="https://kakaoenterprise.github.io/papers/emnlp-alignart" rel="alternate" type="text/html" title="AligNART: Non-autoregressive Neural Machine Translation by JointlyLearning to Estimate Alignment and Translate" /><published>2021-11-07T00:00:00-05:00</published><updated>2021-11-07T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/emnlp-alignart</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/emnlp-alignart">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition. Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we introduce AligNART, which leverages full alignment information to explicitly reduce the modality of the target distribution. AligNART divides the machine translation task into (i) alignment estimation and (ii) translation with aligned decoder inputs, guiding the decoder to focus on simplified one-to-one translation. To alleviate the alignment estimation problem, we further propose a novel alignment decomposition method. Our experiments show that AligNART outperforms previous non-iterative NART models that focus on explicit modality reduction on WMT14 En↔De and WMT16 Ro→En. Furthermore, AligNART achieves BLEU scores comparable to those of the state-of-the-art connectionist temporal classification based models on WMT14 En↔De. We also observe that AligNART effectively addresses the token repetition problem even without sequence-level knowledge distillation.&lt;/p&gt;</content><author><name>송종윤:카카오엔터라이즈, 서울대</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Distilling Global and Local Logits with Densely Connected Relations</title><link href="https://kakaoenterprise.github.io/papers/iccv-distilling-global-and-local-logits" rel="alternate" type="text/html" title="Distilling Global and Local Logits with Densely Connected Relations" /><published>2021-10-11T00:00:00-05:00</published><updated>2021-10-11T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/iccv-distilling-global-and-local-logits</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/iccv-distilling-global-and-local-logits">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;In prevalent knowledge distillation, logits in most image recognition models are computed by global average pooling, then used to learn to encode the high-level and task-relevant knowledge. In this work, we solve the limitation of this global logit transfer in this distillation context. We point out that it prevents the transfer of informative spatial information, which provides localized knowledge as well as rich relational information across contexts of an input scene. To exploit the rich spatial information, we propose a simple yet effective logit distillation approach. We add a local spatial pooling layer branch to the penultimate layer, thereby our method extends the standard logit distillation and enables learning of both finely-localized knowledge and holistic representation. Our proposed method shows favorable accuracy improvement against the state-of-the-art methods on several image classification datasets. We show that our distilled students trained on the image classification task can be successfully leveraged for object detection and semantic segmentation tasks; this result demonstrates our method’s high transferability.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;본 글에서는 카카오엔터프라이즈 AI Lab과 경희대, 포항공대에서 함께 연구한 새로운 지식 증류(Knowledge Distillation) 방법론이 ICCV 2021 학회를 통해 발표되어, 간략하게 소개드리고자 합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-지식-증류-기법의-등장&quot;&gt;1. 지식 증류 기법의 등장&lt;/h1&gt;

&lt;p&gt;지식 증류 기법은 기 학습된 거대 모델(teacher)의 지식을 새로운 네트워크(student)가 전달받아 학습하는 방식으로, ‘teacher-student’ 모델이라고도 합니다. 잘 학습된 teacher 모델을 기반으로, 경량화된 student 모델이 학습을 통해 그에 버금가는 우수한 성능을 내고자 한다는 점이 특징입니다.&lt;/p&gt;

&lt;p&gt;지식 증류 기법은 기존 딥러닝 모델이 가지는 컴퓨팅 리소스과 메모리의 한계, 긴 추론시간 등의 문제를 해결하고자 제시된 방법입니다. 일반적으로 딥러닝 모델은 모델의 크기가 커질수록, 즉 파라미터 수가 많아질수록 더 높은 성능을 내게 됩니다. 더 많은 연산량을 처리하기 위해서는 대량의 컴퓨팅 리소스와 학습시간을 필요로 하기 때문에, 상대적으로 저성능, 저전력의 소형 모바일 디바이스나 IoT 기기에는 이를 활용하기 어렵다는 문제가 있었습니다. 이같은 문제를 해결하고 효율적인 딥러닝 학습을 시도하기 위한 움직임으로, 현재 지식 증류 기법 연구가 주목받고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-기존-지식-증류-기법의-한계&quot;&gt;2. 기존 지식 증류 기법의 한계&lt;/h1&gt;

&lt;p&gt;지식 증류 과정에서 teacher 모델의 정보를 student 모델에 전달하기 위해, logit 값이 활용됩니다. 여기서 logit은 해당 task와 직접적인 연관을 가지는 모델의 출력값으로, feature보다 입력 이미지에 대한 모델의 representation 정보를 풍부하게 담고 있어 학습 데이터와 함께 사용될 경우 효율적인 학습이 가능해집니다.&lt;/p&gt;

&lt;p&gt;이때 logit은 과적합(overfitting)을 방지하기 위해 global average pooling(GAP)으로 계산되는데, 이로 인해 logit을 활용한 기존 연구들에서는 세부 공간에 대한 logit을 사용할 수 없다는 한계가 존재했습니다. 이에 카카오엔터프라이즈 연구팀은 global logit과 local logit이라는 개념을 새롭게 제시한 ‘Global and Local Logit with Densely Connected Relations(GLD)’ 방법론을 제안하여 이같은 문제를 해결하고자 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-global-and-local-logit-with-densely-connected-relationsgld-방법론-소개&quot;&gt;3. Global and Local Logit with Densely Connected Relations(GLD) 방법론 소개&lt;/h1&gt;

&lt;p&gt;해당 방법론의 가장 큰 특징은 앞서 언급한 global logit과 local logit을 모두 사용했다는 점입니다. 먼저 global logit은 입력된 이미지의 global feature에서 classifier를 거쳐 최종적으로 산출된 값으로, 기존 연구에서 주로 활용되던 정보입니다. 여기에 global feature를 세분화한 뒤, 동일한 방식으로 산출한 값이 local logit입니다. 보다 세밀한 공간 정보를 담은 local logit을 추가적으로 사용함으로써 디테일한 정보를 학습에 반영할 수 있게 되었습니다.&lt;/p&gt;

&lt;p&gt;또한, 개별 이미지 샘플에서 세부 영역의 관계 정보를 파악할 수 있음은 물론, 여러 샘플값 간의 관계 정보도 파악하여 모델 성능을 높였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/001.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림1. GLD 전체 구조&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이밖에도 GLD에서는 logit 값을 정규화하기 위해 loss 함수 LND을 새롭게 제안하였습니다. 기존 방법론에서는 softmax 함수 사용으로 정보량이 적은 확률 분포가 도출되는 문제를 해결하기 위해, 고정된 temperature 파라미터를 사용하여 확률 분포의 정보량을 증가시켰습니다. 하지만, 이는 개별 이미지마다 확률 분포의 특징이 다르다는 것을 고려하지 않았다는 한계가 있었기 때문에, GLD에서는 입력 이미지마다 logit의 표준편차를 사용하여 개별 조정하는 방식을 활용하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-성능-평가&quot;&gt;4. 성능 평가&lt;/h1&gt;

&lt;p&gt;카카오엔터프라이즈 연구팀은 모델의 성능을 검증하기 위해, 9가지의 지식 증류 SOTA(State-of-the-art) 모델과 성능 비교 실험을 진행하였습니다. 실험결과는 각각 CIFAR-100, ImageNet, CINIC-10, STL-10, VOC2007, COCO2017 데이터셋으로 측정하였습니다.&lt;/p&gt;

&lt;p&gt;먼저 이미지 분류 문제에서 성능 테스트를 위해 CIFAR-100, ImageNet 데이터를 활용하였습니다. CIFAR-100 테스트에서는 다양한 환경에서의 방법론 검증을 위해 총 4가지의 실험모델을 구성하여 테스트를 진행하였고, ImageNet 테스트에서는 Top-1과 Top-5 정확도를 구분하여 평가한 결과 기존 연구 모델보다 평균적으로 우수한 성능을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/002.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표1. 실험모델 구성 (CIFAR-100 데이터셋 기준)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/003.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표2. GLD와 기존 방법론 성능 비교 (CIFAR-100 데이터셋 기준)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/004.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표3. GLD와 기존 방법론 성능 비교 (ImageNet 데이터셋 기준)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;또한, 실질적으로 teacher 모델에서 student 모델로 얼마나 지식이 잘 전달될 수 있는지, 그 성능을 평가해보고자 학습되지 않은 CINIC-10과 STL-10 데이터셋을 활용하여 실험을 진행한 결과 SOTA보다 높은 성능을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/005.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표4. GLD와 기존 방법론 성능 비교 (CINIC-10과 STL-10 데이터셋 기준)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이밖에도 객체 탐지와 시멘틱 세그멘테이션 문제에서 성능을 확인하기 위해 VOC2007, COCO2017 데이터셋을 활용한 결과, 우수한 성능을 확인함은 물론, 이미지 분류뿐만 아니라 feature representation 영역에서 높은 성능을 나타냄을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/006.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표5. 객체 탐지 문제에서의 GLD와 기존 방법론 성능 비교 (VOC2007과 COCO2017 데이터셋 기준)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/007.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표6. 시멘틱 세그멘테이션 문제에서의 GLD와 기존 방법론 성능 비교 (VOC2007과 COCO2017 데이터셋 기준)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;5-향후-연구-계획&quot;&gt;5. 향후 연구 계획&lt;/h1&gt;

&lt;p&gt;향후 해당 방법론을 기반으로 딥러닝 알고리즘을 경량화하여 얼굴 인식 및 여러가지 비전 기반 서비스의 계산량을 효율화하는 데에 활용할 계획입니다. 앞으로도 카카오엔터프라이즈 연구에 많은 관심과 응원 부탁드립니다. 감사합니다.&lt;/p&gt;</content><author><name>harry:경희대, 카카오엔터프라이즈</name></author><category term="papers" /><category term="ICCV" /><category term="distillation" /><category term="GLD" /><summary type="html">Abstract</summary></entry><entry><title type="html">Improving End-to-End Contextual Speech Recognition via a Word-Matching Algorithm with Backward Search</title><link href="https://kakaoenterprise.github.io/papers/ieee-e2e-csr" rel="alternate" type="text/html" title="Improving End-to-End Contextual Speech Recognition via a Word-Matching Algorithm with Backward Search" /><published>2021-10-04T00:00:00-05:00</published><updated>2021-10-04T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/ieee-e2e-csr</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/ieee-e2e-csr">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;End-to-end automatic speech recognition (E2E-ASR) prefers the common words during training rather than rare ones related to contextual information such as song names. Thus, recognizing contextual information correctly is a hurdle for E2E-ASR to reach the production-level. To overcome the limitations of E2E-ASR in recognizing contextual information, this work presents a post-processing followed by E2E-ASR in an algorithmic way, referred to as a word-matching algorithm with backward search (WMA-BS). At first, we allow E2E-ASR to roughly detect the position of target words that has similar pronunciation with desired contextual phrases. After that, given the hypothesis from E2E-ASR with the rough position of target words, WMA-BS estimates the correct target words and decides whether to replace the target words with the contextual phrase or not, according to their phonetic and literal similarity. Applying the proposed method to E2E-ASR achieved relative improvement up to 52.7% in word error rate across several harsh conditions.&lt;/p&gt;</content><author><name>jaytee:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation</title><link href="https://kakaoenterprise.github.io/papers/interspeech2021-univnet" rel="alternate" type="text/html" title="UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation" /><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech2021-univnet</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech2021-univnet">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈 AI Lab 음성처리팀은 카카오 i에 적용되는 TTS(Text to Speech) 연구를 진행해오고 있습니다.​​​ TTS 시스템은 크게 텍스트에서 acoustic feature를 생성하는 어쿠스틱 모델(acoustic model)과 이 스펙트로그램에서 음성신호를 합성해 AI 음성을 만들어내는 보코더(vocoder)로 구성됩니다. 여기서 보코더는 고품질의 음성을 생성하는데 주요한 역할을 담당하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/001.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림1. TTS 구조&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈 연구팀은 기존 연구보다 개선된 고품질 음성 합성을 가능케하는 뉴럴 보코더 기술 ‘UnivNet’을 고안해, 이번 INTERSPEECH 2021에서 연구 내용을 공개하게 되었습니다. 지난해 음성합성 모델과 음소-오디오 정렬 모델을 한꺼번에 훈련하는 아키텍처 ‘JDI-T’를 공개한데 이어, 2년 연속 연구 성과를 발표하게 되었습니다. 본 글에서는 이번 연구 성과에 대해 간략하게 소개드리고자 합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-기존-뉴럴-보코더-연구의-한계&quot;&gt;1. 기존 뉴럴 보코더 연구의 한계&lt;/h1&gt;

&lt;p&gt;대다수 뉴럴 보코더(neural vocoder) 연구에서는 전체 주파수 대역 중 일부(0-8kHz)에 해당하는 멜 스펙트로그램(mel-spectrogram)을 입력값으로 사용하고 있습니다. 여기서 멜 스펙트로그램은 인간의 인지 기준에 따라 헤르츠(Hz) 단위의 주파수를 mel-scale에 따라 변환한 값으로, 딥러닝에서 오디오 신호 처리에 많이 활용되는 피쳐(feature)입니다. 일반적으로 사람들은 고주파보다 저주파를 잘 인지하기 때문에, 스펙트로그램의 저주파 부분을 보다 잘 인식할 수 있도록 저주파 부분을 확장시킨 점이 특징입니다.&lt;/p&gt;

&lt;p&gt;이때, 전체 대역폭을 입력값으로 사용하면 음향정보가 더욱 많아져 깨끗한 음성을 얻을 수 있음에도 불구하고, 일부값만이 활용되었습니다. 전체 값을 활용한 일부 연구에서는 합성 음성의 고주파수 대역이 흐릿해지는 문제(over smoothing)가 발생해, 음성에 지지직거리는 소리 등 잡음이 섞여 기대했던 것 이상의 음성 품질을 얻기 어려운 경우들이 있었습니다.&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈 연구팀은 이같은 문제를 해결하고자 새로운 뉴럴 보코더 방법론 ‘UnivNet’을 고안해, 실시간 서비스에서 더욱 깨끗한 음성을 제공하고자 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-univnet-특징-소개&quot;&gt;2. UnivNet 특징 소개&lt;/h1&gt;

&lt;p&gt;먼저 UnivNet의 구조는 &lt;그림2&gt;와 같이 크게 생성기(generator)와 판별기(discriminator)로 구분됩니다. generator는 MelGAN 기반 구조이며, 여기에 더해 모델의 크기를 유지하면서도 효과적으로 로컬 정보를 확보하기 위해 LVC(Location-Variable Convolution)를 추가하였습니다. 이로 인해 mel-spectrogram의 지역별 정보를 효율적으로 모델에 제공하여, 더 적은 파라미터 수로도 더 높은 음질을 얻을 수 있었습니다. 또한, 효율적인 연산을 위해 GAU(Gated Activation Unit)를 더했습니다.&lt;/그림2&gt;&lt;/p&gt;

&lt;p&gt;다음으로 discriminator에서는 generator에서 생성된 가짜 데이터와 실제 데이터를 구별하도록 학습을 진행합니다. 여기서 주목할 점은 multi-resolution spectrogram discriminator(이하 MRSD)를 사용했다는 것입니다. MRSD는 다양한 STFT 파라미터셋을 사용하여 실제 데이터와 생성된 가짜 데이터의 여러 선형 스펙트로그램 크기들을 계산해, 각 하위 판별기에 입력값으로 활용합니다. 이때, STFT 파라미터셋에는 1)푸리에 변환 차수, 2)시간(frame) 이동 간격, 3)윈도우(window) 길이가 포함됩니다.&lt;/p&gt;

&lt;p&gt;UnivNet은 MRSD 구조를 통해 전체 대역폭 데이터가 가진 다양한 시간과 해상도(resolution) 정보를 사용하여 실제 사람 음성과 같은 높은 수준의 음성을 생성하고자 하였습니다. MRSD 구조는 MelGAN의 multi-scale waveform discriminator(MSWD) 구조에 기반하며, 여기에 시간 영역에서 적대적 모델링을 개선하기 위해 multi-period waveform discriminator(MPWD)를 더한 점이 특징입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/002.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림2. UnivNet 구조&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-실험-결과&quot;&gt;3. 실험 결과&lt;/h1&gt;

&lt;h3 id=&quot;1-데이터-구성&quot;&gt;1) 데이터 구성&lt;/h3&gt;

&lt;p&gt;해당 실험에는 LibriTTS 데이터셋을 활용하였습니다. LibriTTS 데이터셋은 영어 오디오북 데이터셋으로, 이 중 192시간 분량, 11만 6천개의 발화, 904명의 화자 데이터로 구성된 ‘train-clean-360’을 바탕으로 모델 훈련을 진행하고, 이미 아는 화자(seen speaker)에 대한 평가를 진행하였습니다. 9시간 분량, 4천개의 발화, 39명의 화자 데이터로 구성된 ‘train-clean’ 데이터셋으로는 처음 보는 화자(unseen speaker)에 대한 평가를 진행하였습니다. 또한, TTS 성능 평가를 위해서는 24시간 분량, 1만 3천개 발화 데이터로 구성된 LJSpeech 데이터셋을 활용하였습니다. 해당 데이터셋은 영어로 구성된 단일 화자의 데이터셋입니다.&lt;/p&gt;

&lt;h3 id=&quot;2-ablation-study&quot;&gt;2) Ablation study&lt;/h3&gt;

&lt;p&gt;먼저 해당 모델의 자체 성능을 파악하기 위해 Ablation study를 진행하였습니다. 각 구성요소는 G1=LVC, G2=GAU, D1=MRSD, D2=MPWD, D3=MSWD와 같습니다. 여기서 주목할 점은 &lt;표1&gt;에서 D1(MRSD)이 제거되면 &lt;그림3&gt;의 왼쪽 그림과 같이 생성된 음성의 고주파수 대역이 흐릿해지는 문제가 발생한다는 점입니다. 제안하는 모델의 결과를 나타내는 &lt;그림3&gt;의 가운데 그림을 보면 이러한 문제가 개선되어 실제 녹음 음성의 고주파수 대역과 비슷해진 점을 볼 수 있습니다. 이를 통해 Univnet 모델에서 MRSD 구조가 가지는 중요성을 확인할 수 있었습니다.&lt;/그림3&gt;&lt;/그림3&gt;&lt;/표1&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/003.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표1. Ablation study 결과&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/004.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림3. 오디오 클립에서 생성된 스펙트로그램&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-기존-보코더-모델과-성능-비교&quot;&gt;3) 기존 보코더 모델과 성능 비교&lt;/h3&gt;

&lt;p&gt;다음으로는 generator의 채널 크기가 다른 두 가지 버전의 UnivNet(UnivNet-16, UnivNet-32)을 준비하여 GAN 기반 보코더(MelGAN, Parallel WaveGAN, HiFi-GAN)와 성능을 비교해보았습니다.&lt;/p&gt;

&lt;p&gt;보코더 자체의 성능을 평가하기 위해 실제 화자의 음성(Seen/Unseen speakers)에서 추출된 멜 스펙트로그램을 보코더를 이용하여 음성을 생성하는 과정을 진행하였습니다. &lt;표2&gt;를 보면 UnivNet-16은 학습 때 활용된 화자 데이터 외에 처음 보는 화자 데이터에서도 우수한 품질의 음성을 생성하였습니다. 기존 보코더 모델의 경우 새로운 화자가 추가될 때마다 모델을 추가 학습해야 하는 불편함이 있었지만, UnivNet은 처음 보는 화자 데이터에서도 우수한 성과를 보였다는 점에서 이같은 문제를 상당수 개선하였다고 볼 수 있습니다. 또한, UnivNet-32의 경우 전체 분야에서 기존 보코더 모델보다 높은 점수를 기록하고, 추론 속도도 다소 절감시키는 등 의미있는 결과값을 얻었습니다. 이어 진행된 TTS 성능 평가에서도 UnivNet-32은 우수한 성능을 보이며, 보코더 성능뿐만 아니라, 실제 TTS 구조 상에서도 높은 성능을 보여줌을 확인하였습니다.&lt;/표2&gt;&lt;/p&gt;

&lt;p&gt;해당 연구는 기존 보코더 모델보다 더 많은 대역폭을 쓰면서도 over-smoothing 문제 없이 더 깨끗한, 선명한 합성음을 얻었다는 점에서 의의가 있습니다. 또한, 음성 합성 분야에서 합성음의 품질뿐만 아니라, 중요한 추론 속도를 절감시켰다는 점도 주목할만합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/005.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표2. 기존 보코더 모델과 성능 비교&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-향후-계획&quot;&gt;4. 향후 계획&lt;/h1&gt;

&lt;p&gt;향후 UnivNet은 카카오 i 서비스에 적용되어 실제 다수의 사용자를 대상으로 서비스될 예정입니다. 합성음의 명료도와 자연성이 실제 사람의 발화 수준과 동일할 정도로, 그 품질을 향상시키기 위해 지속 연구할 계획입니다. 또한, fine-tuning 과정 없이도 여러 화자의 TTS 파이프라인에 활용할 수 있는 유니버셜 보코더 연구를 진행하고자 합니다. 앞으로도 카카오엔터프라이즈의 음성처리 연구에 많은 관심 부탁드립니다. 감사합니다.&lt;/p&gt;</content><author><name>taylor:카카오엔터프라이즈</name></author><category term="papers" /><category term="INTERSPEECH" /><category term="UnivNet" /><category term="vocoder" /><summary type="html">Abstract</summary></entry><entry><title type="html">Auxiliary Sequence Labeling Tasks for Disfluency Detection</title><link href="https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection" rel="alternate" type="text/html" title="Auxiliary Sequence Labeling Tasks for Disfluency Detection" /><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Detecting disfluencies in spontaneous speech is an important preprocessing step in natural language processing and speech recognition applications. Existing works for disfluency detection have focused on designing a single objective only for disfluency detection, while auxiliary objectives utilizing linguistic information of a word such as named entity or part-of-speech information can be effective. In this paper, we focus on detecting disfluencies on spoken transcripts and propose a method utilizing named entity recognition(NER) and part-of-speech(POS) as auxiliary sequence labeling(SL) tasks for disfluency detection. First, we investigate cases that utilizing linguistic information of a word can prevent mispredicting important words and can be helpful for the correct detection of disfluencies. Second, we show that training a disfluency detection model with auxiliary SL tasks can improve its F-score in disfluency detection. Then, we analyze which auxiliary SL tasks are influential depending on baseline models. Experimental results on the widely used English Switchboard dataset show that our method outperforms the previous state-of-the-art in disfluency detection.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그동안 카카오엔터프라이즈 AI Lab에서는 음성인식 영역과 자연어처리 분야에 활용 가능한 ‘사족제거(disfluency detection)’를 주제로, 연구를 진행해 왔습니다. 이번 INTERSPEECH 2021에서 NER(개체명 인식), POS(품사 태그) 정보를 활용해 사족제거 작업의 정확도를 높이는 새로운 방법론을 공개하게 되어, 해당 내용을 간략하게 소개드리고자 합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-사족제거-연구의-필요성&quot;&gt;1. 사족제거 연구의 필요성&lt;/h1&gt;

&lt;p&gt;먼저 사족제거 작업은 화자가 “음”, “아”와 같이 의미 없이 발화한 간투사를 교정하고, 발화 중 반복 사용한 표현들을 정제하는 과정을 말합니다. 예를 들어 “어… 오늘, 오늘은 날씨가 참 좋네”라는 문장에서 사족을 제거한다면, 별다른 의미를 가지지 않는 “어…”와 반복된 “오늘”을 삭제할 수 있습니다.&lt;/p&gt;

&lt;p&gt;실제 발화 상황에서는 이와 같은 간투사나 숨소리, 머뭇거림 등 음성 전사 후 언어를 처리하는데 불필요한 발화가 다수 포함될 수 있습니다. 이런 발화를 음성인식 모델이 인식하고, 음성번역, 언어이해 등 자연어 처리(NLP)를 하기 위해서는 이러한 사족을 제거하는 작업이 필수적입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-기존-사족제거-연구의-한계&quot;&gt;2. 기존 사족제거 연구의 한계&lt;/h1&gt;

&lt;p&gt;기존 사족제거 연구는 word 단위의 자질(feature)을 각각 reparandum(RM), interregnum(IM), repair(RP)로 구분하여 우리가 제거해야할 대상(RM, IM)과 아닌 것(RP)을 명확히 파악하는데 초점을 두었습니다. IM에는 ‘음, 아’와 같은 간투사 표현들이 해당되고, 이들은 쉽게 구분되는 특성을 가졌기 때문에 IM보다는 주로 RM에 대한 예측정확도를 높이는데 중점을 둔 연구가 주를 이루고 있습니다. 여기서 RM은 쉽게 말해, 문장 시작에 들어가는 쿠션어, 반복어 표현들을 일컫는데, 먼저 입력된 값(RM)에 대해 오류라고 판단하고 뒤에 오는 값(RP)을 올바른 값으로 보았습니다. 기존 연구들은 이 RM을 파악하는데 목적을 두고, RM으로 분류된 정답 데이터를 학습하는 방식이 주로 활용되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/001.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림1. 기존 사족제거 연구의 어노테이션(annotation) 예시&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;하지만, 이와 같이 제거 대상만을 가려내는데 초점을 두면 중요한 의미를 가진 단어를 사족으로 오예측(false positive)하게 되고 예측정확도가 떨어질 수 있다는 한계점이 있습니다. 이에, 카카오엔터프라이즈 연구팀은 추가적인  정보들을 학습시켜 예측 성능을 높이는 방법론을 고안하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-auxiliary-sequence-labeling-tasks-방법론-소개&quot;&gt;3. Auxiliary Sequence Labeling Tasks 방법론 소개&lt;/h1&gt;

&lt;p&gt;카카오엔터프라이즈 연구팀이 새롭게 제안하는 방법론은 ‘Auxiliary Sequence Labeling Tasks’입니다. 기존 연구에서 추가적인 정보로 NER(개체명 인식, Named Entity Recognition), POS(품사 태그, Part-Of-Speech)를 Multi-Task Learning 방식에 활용해, 총 3개의 목적함수를 사용하여 학습을 진행하였습니다.&lt;/p&gt;

&lt;p&gt;좀 더 자세히 살펴보면, 먼저 NER은 해당 단어의 개체명(Entity Name)이 인명, 장소, 시간 표현 등을 정의하는 과제(task)입니다.  그림2를 예시로 보면 NER 정보는 초록색으로 나타납니다. 기존 연구에서는 사족에 해당하는 ‘i would’를 명확히 파악하지 못하고, 오히려 문장의 중요한 의미를 담고 있는 ‘my forty’를 사족으로 오예측하였습니다. 본 연구에서는 이처럼 중요한 의미를 갖는 단어의 오예측을 방지하고, 정확한 사족 예측이 가능해지도록 NER 정보를 학습에 활용했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/002.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림2. NER 정보 예시 (초록색이 NER에 해당하며, 파란색이 사족에 해당함)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;또한, 추가적으로 사용된 POS는 각 문장 성분의 품사 태그 정보를 의미합니다. 그림3에서는 RM과 RP의 POS가 동일한 태그 형태를 가질 때, 보다 정확한 사족 예측이 가능함을 보여줍니다. 기존 연구에서도 POS를 활용한 연구들은 진행되었지만, rule-base 방식의 경우 단어 의미가 담긴 시맨틱 정보가 제대로 반영되지 않는다는 한계가 있었고, ML방식의 경우 추론(inference) 시 해당 입력 단어들에 대한 feature를 추출하기 위한 추가 시간이 소요된다는 문제가 있었습니다. 카카오엔터프라이즈 연구팀은 POS와 NER 정보를 추가 활용하는 Auxiliary Sequence Labeling Tasks 방법론을 새롭게 제안하여 그동안의 문제점들을 해결하고 더 높은 성능을 얻고자 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/003.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림3. POS 정보 예시&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;본 연구에서는 사족제거, NER, 그리고 POS태그들을 예측을 위해 3가지의 negative log likelihood loss 함수를 사용하였습니다. 사족제거, NER, 그리고 POS 테스크를 위한 loss 함수들을 각각 Ld, Le, Lp라고 정의할때, 최종 loss 함수는 L= Ld + *(Le +Lp)로 정의합니다. 여기서  는 계수(coefficients)로써, NER과 POS 테스크들의 영향도를 학습에 얼마나 반영할지 결정하는 요소입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-성능-평가&quot;&gt;4. 성능 평가&lt;/h1&gt;

&lt;p&gt;제안한 방법의 성능평가를 위해 기존 연구와의 모델 성능비교와 학습요소별 성능비교 2가지 방식을 이용하였습니다. 먼저 동일한 CRF Layer(Decoder)에 입력 자질(feature input)로 사전훈련된 언어모델 transformer, BERT, ELECTRA의 최종 output layer를 사용하여 각각의 성능을 비교해 보았습니다. 현재 성능 테스트에 많이 활용되고 있는 English Switchboard 데이터셋을 활용하여 테스트해본 결과, F1 score에서 기존 연구 모델보다 평균적으로 높은 수치를 기록하는 것을 확인할 수 있었고, 최고치의 경우 93.1(ELECTRA)에 달하는 우수한 결과를 얻었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/004.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표1. 기존 방식으로 학습된 모델과 Auxiliary SL(Sequence Labeling) Tasks 방식으로 학습된 모델의 성능 비교&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;다음으로, 동일한 English Switchboard 데이터셋을 이용해 Ablation 분석을 진행하였습니다. 1)기존 연구 방식으로 학습된 모델, 2)NER을 추가한 경우, 3)POS를 추가한 경우, 4)AUXILIARY SL Tasks 방식(NER과 POS를 모두 추가, SL : Sequence Labeling)을 각각 비교해 보았습니다. 표2에서 볼 수 있듯이 추가 정보를 모두 활용한 경우가 F1 score에서 가장 높은 수치를 기록하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/005.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표2. Ablation Analysis 분석 결과&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이를 통해 추가 정보를 활용하여 목적함수를 확대하는 것이 실제 성능에 영향력을 미친다는 것을 확인할 수 있었다는 점에서 해당 연구가 갖는 의의가 크다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;추론에서도 본 연구의 장점이 있습니다. 기존의 연구에서도 이와 비슷하게 모델의 입력으로 단어의 NER과 POS와 같은 추가 자질들을 사용하는 방법들이 있었지만, 이러한 방법들은 추론시, 단어의 자질들을 추출하는데 추가 시간이 필요한 단점이 있습니다.하지만 본 연구에서는 NER와 POS tasks를 학습시간에만 활용하고, 추론시에는 사용을 하지 않기 때문에 추론 시간을 단축시키는 효과가 있습니다. 그 결과, 아래의 표3과 같이 추론을 위해 추가적으로 요구되는 시간은 없다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/006.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표3. Auxiliary SL Tasks를 활용한 방식(Ours)과 활용하지 않은 방식(Ours w/o aux)간의 추론 속도 비교&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;5-향후-연구-계획&quot;&gt;5. 향후 연구 계획&lt;/h1&gt;

&lt;p&gt;해당 연구는 현재 헤이카카오앱 중 받아쓰기 기능에 적용되었고, 녹음 내용을 전사한 결과에 사족제거기능으로 활용되고 있습니다. 향후 한국어 서비스 고도화를 위해 한국어용 사족제거 모델 개선에 집중하고 유의미한 성능을 얻을 수 있도록 연구를 발전시켜나갈 계획입니다. 앞으로도 많은 관심 부탁드립니다. 감사합니다.&lt;/p&gt;</content><author><name>이동엽:카카오</name></author><category term="papers" /><category term="INTERSPEECH" /><category term="NLP" /><category term="Disfluency-detection" /><summary type="html">Abstract</summary></entry></feed>