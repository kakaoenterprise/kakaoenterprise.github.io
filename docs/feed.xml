<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://kakaoenterprise.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kakaoenterprise.github.io/" rel="alternate" type="text/html" /><updated>2022-05-19T21:07:44-05:00</updated><id>https://kakaoenterprise.github.io/feed.xml</id><title type="html">ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Research</title><subtitle>ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Labì—ì„œ ë°œí‘œí•œ AI ë…¼ë¬¸ê³¼ ì—°êµ¬ ì„±ê³¼ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.</subtitle><author><name>ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><entry><title type="html">Classification-based Multi-task Learning for Efficient Pose Estimation Network</title><link href="https://kakaoenterprise.github.io/papers/icpr-pose-estimation" rel="alternate" type="text/html" title="Classification-based Multi-task Learning for Efficient Pose Estimation Network" /><published>2022-08-21T00:00:00-05:00</published><updated>2022-08-21T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/ICPR-pose-estimation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icpr-pose-estimation">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Human pose estimation is an interesting and underlying topic in various fields such as action recognition and human-computer interaction. Although many methods have been developed recently, they are still far from perfect in accuracy and speed at a time. In this paper, we propose a Classification-based Pose Estimation Network with Multi-task Learning (CPENML) based on the low-resolution feature map to improve accuracy and inference time simultaneously. The proposed CPENML consists of two ideas. Firstly, novel proposed keypoint and offset estimation
tasks based on classification achieve better performance than regression. Secondly, the proposed Multi-Scale Network
(MSN) makes robust feature maps and balances the keypoint and offset tasks to maximize performance. To prove the effectiveness of the proposed method, we conduct ablation studies on the COCO dataset for proposed ideas. Compared to benchmarks, we demonstrate the superiority of our proposed method on COCO dataset in terms of inference time and accuracy.&lt;/p&gt;</content><author><name>benjamin:ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">ComDensE : Combined Dense Embedding of Relation-aware and Common Features for Knowledge Graph Completion</title><link href="https://kakaoenterprise.github.io/papers/icpr-comdense" rel="alternate" type="text/html" title="ComDensE : Combined Dense Embedding of Relation-aware and Common Features for Knowledge Graph Completion" /><published>2022-08-21T00:00:00-05:00</published><updated>2022-08-21T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/ICPR-ComDensE</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icpr-comdense">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Real-world knowledge graphs (KG) are mostly incomplete. The problem of recovering missing relations, called KG completion, has recently become an active research area. Knowledge graph (KG) embedding, a low-dimensional representation of entities and relations, is the crucial technique for KG completion. Convolutional neural networks in models such as ConvE, SACN, InteractE, and RGCN achieve recent successes. This paper takes a different architectural view and proposes ComDensE which combines relation-aware and common features using dense neural networks. In the relation-aware feature extraction, we attempt to create relational inductive bias by applying an encoding function specific to each relation. In the common feature extraction, we apply the common encoding function to all input embeddings. These encoding functions are implemented using dense layers in ComDensE. ComDensE achieves the state-of-the-art performance in the link prediction in terms of MRR, HIT@1 on FB15k-237 and HIT@1 on WN18RR compared to the previous baseline approaches. We conduct an extensive ablation study to examine the effects of the relation-aware layer and the common layer of the ComDensE. Experimental results illustrate that the combined dense architecture as implemented in ComDensE achieves the best performance.&lt;/p&gt;</content><author><name>lucas:ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">A Statistical Manifold Framework for Point Cloud Data</title><link href="https://kakaoenterprise.github.io/papers/icml-point-cloud-data" rel="alternate" type="text/html" title="A Statistical Manifold Framework for Point Cloud Data" /><published>2022-07-17T00:00:00-05:00</published><updated>2022-07-17T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/ICML-point-cloud-data</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icml-point-cloud-data">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Many problems in machine learning involve data sets in which each data point is a point cloud in R^D. A growing number of applications require a means of measuring not only distances between point clouds, but also angles, volumes, derivatives, and other more advanced concepts. To formulate and quantify these concepts in a coordinate-invariant way, we develop a Riemannian geometric framework for point cloud data. By interpreting each point in a point cloud as a sample drawn from some given underlying probability density, the space of point cloud data can be given the structure of a statistical manifold â€“ each point on this manifold represents a point cloud â€“ with the Fisher information metric acting as a natural Riemannian metric. Two autoencoder applications of our framework are presented: (i) smoothly deforming one 3D object into another via interpolation between the two corresponding point clouds; (ii) learning an optimal set of latent space coordinates for point cloud data that best preserves angles and distances, and thus produces a more discriminative representation space. Experiments with large-scale standard benchmark point cloud data show greatly improved classification accuracy vis-Â´a-vis existing methods.&lt;/p&gt;</content><author><name>ì´ìš©í˜„:ì„œìš¸ëŒ€</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Revisiting Interactive Recommender System with Reinforcement Learning</title><link href="https://kakaoenterprise.github.io/papers/sigir-rl-irs" rel="alternate" type="text/html" title="Revisiting Interactive Recommender System with Reinforcement Learning" /><published>2022-07-11T00:00:00-05:00</published><updated>2022-07-11T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/SIGIR-RL-IRS</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/sigir-rl-irs">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Interactive Recommender Systems (IRS) have drawn a lot of attention, due to their ability in modeling the interactive process between the user and the recommender system. Recently, numerous works have adopted Reinforcement Learning (RL) algorithms, which directly maximize the userâ€™s cumulative rewards, in IRS.
In IRS, researchers commonly utilize the publicly available review datasets to compare and evaluate the algorithms. However, the user feedbacks provided in the public datasets only include an instant response (e.g., rating), without any inclusion of delayed response (e.g., dwell-time, lifetime value). Thus, the question remains whether the review datasets are an appropriate choice to evaluate the long-term effects in IRS.&lt;br /&gt;
In this work, we revisit the experiments on the IRS with the review datasets and compare the RL-based models with a simple reward model that greedily recommends the item with the highest one-step reward. Through extensive experiments, we found the followings: First, a simple greedy reward model outperforms the RL-based models in maximizing the cumulative rewards. Second, applying more weights on long-term rewards degrades the recommendation performance. Third, recommended items have mere long-term effects in the benchmark datasets. From these findings, we conclude that a dataset must be carefully verified and a simple greedy baseline should be included for a proper evaluation in the RL-based IRS.&lt;/p&gt;</content><author><name>ì´í˜¸ì¤€:ì¹´ì´ìŠ¤íŠ¸</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">CoMPM: Context Modeling with Speakerâ€™s Pre-trained Memory Tracking for Emotion Recognition in Conversation</title><link href="https://kakaoenterprise.github.io/papers/naacl-compm" rel="alternate" type="text/html" title="CoMPM: Context Modeling with Speakerâ€™s Pre-trained Memory Tracking for Emotion Recognition in Conversation" /><published>2022-07-10T00:00:00-05:00</published><updated>2022-07-10T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/NAACL-CoMPM</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/naacl-compm">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;As the use of interactive machines grow, the task of Emotion Recognition in Conversation (ERC) became more important. If the machine-generated sentences reflect emotion, more human-like sympathetic conversations are possible. Since emotion recognition in conversation is inaccurate if the previous utterances are not taken into account, many studies reflect the dialogue context to improve the performances. Many recent approaches show performance improvement by combining knowledge into modules learned from external structured data. However, structured data is difficult to access in non-English languages, making it difficult to extend to other languages. Therefore, we extract the pre-trained memory using the pre-trained language model as an extractor of external knowledge. We introduce CoMPM, which combines the speakerâ€™s pre-trained memory with the context model, and find that the pre-trained memory significantly improves the performance of the context model. CoMPM achieves the first or second performance on all data and is state-of-the-art among systems that do not leverage structured data. In addition, our method shows that it can be extended to other languages because structured knowledge is not required, unlike previous methods.&lt;/p&gt;</content><author><name>rung:ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Contrastive Regularization for Semi-Supervised Learning</title><link href="https://kakaoenterprise.github.io/papers/cvpr-contrastive-regularization" rel="alternate" type="text/html" title="Contrastive Regularization for Semi-Supervised Learning" /><published>2022-06-20T00:00:00-05:00</published><updated>2022-06-20T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/cvpr-contrastive-regularization</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/cvpr-contrastive-regularization">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Consistency regularization on label predictions becomes a fundamental technique in semi-supervised learning, but it still requires a large number of training iterations for high performance. In this study, we analyze that the consistency regularization restricts the propagation of labeling information due to the exclusion of samples with unconfident pseudo-labels in the model updates. Then, we propose contrastive regularization to improve both efficiency and accuracy of the consistency regularization by well-clustered features of unlabeled data. In specific, after strongly augmented samples are assigned to clusters by their pseudo-labels, our contrastive regularization updates the model so that the features with confident pseudo-labels aggregate the features in the same cluster, while pushing away features in different clusters. As a result, the information of confident pseudo-labels can be effectively propagated into more unlabeled samples during training by the well-clustered features. On benchmarks of semi-supervised learning tasks, our contrastive regularization improves the previous consistency-based methods and achieves state-of-the-art results, especially with fewer training iterations. Our method also shows robust performance on open-set semi-supervised learning where unlabeled data includes out-of-distribution samples.&lt;/p&gt;</content><author><name>ì´ë„ì—½:POSTECH,ì¹´ì¹´ì˜¤ë¸Œë ˆì¸</name></author><category term="papers" /><category term="Machine_Learning" /><summary type="html">Abstract</summary></entry><entry><title type="html">Vacillating Human Correlation of SacreBLEU in Unprotected Languages</title><link href="https://kakaoenterprise.github.io/papers/humeval-meta-evaluation" rel="alternate" type="text/html" title="Vacillating Human Correlation of SacreBLEU in Unprotected Languages" /><published>2022-05-27T00:00:00-05:00</published><updated>2022-05-27T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/HumEval-meta-evaluation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/humeval-meta-evaluation">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;SacreBLEU, by incorporating a text normalizing step in the pipeline, has become a rising automatic evaluation metric in recent MT studies. With agglutinative languages such as Korean, however, the lexical-level metric cannot provide a conceivable result without a customized pre-tokenization. In this regard, this paper endeavors to examine the influence of diversified tokenization schemes â€“-word, morpheme, subword, character, and consonants &amp;amp; vowels (CV)â€“- on the metric, after its protective layer is peeled off.&lt;br /&gt;
By performing meta-evaluation with manually-constructed into-Korean resources, our empirical study demonstrates that the human correlation of the surface-based metric and other homogeneous ones (as an extension) vacillates greatly by the token type. Moreover, the human correlation of the metric often deteriorates due to some tokenization, with CV one of its culprits. Guiding through the proper usage of tokenizers for the given metric, we discover i) the feasibility of the character tokens, and ii) the deficit of CV in the Korean MT evaluation.&lt;/p&gt;</content><author><name>ria:ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Multimodal Interactions Using Pretrained Unimodal Models for SIMMC 2.0</title><link href="https://kakaoenterprise.github.io/papers/aaai-simmc" rel="alternate" type="text/html" title="Multimodal Interactions Using Pretrained Unimodal Models for SIMMC 2.0" /><published>2022-02-28T00:00:00-06:00</published><updated>2022-02-28T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/aaai-simmc</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/aaai-simmc">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;This paper presents our work on the Situated Interactive MultiModal Conversations 2.0 challenge held at Dialog State Tracking Challenge 10. SIMMC 2.0 includes 4 subtasks, and we introduce our multimodal approaches for the subtask #1, #2 and the generation of subtask #4. SIMMC 2.0 dataset is a multimodal dataset containing image and text information, which is more challenging than the problem of only text-based conversations because it must be solved by understanding the relationship between image and text. Therefore, since there is a limit to solving only text models such as BERT or GPT2, we propose a multimodal model combining image and text. We first pretrain the multimodal model to understand the relationship between image and text, then finetune our model for each task. We achieve the 3rd best performance in subtask #1, #2 and a runner-up in the generation of subtask #4. The source code is available at https://github.com/rungjoo/simmc2.0.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì€ ì§€ë‚œí•´ 10ì›” ê°œìµœëœ &lt;strong&gt;Situated Interactive MultiModal Conversations 2.0&lt;/strong&gt; (ì´í•˜ SIMMC 2.0) ì±Œë¦°ì§€ì— ì°¸ì—¬í•˜ì—¬ subtask #1ê³¼ #2ì—ì„œëŠ” &lt;strong&gt;3ìœ„&lt;/strong&gt;ë¥¼, #4ì—ì„œëŠ” &lt;strong&gt;2ìœ„&lt;/strong&gt;ë¥¼ ë‹¬ì„±í•˜ëŠ” ì„±ê³¼ë¥¼ ê±°ë‘ì—ˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì„ í†µí•´ ì±Œë¦°ì§€ ì°¸ì—¬ ê³¼ì •ì„ ì†Œê°œí•˜ê³ ì í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-ì±Œë¦°ì§€-ì†Œê°œ&quot;&gt;1. ì±Œë¦°ì§€ ì†Œê°œ&lt;/h1&gt;

&lt;p&gt;ë¨¼ì € í•´ë‹¹ ì±Œë¦°ì§€ì— ëŒ€í•´ ì§§ê²Œ ì†Œê°œë“œë¦¬ë ¤ê³  í•©ë‹ˆë‹¤. ì§€ë‚œí•´ 2íšŒì§¸ë¥¼ ë§ì€ SIMMC 2.0ëŠ” AI ëŒ€í™” ì‹œìŠ¤í…œ ë¶„ì•¼ì˜ ëŒ€í‘œì ì¸ êµ­ì œ ê²½ì§„ëŒ€íšŒì¸ &lt;strong&gt;DSTC(Dialog State Tracking Challenge)10&lt;/strong&gt;ì„ í†µí•´ ê°œìµœë˜ì—ˆìŠµë‹ˆë‹¤.
ëŒ€íšŒ ì£¼ì œëŠ” ë°”ë¡œ, ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ì„ í™œìš©í•´ ì‹¤ìƒí™œì— ì“°ì¼ ìˆ˜ ìˆëŠ” ì–´ì‹œìŠ¤í„´íŠ¸(assistant) ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ì—ˆëŠ”ë°ìš”. ì£¼ì–´ì§„ ë°ì´í„°ì…‹ì€ ì‡¼í•‘ ë„ë©”ì¸ì— íŠ¹í™”ëœ ëª©ì ì§€í–¥ ëŒ€í™”(task-oriented dialog) ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆì—ˆê³ , ì´ ë°ì´í„°ì…‹ì„ í™œìš©í•´ ì‚¬ìš©ìê°€ ì˜ë¥˜ ë˜ëŠ” ê°€êµ¬ë¥¼ ì‡¼í•‘í•˜ëŠ” ìƒí™©ì—ì„œ AI ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ì–¼ë§ˆë‚˜ ëŒ€í™” ë§¥ë½ì— ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ê³¼ì œë“¤ì´ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-ê³¼ì œ-ì†Œê°œ&quot;&gt;2. ê³¼ì œ ì†Œê°œ&lt;/h1&gt;

&lt;p&gt;ì—°êµ¬íŒ€ì€ SIMMC 2.0ì—ì„œ ì£¼ì–´ì§„ 4ê°€ì§€ ê³¼ì œ ì¤‘, subtask #1, #2ì™€ #4ì— ì°¸ì—¬í•˜ì˜€ìŠµë‹ˆë‹¤. ê° subtaskë§ˆë‹¤ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì™€ ëª¨ë¸ë§ì— ìˆì–´ ì°¨ì´ê°€ ìˆì—ˆê³ , í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ ê³¼ì •ì—ì„œ í™œìš© ê°€ëŠ¥í•œ ë°ì´í„°ì—ì„œë„ ì œí•œì´ ìˆì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ëª¨ë“  í…ŒìŠ¤íŠ¸ ê³¼ì •ì—ì„œ ê°ì²´ì˜ ì‹œê°ì  ë©”íƒ€ë°ì´í„°, ë¼ë²¨ë§ëœ ì‚¬ìš©ì ì •ë³´, ëŒ€í™”ì— ì–¸ê¸‰ëœ ëª¨ë“  ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì •ë³´ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ì—ˆëŠ”ë°ìš”. ë‹¤ë§Œ, ê°ì²´ì˜ ë¹„ì‹œê°ì  ë©”íƒ€ë°ì´í„°, ì‹œìŠ¤í…œ ë°œí™”ì—ì„œ ì–¸ê¸‰ëœ ê°ì²´ ë¦¬ìŠ¤íŠ¸, ëŒ€í™”ì— í•´ë‹¹í•˜ëŠ” ë°°ê²½ì´ë¯¸ì§€, ëª¨ë“  ê°ì²´ì˜ ê²½ê³„ ë°•ìŠ¤(bounding box) ë°ì´í„°ëŠ” ëª¨ë‘ í™œìš© ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤. ì°¸ê³ ë¡œ, ì—¬ê¸°ì„œ ê°ì²´ëŠ” ì˜ë¥˜ ë„ë©”ì¸ ìƒì—ì„œ ì˜ë¥˜ ì´ë¯¸ì§€ì— í•´ë‹¹ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì¢€ ë” ìì„¸íˆ ê° ê³¼ì œ ë‚´ìš©ì„ ì‚´í´ë³´ë©´, &lt;strong&gt;#1 Multimodal Disambiguation&lt;/strong&gt;ì€ ë°°ê²½ ì „ì²´ ì´ë¯¸ì§€ì™€ ëŒ€í™” ë§¥ë½ì´ ì£¼ì–´ì§ˆ ë•Œ, ì´ ìƒí™©ì—ì„œ ë§ˆì§€ë§‰ ë°œí™”ì˜ ëª…í™•ì„±ì„ íŒë‹¨(True/False)í•˜ëŠ” ê³¼ì œì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì˜¤ë¥¸ìª½ ì˜·ê±¸ì´ì— íŒŒë€ìƒ‰ ì˜· 3ê°€ì§€ê°€ ê±¸ë ¤ìˆë‹¤ê³  ê°€ì •í–ˆì„ ë•Œ, ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë°œí™”ê°€ ì´ë£¨ì–´ì§„ë‹¤ë©´ ì–´ë–¤ ì˜·ì„ ê°€ë¥´í‚¤ëŠ”ì§€ ëª…í™•íˆ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ë¶ˆëª…í™•í•¨ì„ ì˜ë¯¸í•˜ëŠ” Falseë¥¼ ë„ì¶œí•˜ê²Œ ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;ë°œí™”1 : íŒŒë€ìƒ‰ ì˜·ì´ ì–´ë”” ìˆë‚˜ìš”?&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;ë°œí™”2 : ì €ê¸° ì˜¤ë¥¸ìª½ ìœ„ì— ìˆìŠµë‹ˆë‹¤.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ë‹¤ìŒìœ¼ë¡œ &lt;strong&gt;#2 Multimodal Coreference Resolution&lt;/strong&gt;ì€ ì‚¬ìš©ìì˜ ë°œí™”ì— ì–¸ê¸‰ëœ ê°ì²´ë¥¼ ì°¾ëŠ” ê³¼ì œë¡œ, #1 ê³¼ì œì™€ ë™ì¼í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©ì´ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤. ì´ì–´ &lt;strong&gt;#4 Multimodal Dialog Response Generation &amp;amp; Retrieval&lt;/strong&gt;ì—ì„œëŠ” ì‚¬ìš©ì ë°œí™”ì— ì ì ˆí•œ ì‹œìŠ¤í…œ ì‘ë‹µì„ ìƒì„±í•˜ê±°ë‚˜ ê²€ìƒ‰í•˜ëŠ” ê³¼ì œì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-ê³¼ì œ-í•´ê²°ê³¼ì •&quot;&gt;3. ê³¼ì œ í•´ê²°ê³¼ì •&lt;/h1&gt;

&lt;p&gt;ì—°êµ¬íŒ€ì€ ê³¼ì œ í•´ê²°ì„ ìœ„í•´, ê¸°ì¡´ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ BERTì™€ GPT2ì—ì„œ ë” ë‚˜ì•„ê°„ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ìƒˆë¡­ê²Œ ê³ ì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤. ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ì€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ê³  ìˆì–´, í…ìŠ¤íŠ¸ë¡œë§Œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ë³´ë‹¤ ì–´ë ¤ìš´ ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆëŠ”ë°ìš”. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ê´€ê³„ë¥¼ ë¯¸ë¦¬ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë©€í‹°ëª¨ë‹¬ ì‚¬ì „í•™ìŠµ(pre-training) ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤.
ì‚¬ì „í•™ìŠµì—ëŠ” ê·¸ë¦¼1ê³¼ ê°™ì´ ì´ 2ê°€ì§€ ëª¨ë¸ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ë‚˜ëŠ” ê°ì²´ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì„¤ëª…(description)ì´ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨í•˜ëŠ” ëª¨ë¸(ITM)ì´ì—ˆê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë°°ê²½ ì´ë¯¸ì§€ì™€ ëŒ€í™” ë§¥ë½(context)ì´ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨í•˜ëŠ” ëª¨ë¸(BTM)ì´ì—ˆìŠµë‹ˆë‹¤. ë‘ ê°€ì§€ ëª¨ë¸ì„ ê²°í•©í•œ ë’¤ ì´ë¥¼ ê° íƒœìŠ¤í¬ì— ë§ì¶° ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, í•˜ë‚˜ì˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì „ì²´ ê³¼ì œë¥¼ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-SIMMC/001.png&quot; width=&quot;90%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼1. ë©€í‹°ëª¨ë‹¬ ì‚¬ì „í•™ìŠµ ê³¼ì •&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ì „ì²´ì ì¸ ëª¨ë¸ êµ¬ì¡°ëŠ” ê·¸ë¦¼2ì™€ ê°™ì€ë°ìš”. ì•ì„œ ê·¸ë¦¼1ì—ì„œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ë“¤ì„ ëª¨ë“ˆí™”í•´ ì „ì²´ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-SIMMC/002.png&quot; width=&quot;90%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼2. ì „ì²´ ëª¨ë¸ êµ¬ì¡°&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ê° êµ¬ì¡°ë¥¼ ì‚´í´ë³´ë©´ &lt;strong&gt;subtask #1&lt;/strong&gt;ì—ì„œëŠ” ë°œí™”ì˜ ëª…í™•ì„±ì„ íŒë‹¨í•˜ê¸° ìœ„í•´ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ê°€ ë‹´ê¸´ ë©€í‹°ëª¨ë‹¬ ì‚¬ì „í•™ìŠµì´ ë˜ì§€ ì•Šì€ RoBERTaì— ì‚¬ì „í•™ìŠµëœ DeIT-Iì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì´ë¯¸ì§€ì—ì„œ ì˜ë¥˜ í˜•íƒœë§Œ í¬ë¡­í•œ í›„, ê°ì²´ ì„¤ëª…ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ íŒë‹¨í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;subtask #2&lt;/strong&gt;ì—ì„œëŠ” #1ê³¼ ë°ì´í„°ëŠ” ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ì˜€ì§€ë§Œ, ì‚¬ìš©ìì˜ ë°œí™”ì— ì–¸ê¸‰ëœ ê°ì²´ë¥¼ ì°¾ê¸° ìœ„í•´ ëŒ€í™”ë§¥ë½, ê°ì²´, ë°°ê²½ ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•˜ëŠ” êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë¨¼ì € context featureë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´, ê·¸ë¦¼3ê³¼ ê°™ì€ êµ¬ì¡° í•˜ì— ë‘ ê°€ì§€ ë©€í‹°íƒœìŠ¤í¬ ëŸ¬ë‹ì„ ì¶”ê°€ë¡œ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. 1ë‹¨ê³„(utterance classification)ì—ì„œëŠ” ë§¤ì¹­ íŒë‹¨ì— ìœ ì˜ë¯¸í•˜ì§€ ì•Šì€ ë°œí™”ë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ ë°œí™”ì™€ ê°ì²´ì˜ ì¼ì¹˜ì„±ì„ íŒë‹¨í•˜ê³ , 2ë‹¨ê³„(system matching)ì—ì„œëŠ” ì´ ê°ì²´ë¥¼ ì´ì „ ì‹œìŠ¤í…œ ë°œí™”ì— í•´ë‹¹í•˜ëŠ” &lt;code&gt;object_ids&lt;/code&gt;ì— ë§¤ì¹­í•˜ì˜€ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë°ì´í„°ì— ë”°ë¼ ì¶”ë¡ ê³¼ì •ë„ ì¡°ê¸ˆ ë‹¬ë¼ì§€ê²Œ ë˜ëŠ”ë°ìš”. í•™ìŠµ ë°ì´í„°ë¡œ ì–´ë–¤ &lt;code&gt;mention_objects&lt;/code&gt;ë¥¼ ì‚¬ìš©í•˜ëƒì— ë”°ë¼ ëª¨ë¸ì˜ ê°•ì ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì´ì— ë”°ë¼ ì´ì „ ë°œí™”ì— ì–¸ê¸‰ëë˜ &lt;code&gt;related objects&lt;/code&gt;ëŠ” 1 ë˜ëŠ” matchingìœ¼ë¡œ, ê´€ë ¨ì´ ì—†ëŠ” &lt;code&gt;unrelated objects&lt;/code&gt;ëŠ” 0ìœ¼ë¡œ, ì´ì™¸ ë‚˜ë¨¸ì§€ë¥¼ ì˜ë¯¸í•˜ëŠ” &lt;code&gt;others&lt;/code&gt;ëŠ” 0 ë˜ëŠ” matchingìœ¼ë¡œ ê²°ê³¼ê°’ì„ ë„ì¶œí•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-SIMMC/003.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼3. ì£¼ì–´ì§„ ë°œí™”ì—ì„œ ê°ì²´ì˜ í¬í•¨ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ë¡œì§ (two multi-task learning)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ì˜ˆë¥¼ ë“¤ì–´ ê·¸ë¦¼4ì™€ ê°™ì´ ìœ ì €ì™€ ì‹œìŠ¤í…œì˜ ë°œí™”ìŒì´ ìˆë‹¤ë©´, ì‹œìŠ¤í…œ 1ê³¼ ì‹œìŠ¤í…œ 2ì˜ system matching ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ê±´ë°ìš”. Case1ì—ì„œëŠ” &lt;strong&gt;â€œíŒŒë€ìƒ‰ ì˜·â€&lt;/strong&gt;ìœ¼ë¡œ ì¼ì¹˜í•˜ê¸° ë•Œë¬¸ì— 1ì´, Case2ì—ì„œëŠ” &lt;strong&gt;â€œê²€ì€ìƒ‰ê³¼ ë¹¨ê°„ìƒ‰ ë°”ì§€â€&lt;/strong&gt;, &lt;strong&gt;â€œíŒŒë€ìƒ‰ ì˜·â€&lt;/strong&gt;ìœ¼ë¡œ ì¼ì¹˜í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— 0ì´ ë„ì¶œë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-SIMMC/004.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼4. ê·¸ë¦¼3ì˜ ë¡œì§ íŒë‹¨ ì˜ˆì‹œ (ìœ ì € ë°œí™”:íŒŒë€ìƒ‰, ì‹œìŠ¤í…œ ë°œí™”:ë…¸ë€ìƒ‰)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ì´ì²˜ëŸ¼ ëŒ€í™” íŠ¹ì„±ìƒ ì•ì„œ ì–¸ê¸‰ëœ ë‹¨ì–´ê°€ ë’¤ì—ë„ ë™ì¼í•˜ê²Œ ì–¸ê¸‰ë  ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì—, ìµœì¢…ì ìœ¼ë¡œ system matchingì´ 1ë¡œ íŒë³„ëœ ì‹œìŠ¤í…œ ë°œí™”ì˜ objectsë§Œì„ ì‚¬ìš©í•˜ì—¬ ê°ì²´ì˜ í›„ë³´êµ°ì„ ì¶•ì†Œì‹œí‚¤ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì´ ì •ë³´ì— object featureì™€ background featureë¥¼ ì¶”ì¶œí•˜ëŠ” DeIT-Iì™€ DeIT-B ê°’ì„ ë§¤ì¹­í•´ ìµœì¢…ì ìœ¼ë¡œ ë°œí™”ì— ì–¸ê¸‰ëœ ê°ì²´ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë§ˆì§€ë§‰ìœ¼ë¡œ, ì‚¬ìš©ì ë°œí™”ì— ì ì ˆí•œ ì‹œìŠ¤í…œ ì‘ë‹µì„ ìƒì„±í•˜ê±°ë‚˜ ê²€ìƒ‰í•˜ëŠ” &lt;strong&gt;subtask #4&lt;/strong&gt;ì—ì„œëŠ” ì‘ë‹µ ìƒì„±ë§Œì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” í…ìŠ¤íŠ¸ ëª¨ë¸ë¡œ GPT2-Largeë¥¼ í™œìš©í•˜ì—¬ ì „ì²´ ë°œí™”ë¥¼ ì…ë ¥í•˜ì˜€ê³ , ì‹œìŠ¤í…œì´ ë§í•  ë‹¤ìŒ ë°œí™”ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ ì •ë³´(slot values)ëŠ”  DeIT-Ië¥¼ ì‚¬ìš©í•˜ì—¬ object featureë¥¼ ì¶”ì¶œí•´ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íƒœìŠ¤í¬ì™€ ë‹¬ë¦¬, ì´ë²ˆ íƒœìŠ¤í¬ì—ì„œëŠ” í˜„ì¬ ìˆœì„œì— ëŒ€í•œ ì£¼ì„ ì •ë³´(system_transcript_annnotated)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-ì±Œë¦°ì§€-ì„±ê³¼-ì†Œê°œ&quot;&gt;4. ì±Œë¦°ì§€ ì„±ê³¼ ì†Œê°œ&lt;/h1&gt;

&lt;p&gt;í•™ìŠµì—ëŠ” hugingface libraryë¥¼ ì‚¬ìš©í•˜ì˜€ê³ , ê²°ê³¼ê°’ì€ ì•„ë˜ í‘œì™€ ê°™ìŠµë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ ë² ì´ìŠ¤ëª¨ë¸ì¸ GPT2 ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ê·¸ê²°ê³¼ ìµœì¢…ì ìœ¼ë¡œ subtask #1, #2ì—ì„œ ê°ê° 3ìœ„, #4 ìƒì„± ë¶„ì•¼ì—ì„œëŠ” ì¤€ìš°ìŠ¹ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-SIMMC/005.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ1. subtask #1 ì„±ëŠ¥ ë¹„êµ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-SIMMC/006.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ2. subtask #2 ì„±ëŠ¥ ë¹„êµ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-SIMMC/007.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ3. subtask #4 ì„±ëŠ¥ ë¹„êµ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;5-ë§ˆì¹˜ë©°&quot;&gt;5. ë§ˆì¹˜ë©°&lt;/h1&gt;

&lt;p&gt;ì±Œë¦°ì§€ì— ì°¸ì—¬í•œ ìƒì„¸í•œ ì†ŒìŠ¤ ì½”ë“œëŠ” &lt;a href=&quot;https://github.com/rungjoo/simmc2.0&quot;&gt;ê¹ƒí—ˆë¸Œ&lt;/a&gt;ë¥¼ í†µí•´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì€ ë³´ë‹¤ ì‚¬ëŒê°™ì€ ì±—ë´‡ ì„œë¹„ìŠ¤ êµ¬í˜„ì„ ìœ„í•´, ì´ë²ˆ ì—°êµ¬ê²°ê³¼ë¥¼ ì ê·¹ í™œìš©í•  ê³„íšì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì˜ AI ì—°êµ¬ì— ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦¬ë©°, ìŠ¤ëª°í†¡ ì±—ë´‡ â€˜ì™¸ê°œì¸ì•„ê°€â€™ì™€ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”!&lt;/p&gt;

&lt;p&gt;ğŸ¶ &lt;a href=&quot;https://pf.kakao.com/_lKxoMT&quot;&gt;ì™¸ê°œì¸ì•„ê°€ ë§Œë‚˜ëŸ¬ê°€ê¸°&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ğŸ‘¨ğŸ»â€ğŸ’» &lt;a href=&quot;http://kko.to/ailab_career&quot;&gt;ì¸ì¬ì˜ì…&lt;/a&gt;&lt;/p&gt;</content><author><name>rung:ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Proxyless Neural Architecture Adaptation for Supervised Learning and Self-Supervised Learning</title><link href="https://kakaoenterprise.github.io/papers/aaai-pnaa" rel="alternate" type="text/html" title="Proxyless Neural Architecture Adaptation for Supervised Learning and Self-Supervised Learning" /><published>2022-02-28T00:00:00-06:00</published><updated>2022-02-28T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/aaai-pnaa</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/aaai-pnaa">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Recently, Neural Architecture Search (NAS) methods are introduced and show impressive performance on many benchmarks.
Among those NAS studies, Neural Architecture Transformer (NAT) aims to adapt the given neural architecture to improve performance while maintaining computational costs.
However, NAT lacks reproducibility and it requires an additional architecture adaptation process before network weight training.
In this paper, we propose proxyless neural architecture adaptation that is reproducible and efficient.
Our method can be applied to both supervised learning and self-supervised learning.
The proposed method shows stable performance on various architectures.
Extensive reproducibility experiments on two datasets, i.e., CIFAR-10 and Tiny Imagenet, present that the proposed method definitely outperforms NAT and be applicable to other models and datasets.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ë³¸ ê¸€ì—ì„œëŠ” ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì™€ ì¸í•˜ëŒ€ ê³µë™ ì—°êµ¬íŒ€ì´ ì—°ì‚° ìì›ì„ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê¸°ì¡´ NASì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê³ ì, ìƒˆë¡­ê²Œ ì œì•ˆí•œ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ì†Œê°œë“œë¦¬ë ¤ê³  í•©ë‹ˆë‹¤. í•´ë‹¹ ì—°êµ¬ ê²°ê³¼ëŠ” AAAI 2022 í•™íšŒì—ì„œ Workshopìœ¼ë¡œ ê°œìµœëœ &lt;strong&gt;Learning Network Architecture during Training&lt;/strong&gt; ì„ í†µí•´ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-nasneural-architecture-searchì˜-ë“±ì¥&quot;&gt;1. NAS(Neural Architecture Search)ì˜ ë“±ì¥&lt;/h1&gt;

&lt;p&gt;ì¼ë°˜ì ìœ¼ë¡œ ë”¥ëŸ¬ë‹ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ì–»ê¸° ìœ„í•´ì„œëŠ” ì£¼ì–´ì§„ taskì™€ ë°ì´í„°ì…‹ì— ìµœì í™”ëœ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì‚¬ëŒì´ ì§ì ‘ ê° ë ˆì´ì–´ì™€ í•„í„° ê°œìˆ˜ ë“± ì—¬ëŸ¬ ì„¤ì •ì„ ì¼ì¼ì´ ë¯¸ì„¸ì¡°ì •í•˜ê³  ì„¤ê³„í•˜ëŠ” ê³¼ì •ì„ ê±°ì¹˜ëŠ”ë°ìš”. ìµœì í™”ëœ ëª¨ë¸ êµ¬ì¡°ëŠ” ê° taskì™€ ë°ì´í„°ì…‹ì— ë”°ë¼ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì—, í•´ë‹¹ êµ¬ì¡°ì˜ ì„±ëŠ¥ì€ ì‹¤ì œ í•™ìŠµì„ ì§„í–‰í•œ ë’¤ ê·¸ ê²°ê³¼ë¡œë§Œ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë°”ë¡œ ì´ëŸ¬í•œ ë¶ˆí¸í•¨ì„ ê°œì„ í•˜ê³ ì ë“±ì¥í•œ ì—°êµ¬ ë¶„ì•¼ê°€ &lt;strong&gt;NAS(Neural Architecture Search)&lt;/strong&gt; ì…ë‹ˆë‹¤. NASëŠ” ìë™í™”ë¥¼ í†µí•´ ì£¼ì–´ì§„ taskì— ìµœì í™”ëœ ëª¨ë¸ êµ¬ì¡°ë¥¼ í¸ë¦¬í•˜ê³  ë¹ ë¥´ê²Œ íƒìƒ‰í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ, ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ëˆˆì— ë„ëŠ” ìš°ìˆ˜í•œ ì—°êµ¬ì„±ê³¼ë“¤ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì„œ ë” ë‚˜ì•„ê°€, ìµœê·¼ì—ëŠ” NASì˜ ì´ì ì€ ìœ ì§€í•˜ë©´ì„œ ì—°ì‚°ë¹„ìš©ì„ ì¤„ì¸ ì—¬ëŸ¬ ì—°êµ¬ê°€ ì£¼ëª©ë°›ê³  ìˆëŠ”ë°ìš”. ê·¸ ì¤‘ í•˜ë‚˜ë¡œëŠ”, ê¸°ì¡´ì— ë°©ëŒ€í•œ ì•„í‚¤í…ì²˜ í›„ë³´êµ°(Search Space)ì„ ì•„ì£¼ ì‘ê²Œ ì¤„ì—¬ì„œ ìµœì ì˜ ì•„í‚¤í…ì²˜ë¥¼ ì°¾ëŠ” &lt;strong&gt;NAT(Neural Architecture Transformer)&lt;/strong&gt; ë°©ì‹ì´ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, NATì€ ì•Œê³ ë¦¬ì¦˜ì˜ ì¬í˜„ì„±(reproducibility)ì´ ë–¨ì–´ì§€ê³ , ë™ì¼í•œ ì…€ ì•„í‚¤í…ì²˜ êµ¬ì¡° í•˜ì—ì„œë§Œ íƒìƒ‰ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆëŠ”ë°ìš”.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-proxyless-neural-architecture-adaption-ë°©ë²•ë¡ -ì†Œê°œ&quot;&gt;2. Proxyless Neural Architecture Adaption ë°©ë²•ë¡  ì†Œê°œ&lt;/h1&gt;

&lt;p&gt;ë³¸ ì—°êµ¬ì—ì„œëŠ” NATì˜ ë‹¨ì ì„ ê°œì„ í•œ &lt;strong&gt;Proxyless Neural Architecture Adaption&lt;/strong&gt; ë°©ë²•ë¡ ì„ ìƒˆë¡­ê²Œ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ ë°©ë²•ë¡ ì€ NATê³¼ ë¹„êµí•´ ì¬í˜„ì„±ì´ ë†’ê³ , íš¨ìœ¨ì ì´ë¼ëŠ” ì ì´ íŠ¹ì§•ì¸ë°ìš”. NATì—ì„œëŠ” ì¶”ê°€ì ì¸ ì•„í‚¤í…ì²˜ ì„œì¹˜ ê³¼ì •ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì‹œê°„ê³¼ GPU ìì› ì†Œëª¨ê°€ í°ë° ë°˜í•´, ë³¸ ë°©ë²•ë¡ ì€ ì•„í‚¤í…ì²˜ ì„œì¹˜ì™€ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ì§„í–‰í•˜ì—¬ ìì›ì„ í¬ê²Œ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜í•œ, í•˜ë‚˜ì˜ ì…€(cell) ë‹¨ìœ„ê°€ ì•„ë‹Œ ë‹¤ì–‘í•œ ì…€ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§„ ë§¤í¬ë¡œë¸”ë¡(macroblock) ê¸°ë°˜ íƒìƒ‰ìœ¼ë¡œ ì „ì²´ ë²”ìœ„ë¥¼ íƒìƒ‰í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ë¿ë§Œ ì•„ë‹ˆë¼, ì§€ë„í•™ìŠµ(Supervised Learning)ê³¼ ìê¸°ì§€ë„í•™ìŠµ(Self-Supervised Learning)ì— ëª¨ë‘ ì ìš©ë  ìˆ˜ ìˆì–´ í™œìš©ë„ê°€ ë†’ì€ë°ìš”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-PNAA/001.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼1. Proxyless Neural Architecture Adaption ë°©ë²•ë¡ ì„ ì ìš©í•œ ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ ê²€ìƒ‰ êµ¬ì¡°&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-ì„±ëŠ¥-ë¹„êµ&quot;&gt;3. ì„±ëŠ¥ ë¹„êµ&lt;/h1&gt;

&lt;p&gt;ì´ ë°©ë²•ë¡ ì˜ ì„±ëŠ¥ê³¼ ê´‘ë²”ìœ„í•œ ì¬í˜„ì„±ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ &lt;strong&gt;CIFAR-10&lt;/strong&gt;ê³¼ &lt;strong&gt;Tiny Imagenet&lt;/strong&gt; ë°ì´í„°ì…‹ì— ì—¬ëŸ¬ê°€ì§€ ëª¨ë¸ë¡œ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë¨¼ì € ì§€ë„í•™ìŠµ í™˜ê²½ì—ì„œ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì„¤ê³„ëœ Resnet20ê³¼ MobilentV2, NAS ëª¨ë¸ì¸ DARTSì™€ Proxyless NAS ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°©ì‹ê³¼ NAT, ë³¸ ë°©ë²•ë¡ ìœ¼ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•œ ê²°ê³¼, [í‘œ1]ê³¼ ê°™ì´ ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ì—ì„œ ê¸°ì¡´ ë°©ë²•ë¡  ëŒ€ë¹„ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë”ë¶ˆì–´ ì „ì²´ì ì¸ ì—°ì‚°ë¹„ìš© ì¸¡ë©´ì—ì„œë„ NATê³¼ ë¹„êµí•´ ë” ì ì€ ë¹„ìš©ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-PNAA/002.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ1. ì§€ë„í•™ìŠµì—ì„œì˜ í‰ê·  ì •í™•ë„, í‘œì¤€í¸ì°¨, ì—°ì‚°ì‹œê°„ ë¹„êµ (CIFAR-10 ê¸°ì¤€)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;í‘œ2ì—ì„œëŠ” ë§ˆì°¬ê°€ì§€ë¡œ CIFAR-10 ë°ì´í„°ì…‹ ìƒì—ì„œ 5ë²ˆì˜ ë¬´ì‘ìœ„ ì‹œë„ë¥¼ ê±°ì³ ì¬í˜„ì„±ì„ í…ŒìŠ¤íŠ¸ ì§„í–‰í•˜ì˜€ê³ , ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-PNAA/003.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ2. ì§€ë„í•™ìŠµì—ì„œì˜ ì¬í˜„ì„± ë¹„êµ (CIFAR-10 ê¸°ì¤€)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-PNAA/004.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ3. ì§€ë„í•™ìŠµì—ì„œì˜ í‰ê·  ì •í™•ë„, í‘œì¤€í¸ì°¨, ì—°ì‚°ì‹œê°„ ë¹„êµ (Tiny Imagenet ê¸°ì¤€)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ë˜í•œ, ìê¸°ì§€ë„í•™ìŠµ í™˜ê²½ì—ì„œë„ ì„±ëŠ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´ NASëª¨ë¸ì¸ DARTSì™€ Proxyless NASì— ì¶”ê°€ì ì¸ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ì˜€ê³ , ì—¬ê¸°ì—ì„œë„ ê¸°ì¡´ ë°©ì‹ ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-28-AAAI-PNAA/005.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ4. ìê¸°ì§€ë„í•™ìŠµì—ì„œì˜ ì •í™•ë„ ë¹„êµ (CIFAR-10 ê¸°ì¤€)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-í–¥í›„-ì—°êµ¬-ê³„íš&quot;&gt;4. í–¥í›„ ì—°êµ¬ ê³„íš&lt;/h1&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì€ í•´ë‹¹ ë°©ë²•ë¡ ì„ í™œìš©í•˜ì—¬ ì •í™•ë„ë¥¼ ë„˜ì–´, ê²°ê³¼ê°€ ë„ì¶œë˜ëŠ” ì‹œê°„, ì†ë„(latency)ê¹Œì§€ ê³ ë ¤í•œ ëª¨ë¸ì„ ë§Œë“¤ê³ ì í•©ë‹ˆë‹¤. íŠ¹íˆ ì»´í“¨í„°ë¹„ì „ ë¶„ì•¼ ì—°êµ¬ì— ì ìš©í•´ ìµœì ì˜ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë¹ ë¥´ê³ , ì €ë¹„ìš©ìœ¼ë¡œ íƒìƒ‰í•˜ëŠ” ë°ì— ì¤‘ì ì„ ë‘˜ ê³„íšì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì•ìœ¼ë¡œë„ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì˜ AI ì—°êµ¬ì— ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦¬ë©°, &lt;strong&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Lab &amp;amp; Service&lt;/strong&gt;ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”!&lt;/p&gt;

&lt;p&gt;ğŸ‘¨ğŸ»â€ğŸ’» &lt;a href=&quot;http://kko.to/ailab_career&quot;&gt;ì¸ì¬ì˜ì…&lt;/a&gt;&lt;/p&gt;</content><author><name>ê¹€ë„êµ­:ì¸í•˜ëŒ€í•™êµ</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets</title><link href="https://kakaoenterprise.github.io/papers/arxiv-apeach" rel="alternate" type="text/html" title="APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets" /><published>2022-02-25T00:00:00-06:00</published><updated>2022-02-25T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/arxiv-apeach</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/arxiv-apeach">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Detecting toxic or pejorative expressions in online communities has become one of the main concerns for preventing the usersâ€™ men- tal harm. This led to the development of large- scale hate speech detection datasets of var- ious domains, which are mainly built upon web-crawled texts with labels by crowdwork- ers. However, for languages other than English, researchers might have to rely on only a small- sized corpus due to the lack of data-driven re- search of hate speech detection. This some- times misleads the evaluation of prevalently used pretrained language models (PLMs) such as BERT, given that PLMs often share the do- main of pretraining corpus with the evaluation set, resulting in over-representation of the de- tection performance. Also, the scope of pejo- rative expressions might be restricted if the dataset is built on a single domain text.&lt;/p&gt;

&lt;p&gt;To alleviate the above problems in Korean hate speech detection, we propose APEACH, a method that allows the collection of hate speech generated by unspecified users. By con- trolling the crowd-generation of hate speech and adding only a minimum post-labeling, we create a corpus that enables the general- izable and fair evaluation of hate speech de- tection regarding text domain and topic. We compare our outcome with prior work on an annotation-based toxic news comment dataset using publicly available PLMs. We check that our dataset is less sensitive to the lexical over- lap between the evaluation set and pretraining corpus of PLMs, showing that it helps mitigate the unexpected under/over-representation of model performance. We distribute our dataset publicly online to further facilitate the general- domain hate speech detection in Korean.&lt;/p&gt;</content><author><name>ì–‘ê¸°ì°½:ì¹´ì¹´ì˜¤, ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ, ìˆ­ì‹¤ëŒ€</name></author><category term="papers" /><summary type="html">Abstract</summary></entry></feed>