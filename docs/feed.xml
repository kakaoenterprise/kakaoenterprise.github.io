<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://kakaoenterprise.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kakaoenterprise.github.io/" rel="alternate" type="text/html" /><updated>2021-09-30T03:34:58-05:00</updated><id>https://kakaoenterprise.github.io/feed.xml</id><title type="html">카카오엔터프라이즈 AI Research</title><subtitle>카카오엔터프라이즈 AI Lab에서 발표한 AI 논문과 연구 성과를 소개합니다.</subtitle><author><name>카카오엔터프라이즈</name></author><entry><title type="html">SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustness</title><link href="https://kakaoenterprise.github.io/papers/neurips-smoothmix" rel="alternate" type="text/html" title="SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustness" /><published>2021-12-06T00:00:00-06:00</published><updated>2021-12-06T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/neurips-smoothmix</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/neurips-smoothmix">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Randomized smoothing is currently a state-of-the-art method to construct a certifiably robust classifier from neural networks against l2-adversarial perturbations. Under the paradigm, the robustness of a classifier is aligned with the prediction confidence, i.e., the higher confidence from a smoothed classifier implies the better robustness. This motivates us to rethink the fundamental trade-off between accuracy and robustness in terms of calibrating confidences of smoothed classifier. In this paper, we propose a simple training scheme, coined SmoothMix, to control the robustness of smoothed classifiers via self-mixup: it trains convex combinations of samples along the direction of adversarial perturbation for each input. The proposed procedure effectively identifies over-confident, near off-class samples as a cause of limited robustness in case of smoothed classifiers, and offers an intuitive way to adaptively set a new decision boundary between these samples for better robustness. Our experimental results demonstrate that the proposed method can significantly improve the certified l2-robustness of smoothed classifiers compared to existing state-of-the-art robust training methods.&lt;/p&gt;</content><author><name>정종현:카이스트</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Kakao Enterprise’s WMT21 Machine Translation using Terminologies Task Submission</title><link href="https://kakaoenterprise.github.io/papers/wmt21-terminology-translation" rel="alternate" type="text/html" title="Kakao Enterprise’s WMT21 Machine Translation using Terminologies Task Submission" /><published>2021-11-19T00:00:00-06:00</published><updated>2021-11-19T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/wmt21-terminology-translation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/wmt21-terminology-translation">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;This paper describes Kakao Enterprise’s submission to the WMT21 shared Machine Translation using Terminologies task. We integrate terminology constraints by pre-training with target lemma annotations and fine-tuning with exact target annotations utilizing the given terminology dataset. This approach yields a model that achieves outstanding results in terms of both translation quality and term consistency, ranking first based on COMET in the En→Fr language direction. Furthermore, we explore various methods such as back-translation, explicitly training terminologies as additional parallel data, and in-domain data selection.&lt;/p&gt;</content><author><name>juliette:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Capturing Speaker Incorrectness: Speaker-Focused Post-Correction for Abstractive Dialogue Summarization</title><link href="https://kakaoenterprise.github.io/papers/newsum-csi" rel="alternate" type="text/html" title="Capturing Speaker Incorrectness: Speaker-Focused Post-Correction for Abstractive Dialogue Summarization" /><published>2021-11-10T00:00:00-06:00</published><updated>2021-11-10T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/NewSum-CSI</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/newsum-csi">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;In this paper, we focus on improving the quality of the summary generated by neural abstractive dialogue summarization systems.&lt;/p&gt;

&lt;p&gt;Even though pre-trained language models generate well-constructed and promising results, it is still challenging to summarize the conversation of multiple participants since the summary should include a description of the overall situation and the actions of each speaker.&lt;/p&gt;

&lt;p&gt;This paper proposes self-supervised strategies for speaker-focused post-correction in abstractive dialogue summarization. Specifically, our model first discriminates which type of speaker correction is required in a draft summary and then generates a revised summary according to the required type.&lt;/p&gt;

&lt;p&gt;Experimental results show that our proposed method adequately corrects the draft summaries, and the revised summaries are significantly improved in both quantitative and qualitative evaluations.&lt;/p&gt;</content><author><name>이동엽:카카오</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model</title><link href="https://kakaoenterprise.github.io/papers/emnlp-evaluation-dataset-and-strategy" rel="alternate" type="text/html" title="An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model" /><published>2021-11-07T00:00:00-05:00</published><updated>2021-11-07T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/emnlp-evaluation-dataset-and-strategy</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/emnlp-evaluation-dataset-and-strategy">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Multi-turn response selection models have recently shown comparable performance to humans in several benchmark datasets. However, in the real environment, these models often have weaknesses, such as making incorrect predictions based heavily on superficial patterns without a comprehensive understanding of the context. For example, these models often give a high score to the wrong response candidate containing several keywords related to the context but using the inconsistent tense. In this study, we analyze the weaknesses of the open-domain Korean Multi-turn response selection models and publish an adversarial dataset to evaluate these weaknesses. We also suggest a strategy to build a robust model in this adversarial environment.&lt;/p&gt;</content><author><name>mat:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">AligNART: Non-autoregressive Neural Machine Translation by JointlyLearning to Estimate Alignment and Translate</title><link href="https://kakaoenterprise.github.io/papers/emnlp-alignart" rel="alternate" type="text/html" title="AligNART: Non-autoregressive Neural Machine Translation by JointlyLearning to Estimate Alignment and Translate" /><published>2021-11-07T00:00:00-05:00</published><updated>2021-11-07T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/emnlp-alignart</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/emnlp-alignart">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition. Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we introduce AligNART, which leverages full alignment information to explicitly reduce the modality of the target distribution. AligNART divides the machine translation task into (i) alignment estimation and (ii) translation with aligned decoder inputs, guiding the decoder to focus on simplified one-to-one translation. To alleviate the alignment estimation problem, we further propose a novel alignment decomposition method. Our experiments show that AligNART outperforms previous non-iterative NART models that focus on explicit modality reduction on WMT14 En↔De and WMT16 Ro→En. Furthermore, AligNART achieves BLEU scores comparable to those of the state-of-the-art connectionist temporal classification based models on WMT14 En↔De. We also observe that AligNART effectively addresses the token repetition problem even without sequence-level knowledge distillation.&lt;/p&gt;</content><author><name>송종윤:카카오엔터라이즈, 서울대</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Distilling Global and Local Logits with Densely Connected Relations</title><link href="https://kakaoenterprise.github.io/papers/iccv-distilling-global-and-local-logits" rel="alternate" type="text/html" title="Distilling Global and Local Logits with Densely Connected Relations" /><published>2021-10-11T00:00:00-05:00</published><updated>2021-10-11T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/iccv-distilling-global-and-local-logits</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/iccv-distilling-global-and-local-logits">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;In prevalent knowledge distillation, logits in most image recognition models are computed by global average pooling, then used to learn to encode the high-level and task-relevant knowledge. In this work, we solve the limitation of this global logit transfer in this distillation context. We point out that it prevents the transfer of informative spatial information, which provides localized knowledge as well as rich relational information across contexts of an input scene. To exploit the rich spatial information, we propose a simple yet effective logit distillation approach. We add a local spatial pooling layer branch to the penultimate layer, thereby our method extends the standard logit distillation and enables learning of both finely-localized knowledge and holistic representation. Our proposed method shows favorable accuracy improvement against the state-of-the-art methods on several image classification datasets. We show that our distilled students trained on the image classification task can be successfully leveraged for object detection and semantic segmentation tasks; this result demonstrates our method’s high transferability.&lt;/p&gt;</content><author><name>harry:경희대, 카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation</title><link href="https://kakaoenterprise.github.io/papers/interspeech2021-univnet" rel="alternate" type="text/html" title="UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation" /><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech2021-univnet</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech2021-univnet">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈 AI Lab 음성처리팀은 카카오 i에 적용되는 TTS(Text to Speech) 연구를 진행해오고 있습니다.​​​ TTS 시스템은 크게 텍스트에서 acoustic feature를 생성하는 어쿠스틱 모델(acoustic model)과 이 스펙트로그램에서 음성신호를 합성해 AI 음성을 만들어내는 보코더(vocoder)로 구성됩니다. 여기서 보코더는 고품질의 음성을 생성하는데 주요한 역할을 담당하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/001.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림1. TTS 구조&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈 연구팀은 기존 연구보다 개선된 고품질 음성 합성을 가능케하는 뉴럴 보코더 기술 ‘UnivNet’을 고안해, 이번 INTERSPEECH 2021에서 연구 내용을 공개하게 되었습니다. 지난해 음성합성 모델과 음소-오디오 정렬 모델을 한꺼번에 훈련하는 아키텍처 ‘JDI-T’를 공개한데 이어, 2년 연속 연구 성과를 발표하게 되었습니다. 본 글에서는 이번 연구 성과에 대해 간략하게 소개드리고자 합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-기존-뉴럴-보코더-연구의-한계&quot;&gt;1. 기존 뉴럴 보코더 연구의 한계&lt;/h1&gt;

&lt;p&gt;대다수 뉴럴 보코더(neural vocoder) 연구에서는 전체 주파수 대역 중 일부(0-8kHz)에 해당하는 멜 스펙트로그램(mel-spectrogram)을 입력값으로 사용하고 있습니다. 여기서 멜 스펙트로그램은 인간의 인지 기준에 따라 헤르츠(Hz) 단위의 주파수를 mel-scale에 따라 변환한 값으로, 딥러닝에서 오디오 신호 처리에 많이 활용되는 피쳐(feature)입니다. 일반적으로 사람들은 고주파보다 저주파를 잘 인지하기 때문에, 스펙트로그램의 저주파 부분을 보다 잘 인식할 수 있도록 저주파 부분을 확장시킨 점이 특징입니다.&lt;/p&gt;

&lt;p&gt;이때, 전체 대역폭을 입력값으로 사용하면 음향정보가 더욱 많아져 깨끗한 음성을 얻을 수 있음에도 불구하고, 일부값만이 활용되었습니다. 전체 값을 활용한 일부 연구에서는 합성 음성의 고주파수 대역이 흐릿해지는 문제(over smoothing)가 발생해, 음성에 지지직거리는 소리 등 잡음이 섞여 기대했던 것 이상의 음성 품질을 얻기 어려운 경우들이 있었습니다.&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈 연구팀은 이같은 문제를 해결하고자 새로운 뉴럴 보코더 방법론 ‘UnivNet’을 고안해, 실시간 서비스에서 더욱 깨끗한 음성을 제공하고자 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-univnet-특징-소개&quot;&gt;2. UnivNet 특징 소개&lt;/h1&gt;

&lt;p&gt;먼저 UnivNet의 구조는 &lt;그림2&gt;와 같이 크게 생성기(generator)와 판별기(discriminator)로 구분됩니다. generator는 MelGAN 기반 구조이며, 여기에 더해 모델의 크기를 유지하면서도 효과적으로 로컬 정보를 확보하기 위해 LVC(Location-Variable Convolution)를 추가하였습니다. 이로 인해 mel-spectrogram의 지역별 정보를 효율적으로 모델에 제공하여, 더 적은 파라미터 수로도 더 높은 음질을 얻을 수 있었습니다. 또한, 효율적인 연산을 위해 GAU(Gated Activation Unit)를 더했습니다.&lt;/그림2&gt;&lt;/p&gt;

&lt;p&gt;다음으로 discriminator에서는 generator에서 생성된 가짜 데이터와 실제 데이터를 구별하도록 학습을 진행합니다. 여기서 주목할 점은 multi-resolution spectrogram discriminator(이하 MRSD)를 사용했다는 것입니다. MRSD는 다양한 STFT 파라미터셋을 사용하여 실제 데이터와 생성된 가짜 데이터의 여러 선형 스펙트로그램 크기들을 계산해, 각 하위 판별기에 입력값으로 활용합니다. 이때, STFT 파라미터셋에는 1)푸리에 변환 차수, 2)시간(frame) 이동 간격, 3)윈도우(window) 길이가 포함됩니다.&lt;/p&gt;

&lt;p&gt;UnivNet은 MRSD 구조를 통해 전체 대역폭 데이터가 가진 다양한 시간과 해상도(resolution) 정보를 사용하여 실제 사람 음성과 같은 높은 수준의 음성을 생성하고자 하였습니다. MRSD 구조는 MelGAN의 multi-scale waveform discriminator(MSWD) 구조에 기반하며, 여기에 시간 영역에서 적대적 모델링을 개선하기 위해 multi-period waveform discriminator(MPWD)를 더한 점이 특징입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/002.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림2. UnivNet 구조&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-실험-결과&quot;&gt;3. 실험 결과&lt;/h1&gt;

&lt;h3 id=&quot;1-데이터-구성&quot;&gt;1) 데이터 구성&lt;/h3&gt;

&lt;p&gt;해당 실험에는 LibriTTS 데이터셋을 활용하였습니다. LibriTTS 데이터셋은 영어 오디오북 데이터셋으로, 이 중 192시간 분량, 11만 6천개의 발화, 904명의 화자 데이터로 구성된 ‘train-clean-360’을 바탕으로 모델 훈련을 진행하고, 이미 아는 화자(seen speaker)에 대한 평가를 진행하였습니다. 9시간 분량, 4천개의 발화, 39명의 화자 데이터로 구성된 ‘train-clean’ 데이터셋으로는 처음 보는 화자(unseen speaker)에 대한 평가를 진행하였습니다. 또한, TTS 성능 평가를 위해서는 24시간 분량, 1만 3천개 발화 데이터로 구성된 LJSpeech 데이터셋을 활용하였습니다. 해당 데이터셋은 영어로 구성된 단일 화자의 데이터셋입니다.&lt;/p&gt;

&lt;h3 id=&quot;2-ablation-study&quot;&gt;2) Ablation study&lt;/h3&gt;

&lt;p&gt;먼저 해당 모델의 자체 성능을 파악하기 위해 Ablation study를 진행하였습니다. 각 구성요소는 G1=LVC, G2=GAU, D1=MRSD, D2=MPWD, D3=MSWD와 같습니다. 여기서 주목할 점은 &lt;표1&gt;에서 D1(MRSD)이 제거되면 &lt;그림3&gt;의 왼쪽 그림과 같이 생성된 음성의 고주파수 대역이 흐릿해지는 문제가 발생한다는 점입니다. 제안하는 모델의 결과를 나타내는 &lt;그림3&gt;의 가운데 그림을 보면 이러한 문제가 개선되어 실제 녹음 음성의 고주파수 대역과 비슷해진 점을 볼 수 있습니다. 이를 통해 Univnet 모델에서 MRSD 구조가 가지는 중요성을 확인할 수 있었습니다.&lt;/그림3&gt;&lt;/그림3&gt;&lt;/표1&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/003.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표1. Ablation study 결과&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/004.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림3. 오디오 클립에서 생성된 스펙트로그램&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-기존-보코더-모델과-성능-비교&quot;&gt;3) 기존 보코더 모델과 성능 비교&lt;/h3&gt;

&lt;p&gt;다음으로는 generator의 채널 크기가 다른 두 가지 버전의 UnivNet(UnivNet-16, UnivNet-32)을 준비하여 GAN 기반 보코더(MelGAN, Parallel WaveGAN, HiFi-GAN)와 성능을 비교해보았습니다.&lt;/p&gt;

&lt;p&gt;보코더 자체의 성능을 평가하기 위해 실제 화자의 음성(Seen/Unseen speakers)에서 추출된 멜 스펙트로그램을 보코더를 이용하여 음성을 생성하는 과정을 진행하였습니다. &lt;표2&gt;를 보면 UnivNet-16은 학습 때 활용된 화자 데이터 외에 처음 보는 화자 데이터에서도 우수한 품질의 음성을 생성하였습니다. 기존 보코더 모델의 경우 새로운 화자가 추가될 때마다 모델을 추가 학습해야 하는 불편함이 있었지만, UnivNet은 처음 보는 화자 데이터에서도 우수한 성과를 보였다는 점에서 이같은 문제를 상당수 개선하였다고 볼 수 있습니다. 또한, UnivNet-32의 경우 전체 분야에서 기존 보코더 모델보다 높은 점수를 기록하고, 추론 속도도 다소 절감시키는 등 의미있는 결과값을 얻었습니다. 이어 진행된 TTS 성능 평가에서도 UnivNet-32은 우수한 성능을 보이며, 보코더 성능뿐만 아니라, 실제 TTS 구조 상에서도 높은 성능을 보여줌을 확인하였습니다.&lt;/표2&gt;&lt;/p&gt;

&lt;p&gt;해당 연구는 기존 보코더 모델보다 더 많은 대역폭을 쓰면서도 over-smoothing 문제 없이 더 깨끗한, 선명한 합성음을 얻었다는 점에서 의의가 있습니다. 또한, 음성 합성 분야에서 합성음의 품질뿐만 아니라, 중요한 추론 속도를 절감시켰다는 점도 주목할만합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/005.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표2. 기존 보코더 모델과 성능 비교&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-향후-계획&quot;&gt;4. 향후 계획&lt;/h1&gt;

&lt;p&gt;향후 UnivNet은 카카오 i 서비스에 적용되어 실제 다수의 사용자를 대상으로 서비스될 예정입니다. 합성음의 명료도와 자연성이 실제 사람의 발화 수준과 동일할 정도로, 그 품질을 향상시키기 위해 지속 연구할 계획입니다. 또한, fine-tuning 과정 없이도 여러 화자의 TTS 파이프라인에 활용할 수 있는 유니버셜 보코더 연구를 진행하고자 합니다. 앞으로도 카카오엔터프라이즈의 음성처리 연구에 많은 관심 부탁드립니다. 감사합니다.&lt;/p&gt;</content><author><name>taylor:카카오엔터프라이즈</name></author><category term="papers" /><category term="INTERSPEECH" /><category term="UnivNet" /><category term="vocoder" /><summary type="html">Abstract</summary></entry><entry><title type="html">Auxiliary Sequence Labeling Tasks for Disfluency Detection</title><link href="https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection" rel="alternate" type="text/html" title="Auxiliary Sequence Labeling Tasks for Disfluency Detection" /><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Detecting disfluencies in spontaneous speech is an important preprocessing step in natural language processing and speech recognition applications. Existing works for disfluency detection have focused on designing a single objective only for disfluency detection, while auxiliary objectives utilizing linguistic information of a word such as named entity or part-of-speech information can be effective. In this paper, we focus on detecting disfluencies on spoken transcripts and propose a method utilizing named entity recognition(NER) and part-of-speech(POS) as auxiliary sequence labeling(SL) tasks for disfluency detection. First, we investigate cases that utilizing linguistic information of a word can prevent mispredicting important words and can be helpful for the correct detection of disfluencies. Second, we show that training a disfluency detection model with auxiliary SL tasks can improve its F-score in disfluency detection. Then, we analyze which auxiliary SL tasks are influential depending on baseline models. Experimental results on the widely used English Switchboard dataset show that our method outperforms the previous state-of-the-art in disfluency detection.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그동안 카카오엔터프라이즈 AI Lab에서는 음성인식 영역과 자연어처리 분야에 활용 가능한 ‘사족제거(disfluency detection)’를 주제로, 연구를 진행해 왔습니다. 이번 INTERSPEECH 2021에서 NER(개체명 인식), POS(품사 태그) 정보를 활용해 사족제거 작업의 정확도를 높이는 새로운 방법론을 공개하게 되어, 해당 내용을 간략하게 소개드리고자 합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-사족제거-연구의-필요성&quot;&gt;1. 사족제거 연구의 필요성&lt;/h1&gt;

&lt;p&gt;먼저 사족제거 작업은 화자가 “음”, “아”와 같이 의미 없이 발화한 간투사를 교정하고, 발화 중 반복 사용한 표현들을 정제하는 과정을 말합니다. 예를 들어 “어… 오늘, 오늘은 날씨가 참 좋네”라는 문장에서 사족을 제거한다면, 별다른 의미를 가지지 않는 “어…”와 반복된 “오늘”을 삭제할 수 있습니다.&lt;/p&gt;

&lt;p&gt;실제 발화 상황에서는 이와 같은 간투사나 숨소리, 머뭇거림 등 음성 전사 후 언어를 처리하는데 불필요한 발화가 다수 포함될 수 있습니다. 이런 발화를 음성인식 모델이 인식하고, 음성번역, 언어이해 등 자연어 처리(NLP)를 하기 위해서는 이러한 사족을 제거하는 작업이 필수적입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-기존-사족제거-연구의-한계&quot;&gt;2. 기존 사족제거 연구의 한계&lt;/h1&gt;

&lt;p&gt;기존 사족제거 연구는 word 단위의 자질(feature)을 각각 reparandum(RM), interregnum(IM), repair(RP)로 구분하여 우리가 제거해야할 대상(RM, IM)과 아닌 것(RP)을 명확히 파악하는데 초점을 두었습니다. IM에는 ‘음, 아’와 같은 간투사 표현들이 해당되고, 이들은 쉽게 구분되는 특성을 가졌기 때문에 IM보다는 주로 RM에 대한 예측정확도를 높이는데 중점을 둔 연구가 주를 이루고 있습니다. 여기서 RM은 쉽게 말해, 문장 시작에 들어가는 쿠션어, 반복어 표현들을 일컫는데, 먼저 입력된 값(RM)에 대해 오류라고 판단하고 뒤에 오는 값(RP)을 올바른 값으로 보았습니다. 기존 연구들은 이 RM을 파악하는데 목적을 두고, RM으로 분류된 정답 데이터를 학습하는 방식이 주로 활용되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/001.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림1. 기존 사족제거 연구의 어노테이션(annotation) 예시&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;하지만, 이와 같이 제거 대상만을 가려내는데 초점을 두면 중요한 의미를 가진 단어를 사족으로 오예측(false positive)하게 되고 예측정확도가 떨어질 수 있다는 한계점이 있습니다. 이에, 카카오엔터프라이즈 연구팀은 추가적인  정보들을 학습시켜 예측 성능을 높이는 방법론을 고안하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-auxiliary-sequence-labeling-tasks-방법론-소개&quot;&gt;3. Auxiliary Sequence Labeling Tasks 방법론 소개&lt;/h1&gt;

&lt;p&gt;카카오엔터프라이즈 연구팀이 새롭게 제안하는 방법론은 ‘Auxiliary Sequence Labeling Tasks’입니다. 기존 연구에서 추가적인 정보로 NER(개체명 인식, Named Entity Recognition), POS(품사 태그, Part-Of-Speech)를 Multi-Task Learning 방식에 활용해, 총 3개의 목적함수를 사용하여 학습을 진행하였습니다.&lt;/p&gt;

&lt;p&gt;좀 더 자세히 살펴보면, 먼저 NER은 해당 단어의 개체명(Entity Name)이 인명, 장소, 시간 표현 등을 정의하는 과제(task)입니다.  그림2를 예시로 보면 NER 정보는 초록색으로 나타납니다. 기존 연구에서는 사족에 해당하는 ‘i would’를 명확히 파악하지 못하고, 오히려 문장의 중요한 의미를 담고 있는 ‘my forty’를 사족으로 오예측하였습니다. 본 연구에서는 이처럼 중요한 의미를 갖는 단어의 오예측을 방지하고, 정확한 사족 예측이 가능해지도록 NER 정보를 학습에 활용했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/002.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림2. NER 정보 예시 (초록색이 NER에 해당하며, 파란색이 사족에 해당함)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;또한, 추가적으로 사용된 POS는 각 문장 성분의 품사 태그 정보를 의미합니다. 그림3에서는 RM과 RP의 POS가 동일한 태그 형태를 가질 때, 보다 정확한 사족 예측이 가능함을 보여줍니다. 기존 연구에서도 POS를 활용한 연구들은 진행되었지만, rule-base 방식의 경우 단어 의미가 담긴 시맨틱 정보가 제대로 반영되지 않는다는 한계가 있었고, ML방식의 경우 추론(inference) 시 해당 입력 단어들에 대한 feature를 추출하기 위한 추가 시간이 소요된다는 문제가 있었습니다. 카카오엔터프라이즈 연구팀은 POS와 NER 정보를 추가 활용하는 Auxiliary Sequence Labeling Tasks 방법론을 새롭게 제안하여 그동안의 문제점들을 해결하고 더 높은 성능을 얻고자 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/003.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림3. POS 정보 예시&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;본 연구에서는 사족제거, NER, 그리고 POS태그들을 예측을 위해 3가지의 negative log likelihood loss 함수를 사용하였습니다. 사족제거, NER, 그리고 POS 테스크를 위한 loss 함수들을 각각 Ld, Le, Lp라고 정의할때, 최종 loss 함수는 L= Ld + *(Le +Lp)로 정의합니다. 여기서  는 계수(coefficients)로써, NER과 POS 테스크들의 영향도를 학습에 얼마나 반영할지 결정하는 요소입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-성능-평가&quot;&gt;4. 성능 평가&lt;/h1&gt;

&lt;p&gt;제안한 방법의 성능평가를 위해 기존 연구와의 모델 성능비교와 학습요소별 성능비교 2가지 방식을 이용하였습니다. 먼저 동일한 CRF Layer(Decoder)에 입력 자질(feature input)로 사전훈련된 언어모델 transformer, BERT, ELECTRA의 최종 output layer를 사용하여 각각의 성능을 비교해 보았습니다. 현재 성능 테스트에 많이 활용되고 있는 English Switchboard 데이터셋을 활용하여 테스트해본 결과, F1 score에서 기존 연구 모델보다 평균적으로 높은 수치를 기록하는 것을 확인할 수 있었고, 최고치의 경우 93.1(ELECTRA)에 달하는 우수한 결과를 얻었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/004.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표1. 기존 방식으로 학습된 모델과 Auxiliary SL(Sequence Labeling) Tasks 방식으로 학습된 모델의 성능 비교&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;다음으로, 동일한 English Switchboard 데이터셋을 이용해 Ablation 분석을 진행하였습니다. 1)기존 연구 방식으로 학습된 모델, 2)NER을 추가한 경우, 3)POS를 추가한 경우, 4)AUXILIARY SL Tasks 방식(NER과 POS를 모두 추가, SL : Sequence Labeling)을 각각 비교해 보았습니다. 표2에서 볼 수 있듯이 추가 정보를 모두 활용한 경우가 F1 score에서 가장 높은 수치를 기록하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/005.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표2. Ablation Analysis 분석 결과&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이를 통해 추가 정보를 활용하여 목적함수를 확대하는 것이 실제 성능에 영향력을 미친다는 것을 확인할 수 있었다는 점에서 해당 연구가 갖는 의의가 크다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;추론에서도 본 연구의 장점이 있습니다. 기존의 연구에서도 이와 비슷하게 모델의 입력으로 단어의 NER과 POS와 같은 추가 자질들을 사용하는 방법들이 있었지만, 이러한 방법들은 추론시, 단어의 자질들을 추출하는데 추가 시간이 필요한 단점이 있습니다.하지만 본 연구에서는 NER와 POS tasks를 학습시간에만 활용하고, 추론시에는 사용을 하지 않기 때문에 추론 시간을 단축시키는 효과가 있습니다. 그 결과, 아래의 표3과 같이 추론을 위해 추가적으로 요구되는 시간은 없다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/006.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표3. Auxiliary SL Tasks를 활용한 방식(Ours)과 활용하지 않은 방식(Ours w/o aux)간의 추론 속도 비교&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;5-향후-연구-계획&quot;&gt;5. 향후 연구 계획&lt;/h1&gt;

&lt;p&gt;해당 연구는 현재 헤이카카오앱 중 받아쓰기 기능에 적용되었고, 녹음 내용을 전사한 결과에 사족제거기능으로 활용되고 있습니다. 향후 한국어 서비스 고도화를 위해 한국어용 사족제거 모델 개선에 집중하고 유의미한 성능을 얻을 수 있도록 연구를 발전시켜나갈 계획입니다. 앞으로도 많은 관심 부탁드립니다. 감사합니다.&lt;/p&gt;</content><author><name>이동엽:카카오</name></author><category term="papers" /><category term="INTERSPEECH" /><category term="NLP" /><category term="Disfluency-detection" /><summary type="html">Abstract</summary></entry><entry><title type="html">SE-Conformer: Time-Domain Speech Enhancement using Conformer</title><link href="https://kakaoenterprise.github.io/papers/interspeech2021-se-conformer" rel="alternate" type="text/html" title="SE-Conformer: Time-Domain Speech Enhancement using Conformer" /><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech2021-se-conformer</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech2021-se-conformer">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Convolution-augmented transformer(Conformer) has recently shown competitive results in speech-domain applications, such as automatic speech recognition, continuous speech separation, and sound event detection. Conformer can capture both the short and long-term temporal sequence information by attending to the whole sequence at once with multi-head self-attention and convolutional neural network. However, the effectiveness of conformer in speech enhancement has not been demonstrated. In this paper, we propose an end-to-end speech enhancement architecture(SE-Conformer), incorporating a convolutional encoder–decoder and conformer, designed to be directly applied to the time-domain signal. We performed evaluations on both the VoiceBank-DEMAND Corpus(VCTK) and Librispeech datasets in terms of objective speech quality metrics. The experimental results show that the proposed model outperforms other competitive baselines in speech enhancement performance.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈 AI Lab에서 새롭게 제안하는 ‘SE-Conformer’는 음성 향상(SE, Speech Enhancement) 방법론입니다. 음성 향상 기술은 잡음(noise)이 포함된 음성 신호에서 잡음을 제거하여 음성 신호의 음질을 높이는 역할을 합니다. 실생활에서 녹음되는 음성에는 바람 등 주변 소리, 타인의 목소리 등 다양한 잡음이 섞이게 되는데, 이러한 음성 데이터를 음성 기반 애플리케이션에서 처리하기 위해서는 잡음을 제거하는 전처리 과정이 필수적입니다. 예를 들어 스마트 스피커의 경우, 인식해야되는 사용자의 음성에 잡음이 다수 섞여 있다면 인식 성능이 크게 낮아질 수 있기 때문에 음성 향상 기술이 중요한 역할을 하게 됩니다.&lt;/p&gt;

&lt;p&gt;카카오엔터프라이즈 연구팀은 그동안 음성 처리 연구를 진행해오며, 전처리 기술로서 음성 향상 기술에 대한 연구를 지속해왔습니다. 이번 INTERSPEECH 2021을 통해 발표하게 된 새로운 음성 향상 방법론, SE-Conformer에 대한 내용을 간략하게 소개드리겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-기존-음성-향상-기술-연구의-한계&quot;&gt;1. 기존 음성 향상 기술 연구의 한계&lt;/h1&gt;

&lt;p&gt;기존 뉴럴넷(Neural Network) 기반의 음성 향상 기술은 주로 푸리에 변환(Fourier transform)을 사용하여, 주파수 대역에서 뉴럴넷을 처리하는 방식으로 연구가 진행되었습니다. 최근에는 음성 품질을 높이기 위해 잡음이 섞인 음성으로부터 깨끗한 신호를 뉴럴넷을 통해 직접 추정하는 음성 향상 기법들이 연구되고 있습니다. 그 중 푸리에 변환 대신 Convolutional Encoder-Decoder(CED) 구조를 사용하여 음성 파형을 처리하는 End2End 방식은 음성의 왜곡을 줄이고 품질을 높인다는 장점이 있습니다. 이에 카카오엔터프라이즈는 CED 구조를 기반으로 Conformer 네트워크를 활용하여 시간 영역에서 더욱 깨끗하게 음성의 잡음을 제거할 수 있는 새로운 방법론을 제안하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-se-conformer-특징-소개&quot;&gt;2. SE-conformer 특징 소개&lt;/h1&gt;

&lt;p&gt;SE-conformer는 그림1과 같이 Encoder, Conformer block, Decoder로 구성되어 있습니다. 먼저 Encoder는 Upsampling block을 통하여 resolution을 증가시키고, 컨볼루션 연산 기반의 연산 (E-ConvBlock) 을 통하여 음성 신호를 latent representation으로 변환시켜주는 역할을 합니다.&lt;/p&gt;

&lt;p&gt;그 후에 Conformer를 사용하여 잡음이 포함된 신호의 latent representation으로부터 깨끗한 신호의 representation을 추정합니다. Conformer는 Multi-Head Self-Attention 과 Depth-Wise 컨볼루션을 결합한 네트워크로, 연속적인 데이터에서 장단기 정보를 잘 반영하는데 적합하다고 알려져 있습니다.&lt;/p&gt;

&lt;p&gt;마지막으로  Decoder는 Encoder와 반대 개념으로, conformer block의 아웃풋으로부터 깨끗한 신호를 복원하는 역할을 합니다. 트랜스포즈 컨볼루션 기반의 연산 (D-ConvBlock)을 통하여 latent representation을 다시 음성 신호로 복원시키고, Downsample block을 통해 resolution을 감소시켜 최종적으로 우리가 원하는 잡음이 제거된 깨끗한 신호를 얻을 수 있도록 네트워크를 구성합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SE-Conformer/001.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;그림1. SE-Conformer 구조&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-성능-평가&quot;&gt;3. 성능 평가&lt;/h1&gt;

&lt;p&gt;실험 결과는 VCTK 데이터셋과 LibriSpeech 데이터셋으로 측정되었습니다. VCTK 데이터의 깨끗한 음성 신호에 DEMAND 데이터셋의 잡음을 SNR 15, 10, 5, 0 dB로 섞어 학습 데이터셋을 생성하였으며, 학습에서 사용되지 않은 DEMAND 잡음을  2.5, 7.5, 12.5, 17.5 dB으로 섞어 테스트 데이터셋을 생성하였습니다. 성능은 음성 품질 측정을 위한 PESQ, 왜곡 측정을 위한 CSIG 외에 CBAK, COVL, STOI를 기준으로 측정되었으며, 제안하는 SE-conformer는 기존 연구 모델보다 평균적으로 우수한 성능을 보였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SE-Conformer/002.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표1. SE-Conformer와 기존 방법론의 성능 비교 (VCTK 데이터셋 기준)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Large 데이터셋에서의 성능을 측정하기 위하여 LibriSpeech 데이터셋에서의 평가도 진행하였습니다. 학습 데이터셋을 생성하기 위해 LibriSpeech의 train-clean-100에서 랜덤하게 50h을 오디오 샘플을 선정하여 DEMAND 데이터셋과 MUSAN 데이터셋의 잡음과 -10 dB와 10dB로 섞었습니다. 테스트 데이터셋은 test-clean에서 랜덤하게 선정된 500개의 발화와 NOISEX-92 데이터셋과 DEMAND 데이터셋의 잡음을 -5, 0, 5, 10, 15 dB로 섞어 생성되었습니다. LibriSpeech 데이터셋에서도 기존 연구 모델 대비 우수한 성능을 나타내는 것을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SE-Conformer/003.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;표2. SE-Conformer와 기존 방법론의 성능 비교 (LibriSpeech 데이터셋 기준)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-향후-계획&quot;&gt;4. 향후 계획&lt;/h1&gt;

&lt;p&gt;해당 연구 결과를 카카오 i 서비스에 적용하여 실제 사용자를 대상으로 더 나은 서비스를 제공하기 위한 연구를 이어나갈 계획입니다. 다양한 환경에서의 잡음을 실시간으로 제거해 음성의 품질을 높여 서비스의 만족도를 높이고, 모델 경량화를 통해 연산량 측면으로도 고도화시킬 예정입니다. 앞으로도 카카오엔터프라이즈 AI Lab 연구에 많은 관심 부탁드립니다. 감사합니다.&lt;/p&gt;</content><author><name>chris:카카오엔터프라이즈</name></author><category term="papers" /><category term="INTERSPEECH" /><category term="SE-Conformer" /><summary type="html">Abstract</summary></entry><entry><title type="html">Learning to Walk across Time for Interpretable Temporal Knowledge Graph Completion</title><link href="https://kakaoenterprise.github.io/papers/sigkdd-t-gap" rel="alternate" type="text/html" title="Learning to Walk across Time for Interpretable Temporal Knowledge Graph Completion" /><published>2021-08-14T00:00:00-05:00</published><updated>2021-08-14T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/sigkdd-t-gap</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/sigkdd-t-gap">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Static knowledge graphs(KGs), despite their wide usage in relational reasoning and downstream tasks, fall short of realistic modeling of knowledge and facts that are only temporarily valid. Compared to static knowledge graphs, temporal knowledge graphs(TKGs) inherently reflect the transient nature of real-world knowledge. Naturally, automatic TKG completion has drawn much research interests for a more realistic modeling of relational reasoning. However, most of the existing models for TKG completion extend static KG embeddings that do not fully exploit TKG structure, thus lacking in 1) accounting for temporally relevant events already residing in the local neighborhood of a query, and 2) path-based inference that facilitates multi-hop reasoning and better interpretability. In this paper, we propose T-GAP, a novel model for TKG completion that maximally utilizes both temporal information and graph structure in its encoder and decoder. T-GAP encodes query-specific substructure of TKG by focusing on the temporal displacement between each event and the query timestamp, and performs path-based inference by propagating attention through the graph. Our empirical experiments demonstrate that T-GAP not only achieves superior performance against state-of-the-art baselines, but also competently generalizes to queries with unseen timestamps. Through extensive qualitative analyses, we also show that T-GAP enjoys transparent interpretability, and follows human intuition in its reasoning process.&lt;/p&gt;</content><author><name>hoony:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html">Abstract</summary></entry></feed>