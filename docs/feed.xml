<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kakaoenterprise.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kakaoenterprise.github.io/" rel="alternate" type="text/html" /><updated>2022-09-29T01:17:52-05:00</updated><id>https://kakaoenterprise.github.io/feed.xml</id><title type="html">카카오엔터프라이즈 AI Research</title><subtitle>카카오엔터프라이즈 AI Lab에서 발표한 AI 논문과 연구 성과를 소개합니다.</subtitle><author><name>카카오엔터프라이즈</name></author><entry><title type="html">Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods</title><link href="https://kakaoenterprise.github.io/papers/coling-multi-context-retrieval" rel="alternate" type="text/html" title="Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods" /><published>2022-10-16T00:00:00-05:00</published><updated>2022-10-16T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/Coling-multi-context-retrieval</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/coling-multi-context-retrieval"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Persona and Knowledge dual context opendomain chat is a novel dialogue generation task introduced recently (Jang et al., 2021). While Persona and Knowledge is each interesting context of open-domain dialogue, the combination of both has not been well studied. We tackle Persona-Knowledge identification and response generation tasks in this paper. We design an informed data augmentation strategy that is compatible with neural Q&amp;A retrieval models. With the augmented data, we perform permutative Persona-Knowledge evaluation and successive Persona search fine-tuning. Furthermore, we perform dialogue generation with various decoding techniques and illustrate crucial elements. We achieve SOTA across official metrics with 93.99% Grounding accuracy average and 23.62 SacreBLEU score.</p>]]></content><author><name>오민식:Alexa AI</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers</title><link href="https://kakaoenterprise.github.io/papers/interspeech-rnn-t" rel="alternate" type="text/html" title="Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-rnn-t</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-rnn-t"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Recurrent neural network transducer (RNN-T) is an end-to-end speech recognition framework converting input acoustic frames into a character sequence. The state-of-the-art encoder network for RNN-T is the Conformer, which can effectively model the local-global context information via its convolution and self-attention layers. Although Conformer RNN-T has shown outstanding performance, most studies have been verified in the setting where the train and test data are drawn from the same domain. The domain mismatch problem for Conformer RNN-T has not been intensively investigated yet, which is an important issue for the product-level speech recognition system. In this study, we identified that fully connected self-attention layers in the Conformer caused high deletion errors, specifically in the long-form out-domain utterances. To address this problem, we introduce sparse self-attention layers for Conformer-based encoder networks, which can exploit local and generalized global information by pruning most of the in-domain fitted global connections. Also, we propose a state reset method for the generalization of the prediction network to cope with long-form utterances. Applying proposed methods to an out-domain test, we obtained 27.6% relative character error rate (CER) reduction compared to the fully connected self-attention layer-based Conformers.</p>]]></content><author><name>김준태:SK텔레콤</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning</title><link href="https://kakaoenterprise.github.io/papers/interspeech-pronunciation" rel="alternate" type="text/html" title="Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-pronunciation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-pronunciation"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Self-supervised learning (SSL) approaches such as wav2vec 2.0 and HuBERT models have shown promising results in various downstream tasks in the speech community. In particular, speech representations learned by SSL models have been shown to be effective for encoding various speech-related characteristics. In this context, we propose a novel automatic pronunciation assessment method based on SSL models. First, the proposed method fine-tunes the pre-trained SSL models with connectionist temporal classification to adapt the English pronunciation of English-as-a-second-language (ESL) learners in a data environment. Then, the layer-wise contextual representations are extracted from all across the transformer layers of the SSL models. Finally, the automatic pronunciation score is estimated using bidirectional long short-term memory with the layer-wise contextual representations and the corresponding text. We show that the proposed SSL model-based methods outperform the baselines, in terms of the Pearson correlation coefficient, on datasets of Korean ESL learner children and Speechocean762. Furthermore, we analyze how different representations of transformer layers in the SSL model affect the performance of the pronunciation assessment task.</p>]]></content><author><name>chris:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation</title><link href="https://kakaoenterprise.github.io/papers/interspeech-emotion-recognition" rel="alternate" type="text/html" title="The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-emotion-recognition</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-emotion-recognition"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>In emotion recognition in conversation (ERC), the emotion of the current utterance is predicted by considering the previous context, which can be utilized in many natural language processing tasks. Although multiple emotions can coexist in a given sentence, most previous approaches take the perspective of a classification task to predict only a given label. However, it is expensive and difficult to label the emotion of a sentence with confidence or multi-label. In this paper, we automatically construct a grayscale label considering the correlation between emotions and use it for learning. That is, instead of using a given label as a one-hot encoding, we construct a grayscale label by measuring scores for different emotions. We introduce several methods for constructing grayscale labels and confirm that each method improves the emotion recognition performance. Our method is simple, effective, and universally applicable to previous systems. The experiments show a significant improvement in the performance of baselines.</p>]]></content><author><name>rung:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech</title><link href="https://kakaoenterprise.github.io/papers/interspeech-jets" rel="alternate" type="text/html" title="JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech" /><published>2022-09-18T00:00:00-05:00</published><updated>2022-09-18T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech-JETS</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech-jets"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>In neural text-to-speech (TTS), two-stage system or a cascade of separately learned models have shown synthesis quality close to human speech. For example, FastSpeech2 transforms an input text to a mel-spectrogram and then HiFi-GAN generates a raw waveform from a mel-spectogram where they are called an acoustic feature generator and a neural vocoder respectively. However, their training pipeline is somewhat cumbersome in that it requires a fine-tuning and an accurate speech-text alignment for optimal performance. In this work, we present end-to-end text-to-speech (E2E-TTS) model which has a simplified training pipeline and outperforms a cascade of separately learned models. Specifically, our proposed model is jointly trained FastSpeech2 and HiFi-GAN with an alignment module. Since there is no acoustic feature mismatch between training and inference, it does not requires fine-tuning. Furthermore, we remove dependency on an external speech-text alignment tool by adopting an alignment learning objective in our joint training framework. Experiments on LJSpeech corpus shows that the proposed model outperforms publicly available, state-of-the-art implementations of ESPNet2-TTS on subjective evaluation (MOS) and some objective evaluations.</p>]]></content><author><name>satoshi:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Proxyless Neural Architecture Adaptation at Once</title><link href="https://kakaoenterprise.github.io/papers/ieee-pnaa" rel="alternate" type="text/html" title="Proxyless Neural Architecture Adaptation at Once" /><published>2022-09-15T00:00:00-05:00</published><updated>2022-09-15T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/IEEE-PNAA</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/ieee-pnaa"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Recently, Neural Architecture Search (NAS) methods are introduced and show impressive performance on many benchmarks. Among those NAS studies, Neural Architecture Transformer (NAT) aims to adapt the given neural architecture to improve performance while maintaining computational costs. In the architecture adaptation task, we can utilize the known high-performance architectures, and the architecture adaptation results of NAT showed performance improvements on various architectures in their experiments. However, we verified that NAT lacks reproducibility through multiple trials of experiments. Moreover, it requires an additional architecture adaptation process before network weight training. In this paper, we propose proxyless neural architecture adaptation that is reproducible and efficient. The proposed method doesn’t need a proxy task for architecture adaptation. It directly improves the architecture during the conventional training process, and we can directly use the trained neural network. Moreover, the proposed method can be applied to both supervised learning and self-supervised learning. The proposed method shows stable performance improvements on various architectures and various datasets. Extensive experiments on two benchmark datasets, i.e ., CIFAR-10 and Tiny Imagenet, present that the proposed method definitely outperforms NAT and be applicable to various models and datasets.</p>]]></content><author><name>김도국:인하대</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Efficient Two-Stream Network for Online Video Action Segmentation</title><link href="https://kakaoenterprise.github.io/papers/ieee-action-seg" rel="alternate" type="text/html" title="Efficient Two-Stream Network for Online Video Action Segmentation" /><published>2022-08-24T00:00:00-05:00</published><updated>2022-08-24T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/IEEE-action-seg</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/ieee-action-seg"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Temporal action segmentation is a task of predicting frame-level classes of untrimmed long-termed videos. It can be widely used in various applications including customer analysis, data collection, video indexing, and surveillance. Learning good representations from videos such as motion and spatial representation is a critical factor for model performance. Furthermore, the used model should make reliable decisions with low latency for practical use in a resource-constrained environment. However, many works in action segmentation consider just an accuracy using the pre-calculated representations from heavy 3-dimensional(3D) networks. In this paper, we propose a two-stream action segmentation pipeline that can learn motion and spatial information efficiently and operate online. While the temporal stream combines frame-grouping and TSM for capturing short-term dynamics and long-term temporal information at the same time, the spatial stream captures information on color and appearance complementary to representations from the temporal stream. In addition, the results of both the streams are combined by a cross-attention module to provide a desired classification result for the task. Since it can be operated without heavy 3D convolutional neural networks (CNNs), it takes much less memory and computation than conventional 3D-CNN-based methods. Our proposed network using a non-overlapping sliding window achieved segmentation performance on two action segmentation datasets comparable to many recent works that require full temporal resolution and pre-calculated features of 3D CNNs.</p>]]></content><author><name>marcus:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Classification-based Multi-task Learning for Efficient Pose Estimation Network</title><link href="https://kakaoenterprise.github.io/papers/icpr-pose-estimation" rel="alternate" type="text/html" title="Classification-based Multi-task Learning for Efficient Pose Estimation Network" /><published>2022-08-21T00:00:00-05:00</published><updated>2022-08-21T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/ICPR-pose-estimation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icpr-pose-estimation"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Human pose estimation is an interesting and underlying topic in various fields such as action recognition and human-computer interaction. Although many methods have been developed recently, they are still far from perfect in accuracy and speed at a time. In this paper, we propose a Classification-based Pose Estimation Network with Multi-task Learning (CPENML) based on the low-resolution feature map to improve accuracy and inference time simultaneously. The proposed CPENML consists of two ideas. Firstly, novel proposed keypoint and offset estimation
tasks based on classification achieve better performance than regression. Secondly, the proposed Multi-Scale Network
(MSN) makes robust feature maps and balances the keypoint and offset tasks to maximize performance. To prove the effectiveness of the proposed method, we conduct ablation studies on the COCO dataset for proposed ideas. Compared to benchmarks, we demonstrate the superiority of our proposed method on COCO dataset in terms of inference time and accuracy.</p>]]></content><author><name>benjamin:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">ComDensE : Combined Dense Embedding of Relation-aware and Common Features for Knowledge Graph Completion</title><link href="https://kakaoenterprise.github.io/papers/icpr-comdense" rel="alternate" type="text/html" title="ComDensE : Combined Dense Embedding of Relation-aware and Common Features for Knowledge Graph Completion" /><published>2022-08-21T00:00:00-05:00</published><updated>2022-08-21T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/ICPR-ComDensE</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/icpr-comdense"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>Real-world knowledge graphs (KG) are mostly incomplete. The problem of recovering missing relations, called KG completion, has recently become an active research area. Knowledge graph (KG) embedding, a low-dimensional representation of entities and relations, is the crucial technique for KG completion. Convolutional neural networks in models such as ConvE, SACN, InteractE, and RGCN achieve recent successes. This paper takes a different architectural view and proposes ComDensE which combines relation-aware and common features using dense neural networks. In the relation-aware feature extraction, we attempt to create relational inductive bias by applying an encoding function specific to each relation. In the common feature extraction, we apply the common encoding function to all input embeddings. These encoding functions are implemented using dense layers in ComDensE. ComDensE achieves the state-of-the-art performance in the link prediction in terms of MRR, HIT@1 on FB15k-237 and HIT@1 on WN18RR compared to the previous baseline approaches. We conduct an extensive ablation study to examine the effects of the relation-aware layer and the common layer of the ComDensE. Experimental results illustrate that the combined dense architecture as implemented in ComDensE achieves the best performance.</p>]]></content><author><name>lucas:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base from Unstructured Text</title><link href="https://kakaoenterprise.github.io/papers/sigkdd-oasys" rel="alternate" type="text/html" title="OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base from Unstructured Text" /><published>2022-08-15T00:00:00-05:00</published><updated>2022-08-15T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/SIGKDD-OASYS</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/sigkdd-oasys"><![CDATA[<h1 id="abstract">Abstract</h1>

<p>In recent years, creating and managing knowledge bases have become crucial to the retail product and enterprise domains. We present an automatic knowledge base construction system that mines data from documents. This system can generate training data during the training process without human intervention. Therefore, it is domain-agnostic trainable using only the target domain text corpus and a pre-defined knowledge base. This system is called OASYS and is the first system built with the Korean language in mind. In addition, we also have constructed a new human-annotated benchmark dataset of the Korean Wikipedia corpus paired with a Korean DBpedia to aid system evaluation. The system performance results on human-annotated benchmark test dataset are meaningful and show that the generated knowledge base from OASYS trained on only auto-generated data is useful. We provide both a human-annotated test dataset and an auto-generated dataset.</p>]]></content><author><name>lucas:카카오엔터프라이즈</name></author><category term="papers" /><summary type="html"><![CDATA[Abstract]]></summary></entry></feed>