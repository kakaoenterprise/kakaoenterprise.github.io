<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://kakaoenterprise.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kakaoenterprise.github.io/" rel="alternate" type="text/html" /><updated>2021-12-07T03:16:30-06:00</updated><id>https://kakaoenterprise.github.io/feed.xml</id><title type="html">ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Research</title><subtitle>ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Labì—ì„œ ë°œí‘œí•œ AI ë…¼ë¬¸ê³¼ ì—°êµ¬ ì„±ê³¼ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.</subtitle><author><name>ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><entry><title type="html">SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustness</title><link href="https://kakaoenterprise.github.io/papers/neurips-smoothmix" rel="alternate" type="text/html" title="SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustness" /><published>2021-12-06T00:00:00-06:00</published><updated>2021-12-06T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/neurips-smoothmix</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/neurips-smoothmix">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Randomized smoothing is currently a state-of-the-art method to construct a certifiably robust classifier from neural networks against l2-adversarial perturbations. Under the paradigm, the robustness of a classifier is aligned with the prediction confidence, i.e., the higher confidence from a smoothed classifier implies the better robustness. This motivates us to rethink the fundamental trade-off between accuracy and robustness in terms of calibrating confidences of smoothed classifier. In this paper, we propose a simple training scheme, coined SmoothMix, to control the robustness of smoothed classifiers via self-mixup: it trains convex combinations of samples along the direction of adversarial perturbation for each input. The proposed procedure effectively identifies over-confident, near off-class samples as a cause of limited robustness in case of smoothed classifiers, and offers an intuitive way to adaptively set a new decision boundary between these samples for better robustness. Our experimental results demonstrate that the proposed method can significantly improve the certified l2-robustness of smoothed classifiers compared to existing state-of-the-art robust training methods.&lt;/p&gt;</content><author><name>ì •ì¢…í˜„:ì¹´ì´ìŠ¤íŠ¸</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Learning Debiased Representation via Disentangled Feature Augmentation</title><link href="https://kakaoenterprise.github.io/papers/neurips-learning-debiased-representation" rel="alternate" type="text/html" title="Learning Debiased Representation via Disentangled Feature Augmentation" /><published>2021-12-06T00:00:00-06:00</published><updated>2021-12-06T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/neurips-learning-debiased-representation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/neurips-learning-debiased-representation">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Image classification models tend to make decisions based on peripheral attributes of data items that have strong correlation with a target variable (i.e., dataset bias). These biased models suffer from the poor generalization capability when evaluated on unbiased datasets. Existing approaches for debiasing often identify and emphasize those samples with no such correlation (i.e., bias-conflicting) without defining the bias type in advance. However, such bias-conflicting samples are significantly scarce in biased datasets, limiting the debiasing capability of these approaches. This paper first presents an empirical analysis revealing that training with â€œdiverseâ€ bias-conflicting samples beyond a given training set is crucial for debiasing as well as the generalization capability. Based on this observation, we propose a novel feature-level data augmentation technique in order to synthesize diverse bias-conflicting samples. To this end, our method learns the disentangled representation of (1) the intrinsic attributes (i.e., those inherently defining a certain class) and (2) bias attributes (i.e., peripheral attributes causing the bias), from a large number of bias-aligned samples, the bias attributes of which have strong correlation with the target variable. Using the disentangled representation, we synthesize bias-conflicting samples that contain the diverse intrinsic attributes of bias-aligned samples by swapping their latent features. By utilizing these diversified bias-conflicting features during the training, our approach achieves superior classification accuracy and debiasing results against the existing baselines on both synthetic as well as real-world datasets.&lt;/p&gt;</content><author><name>ì´ì •ìˆ˜:ì¹´ì´ìŠ¤íŠ¸</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Kakao Enterpriseâ€™s WMT21 Machine Translation using Terminologies Task Submission</title><link href="https://kakaoenterprise.github.io/papers/wmt21-terminology-translation" rel="alternate" type="text/html" title="Kakao Enterpriseâ€™s WMT21 Machine Translation using Terminologies Task Submission" /><published>2021-11-19T00:00:00-06:00</published><updated>2021-11-19T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/wmt21-terminology-translation</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/wmt21-terminology-translation">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;This paper describes Kakao Enterpriseâ€™s submission to the WMT21 shared Machine Translation using Terminologies task. We integrate terminology constraints by pre-training with target lemma annotations and fine-tuning with exact target annotations utilizing the given terminology dataset. This approach yields a model that achieves outstanding results in terms of both translation quality and term consistency, ranking first based on COMET in the Enâ†’Fr language direction. Furthermore, we explore various methods such as back-translation, explicitly training terminologies as additional parallel data, and in-domain data selection.&lt;/p&gt;</content><author><name>juliette:ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Capturing Speaker Incorrectness: Speaker-Focused Post-Correction for Abstractive Dialogue Summarization</title><link href="https://kakaoenterprise.github.io/papers/newsum-csi" rel="alternate" type="text/html" title="Capturing Speaker Incorrectness: Speaker-Focused Post-Correction for Abstractive Dialogue Summarization" /><published>2021-11-10T00:00:00-06:00</published><updated>2021-11-10T00:00:00-06:00</updated><id>https://kakaoenterprise.github.io/papers/NewSum-CSI</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/newsum-csi">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;In this paper, we focus on improving the quality of the summary generated by neural abstractive dialogue summarization systems.&lt;/p&gt;

&lt;p&gt;Even though pre-trained language models generate well-constructed and promising results, it is still challenging to summarize the conversation of multiple participants since the summary should include a description of the overall situation and the actions of each speaker.&lt;/p&gt;

&lt;p&gt;This paper proposes self-supervised strategies for speaker-focused post-correction in abstractive dialogue summarization. Specifically, our model first discriminates which type of speaker correction is required in a draft summary and then generates a revised summary according to the required type.&lt;/p&gt;

&lt;p&gt;Experimental results show that our proposed method adequately corrects the draft summaries, and the revised summaries are significantly improved in both quantitative and qualitative evaluations.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì—ì„œëŠ” ë§ˆì¹˜ ì‚¬ëŒì²˜ëŸ¼ ëŒ€í™” ë§¥ë½(context)ì„ ì´í•´í•˜ê³ , ê·¸ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½í•  ìˆ˜ ìˆëŠ” ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì„ ì—°êµ¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ê¸€ì—ì„œëŠ” ì¹´ì¹´ì˜¤, ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì™€ ê³ ë ¤ëŒ€, ì™€ì´ì¦ˆë„›, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ ê³µë™ ì—°êµ¬íŒ€ì´ ë°œí‘œí•œ ìƒˆë¡œìš´ ëŒ€í™” ìƒì„± ìš”ì•½ ë°©ë²•ë¡ ì— ëŒ€í•´ ì†Œê°œë“œë¦¬ê³ ì í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-ê¸°ì¡´-ëŒ€í™”-ìš”ì•½-ê¸°ìˆ ì˜-í•œê³„&quot;&gt;1. ê¸°ì¡´ ëŒ€í™” ìš”ì•½ ê¸°ìˆ ì˜ í•œê³„&lt;/h1&gt;

&lt;p&gt;ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‚¬ì „í›ˆë ¨ ì–¸ì–´ëª¨ë¸(PLM; BERT, BART, T5)ì´ ë§ì€ ë°œì „ì„ ê±°ë“­í•˜ì—¬ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì—¬ì „íˆ ì—¬ëŸ¬ í™”ìì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ìƒí™©ì—ì„œ í•œê³„ì ì´ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ìµœê·¼ ë§ì´ í™œìš©ë˜ëŠ” ìƒì„± ìš”ì•½(abstractive summarization) ë°©ì‹ì€ ëŒ€í™” ë‚´ìš©ì—ì„œ ì¤‘ìš”í•œ í•µì‹¬ ë¬¸ì¥ ë˜ëŠ” ë‹¨ì–´êµ¬ ëª‡ê°€ì§€ë¥¼ ê·¸ëŒ€ë¡œ ë½‘ì•„ ìš”ì•½í•˜ëŠ” ì¶”ì¶œ ìš”ì•½(extractive summarization)ê³¼ ë‹¬ë¦¬, ëŒ€í™” ë¬¸ë§¥ì„ ê³ ë ¤í•´ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. ê¸´ ì‹œê°„ ë‹¤ìˆ˜ì˜ í™”ìê°€ ëŒ€í™”í•œ ë‚´ìš©ìœ¼ë¡œë¶€í„° í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì„ ë³„í•˜ì—¬ ê°€ë…ì„± ì¢‹ì€ ìš”ì•½ë¬¸ì„ ë§Œë“œëŠ”ê²Œ í•µì‹¬ì¸ë°ìš”. ì´ë•Œ ë¬¸ì¥ êµ¬ì¡°ë‚˜ ì–¸ê¸‰ëœ í‘œí˜„ì˜ ìˆ˜ì •ìœ¼ë¡œ, ë³´ë‹¤ ìì—°ìŠ¤ëŸ½ê³  ê¹”ë”í•œ í˜•íƒœì˜ ìš”ì•½ë¬¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ëª¨ë¸ì´ ì¬í•´ì„í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ê¸° ë•Œë¬¸ì— ë‚´ìš©ì´ ì˜ëª» ìš”ì•½ë  ìˆ˜ ìˆëŠ” ë¬¸ì œ(Factual Consistency)ë¥¼ ì•ˆê³  ìˆëŠ”ë°ìš”.&lt;/p&gt;

&lt;p&gt;ì‹¤ì œ 100ê°œì˜ í…ŒìŠ¤íŠ¸ì…‹ì„ ì§ì ‘ ì‚¬ëŒì´ ë¶„ì„í•œ ê²°ê³¼ 53%ê°€ ë‚´ìš©ì— ì˜¤ë¥˜ê°€ ìˆì—ˆê³ , ì´ ì¤‘ ì ˆë°˜ ì´ìƒì´ í™”ìì™€ ê´€ë ¨ëœ ì˜¤ë¥˜ì˜€ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ &lt;ê·¸ë¦¼1&gt;ê³¼ ê°™ì´ í™”ì ì •ë³´ê°€ ë°”ë€ŒëŠ” ê²½ìš°ì¸ë°ìš”. ì£¼ë¡œ í™”ìì™€ ê´€ë ¨ëœ ì—”í‹°í‹°(entity)ë‚˜ ë¦´ë ˆì´ì…˜(relation) ì„œìˆ ì— ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš°ê°€ ë§ì•˜ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì´ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì, í™”ì ì¤‘ì‹¬ì˜ ëŒ€í™”ë¬¸ ì‚¬í›„ í¸ì§‘(post-correction)ì„ í†µí•œ ìƒì„± ìš”ì•½ ë°©ë²•ë¡ ì„ ìƒˆë¡­ê²Œ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/ê·¸ë¦¼1&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-10-NewSum-CSI/001.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ1. í™”ìì™€ ê´€ë ¨ëœ ëŒ€í™” ìš”ì•½ ì˜¤ë¥˜ ì˜ˆì‹œ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2--capturing-speaker-incorrectness-ë°©ë²•ë¡ -ì†Œê°œ&quot;&gt;2.  Capturing Speaker Incorrectness ë°©ë²•ë¡  ì†Œê°œ&lt;/h1&gt;

&lt;p&gt;í•´ë‹¹ ë°©ë²•ë¡ ì˜ ê°€ì¥ í° íŠ¹ì§•ì€ ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ì—ì„œ í™”ì ì¤‘ì‹¬ì˜ ì‚¬í›„ í¸ì§‘ ì „ëµì„ ì ìš©í–ˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. í•´ë‹¹ ëª¨ë¸ì€ ì´ˆì•ˆ ìš”ì•½(draft summary) ë‹¨ê³„ì—ì„œ ì˜ëª» ì–¸ê¸‰ëœ í™”ì ì˜¤ë¥˜ ìœ í˜•ì„ ë¨¼ì € ì°¾ê³ , ì–´ë–¤ ìœ í˜•ìœ¼ë¡œ ì´ë¥¼ ìˆ˜ì •í•´ì•¼ ë ì§€ íŒë‹¨í•©ë‹ˆë‹¤. ì´í›„ ë¹ ì ¸ìˆëŠ” í™”ìëª…ì„ ì¶”ê°€í•˜ê±°ë‚˜, ì˜ëª» ì–¸ê¸‰ëœ í™”ìë¥¼ ì‚­ì œ, í˜¹ì€ êµì²´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìš”ì•½ ë‚´ìš©ì„ ìˆ˜ì •í•©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì •ëœ ê²°ê³¼ ì˜ˆì‹œëŠ” &lt;í‘œ2&gt;ì™€ ê°™ìŠµë‹ˆë‹¤.&lt;/í‘œ2&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-10-NewSum-CSI/002.png&quot; width=&quot;80%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ2. ëŒ€í™” êµì • í›„ ê²°ê³¼ ì˜ˆì‹œ (ë¹¨ê°„ìƒ‰ ì˜¤ë¥˜ ë¶€ë¶„ì„ íŒŒë€ìƒ‰ìœ¼ë¡œ ìˆ˜ì •)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ì¼ë°˜ì ìœ¼ë¡œ ìƒì„± ìš”ì•½ í•™ìŠµì—ì„œëŠ” ë ˆì´ë¸”ë§ëœ ìš”ì•½ë¬¸ ë°ì´í„°ì…‹ì´ í•„ìš”í•œë°ìš”. í•´ë‹¹ ë°©ë²•ë¡ ì—ì„œëŠ” ìê¸°ì§€ë„í•™ìŠµ(Self-supervised Learning) ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ì ì¸ ì–´ë…¸í…Œì´ì…˜(annotation) ì—†ì´ í•™ìŠµ ë°ì´í„°ë¥¼ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ê°•ê±´í•œ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ í•™ìŠµ ì‹œì— ëŒ€í™” ë§¥ë½ê³¼ ì´ˆì•ˆ ìš”ì•½ì´ ì£¼ì–´ì§€ë©´ í™”ì ë¦¬ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•˜ëŠ” í™”ì ìƒì„±ê¸°(speaker generator)ë¥¼ ë³´ì¡° íƒœìŠ¤í¬ë¡œ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-ì„±ëŠ¥-í‰ê°€&quot;&gt;3. ì„±ëŠ¥ í‰ê°€&lt;/h1&gt;

&lt;p&gt;ì‹¤ì œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì , ì •ì„±ì ì¸ í‰ê°€ ê¸°ì¤€ìœ¼ë¡œ ì¸¡ì •í•˜ì˜€ì„ ë•Œ, ì´ˆì•ˆ ìš”ì•½ ê²°ê³¼ì™€ ìˆ˜ì •ëœ ìš”ì•½ ê²°ê³¼ ëª¨ë‘ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ë¨¼ì € ë™ì¼í•œ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ë§ì´ ë§¤ì¹­ë˜ëŠ”ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê¸°ê³„ í‰ê°€ ë©”íŠ¸ë¦­ ROUGE ìŠ¤ì½”ì–´ë¥¼ í™œìš©í–ˆì„ ë•Œ, &lt;í‘œ3&gt;ì™€ ê°™ì€ ì„±ëŠ¥ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ Correction RateëŠ” í•´ë‹¹ ëª¨ë¸ì— ì˜í•´ ìˆ˜ì •ëœ ë¹„ìœ¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.&lt;/í‘œ3&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-10-NewSum-CSI/003.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ3. ì •ëŸ‰ì ì¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ (ROUGE score)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ROUGE ìŠ¤ì½”ì–´ëŠ” ë‹¨ì–´ì˜ ì‹œë©˜í‹± ìš”ì†ŒëŠ” ê³ ë ¤ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ì •ì„±ì ì¸ ì¸¡ë©´ì˜ í‰ê°€ë¥¼ ìœ„í•´ Amazon Mechanical Turk(AMT)ë¥¼ í™œìš©í•˜ì—¬ &lt;í‘œ3&gt;ê³¼ ê°™ì´ ì‹¤ì œ ì‚¬ëŒì˜ í‰ê°€ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/í‘œ3&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-10-NewSum-CSI/004.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ4. ì •ì„±ì ì¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ (Human Evaluation)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-í–¥í›„-ì—°êµ¬-ê³„íš&quot;&gt;4. í–¥í›„ ì—°êµ¬ ê³„íš&lt;/h1&gt;

&lt;p&gt;ì¥ì‹œê°„ì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ì‹œìŠ¤í…œì€ íšŒì˜ë¡ ì‘ì„± ë“± í˜„ì¬ ë‹¤ì–‘í•œ ë¶„ì•¼ì— í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ ê²°ê³¼ëŠ” ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì˜ ëŒ€í™” ìš”ì•½ ì„œë¹„ìŠ¤ì˜ ì ìš©ë˜ì–´ factual consistency ë¬¸ì œë¥¼ ê°œì„ í•˜ëŠ” ë°ì— í™œìš©ë  ì˜ˆì •ì…ë‹ˆë‹¤. ì•ìœ¼ë¡œë„ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì˜ AI ì—°êµ¬ì™€ ì„œë¹„ìŠ¤ì— ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;í˜„ì¬ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Labì—ì„œëŠ” ë‹¤ì–‘í•œ AI ì—°êµ¬ì™€ ì„œë¹„ìŠ¤í™”ë¥¼ í•¨ê»˜ ê³ ë¯¼í•´ë‚˜ê°ˆ ì—¬ëŸ¬ë¶„ì˜ ì§€ì›ì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. AIë¥¼ í†µí•´ ë”ìš± ê°€ì¹˜ìˆëŠ” ì„¸ìƒì„ ë§Œë“¤ê³ , ê¿ˆì„ í˜„ì‹¤ë¡œ ë§Œë“¤ì–´ê°€ëŠ” ì—¬ì •ì— í•¨ê»˜í•˜ì„¸ìš”!&lt;/p&gt;

&lt;p&gt;ğŸ‘¨ğŸ»â€ğŸ’» &lt;a href=&quot;http://kko.to/ailab_career&quot;&gt;ì¸ì¬ì˜ì…&lt;/a&gt;&lt;/p&gt;</content><author><name>ì´ë™ì—½:ì¹´ì¹´ì˜¤</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model</title><link href="https://kakaoenterprise.github.io/papers/emnlp-evaluation-dataset-and-strategy" rel="alternate" type="text/html" title="An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model" /><published>2021-11-07T00:00:00-05:00</published><updated>2021-11-07T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/emnlp-evaluation-dataset-and-strategy</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/emnlp-evaluation-dataset-and-strategy">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Multi-turn response selection models have recently shown comparable performance to humans in several benchmark datasets. However, in the real environment, these models often have weaknesses, such as making incorrect predictions based heavily on superficial patterns without a comprehensive understanding of the context. For example, these models often give a high score to the wrong response candidate containing several keywords related to the context but using the inconsistent tense. In this study, we analyze the weaknesses of the open-domain Korean Multi-turn response selection models and publish an adversarial dataset to evaluate these weaknesses. We also suggest a strategy to build a robust model in this adversarial environment.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì—ì„œëŠ” ì‹¤ì œ ì‚¬ëŒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ê°€ ê°€ëŠ¥í•œ ì˜¤í”ˆ ë„ë©”ì¸ ì±—ë´‡ (open domain Chatbot)ì„ ì—°êµ¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ í•´ë‹¹ ê¸°ìˆ ì€ AI ìŠ¤í”¼ì»¤ â€˜ì¹´ì¹´ì˜¤ë¯¸ë‹ˆâ€™, ì¢…í•© ì—…ë¬´ í”Œë«í¼ â€˜ì¹´ì¹´ì˜¤ì›Œí¬â€™, ì¹´ì¹´ì˜¤í†¡ì±„ë„ â€˜&lt;a href=&quot;https://pf.kakao.com/_lKxoMT&quot;&gt;ì™¸ê°œì¸ì•„ê°€&lt;/a&gt;â€™ ë“±ì— ì ìš©ë˜ì–´ ìˆëŠ”ë°ìš”.&lt;/p&gt;

&lt;p&gt;ì±—ë´‡ì´ ì‹¤ì œ ì‚¬ëŒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ í•˜ê¸° ìœ„í•´ì„œëŠ” ëŒ€í™” ë§¥ë½(context)ì— ë§ëŠ” ë‹µë³€ì„ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ì£¼ì–´ì§„ ì‘ë‹µ í›„ë³´ì—ì„œ ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ ì„ íƒí•˜ëŠ” ì‘ë‹µ ì„ íƒ(Response Selection) íƒœìŠ¤í¬ë¥¼ ì˜ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë³´ë‹¤ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì±—ë´‡ ì‹œìŠ¤í…œ êµ¬í˜„ì„ ìœ„í•œ ì‘ë‹µ ì„ íƒ ëª¨ë¸ êµ¬ì„± ì „ëµê³¼ ê°•ê±´ì„± í‰ê°€ ë°ì´í„°ì…‹ì„ ìƒˆë¡­ê²Œ ì œì•ˆí•˜ì˜€ê³ , &lt;a href=&quot;https://github.com/kakaoenterprise/KorAdvMRSTestData&quot;&gt;ê¹ƒí—ˆë¸Œ&lt;/a&gt; ìƒì— í•´ë‹¹ ë°ì´í„°ì…‹ì„ ê³µê°œí•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-ê¸°ì¡´-multi-turn-response-selection-ëª¨ë¸-ì—°êµ¬ì˜-í•œê³„&quot;&gt;1. ê¸°ì¡´ Multi-turn Response Selection ëª¨ë¸ ì—°êµ¬ì˜ í•œê³„&lt;/h1&gt;

&lt;p&gt;ìµœê·¼ì—ëŠ” ì‹¤ì œ ì‚¬ëŒê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€ì˜ ëŒ€í™” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ì‘ë‹µ ì„ íƒ ëª¨ë¸ ì—°êµ¬ ê²°ê³¼ë“¤ì´ ë‹¤ìˆ˜ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ë¥¼ ì‹¤ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œ ì ìš©í–ˆì„ ë•Œ, ëŒ€í™” ë§¥ë½ì„ í¬ê´„ì ìœ¼ë¡œ ì´í•´í•˜ì—¬ ë‹µë³€ì„ ë„ì¶œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í”¼ìƒì ì¸ íŒ¨í„´ì— í¬ê²Œ ì˜ì¡´í•˜ì—¬ ì˜ëª»ëœ ë‹µë³€ì„ í•˜ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì‹¤ì œ ì˜ë¯¸ ìƒìœ¼ë¡œëŠ” í‹€ë¦° ì‹œì œë‚˜ ë¶€ì •í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ì˜ëª»ëœ ë‹µì´ì§€ë§Œ, ëŒ€í™” ë§¥ë½ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œë¥¼ ë§ì´ ì‚¬ìš©í•˜ì—¬ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì•„ ì¼ì–´ë‚˜ëŠ” í˜„ìƒì¸ë°ìš”.&lt;/p&gt;

&lt;p&gt;ë³¸ ì—°êµ¬ì—ì„œëŠ” ì´ëŸ¬í•œ ì˜¤ë¥˜ íŒ¨í„´ì„ 7ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³ , ì´ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ê³¼ í•´ë‹¹ í™˜ê²½ì—ì„œ íš¨ê³¼ì ì¸ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì „ëµì„ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-í‰ê°€-ë°ì´í„°ì…‹ê³¼-ë°©ë²•ë¡ -ì†Œê°œ&quot;&gt;2. í‰ê°€ ë°ì´í„°ì…‹ê³¼ ë°©ë²•ë¡  ì†Œê°œ&lt;/h1&gt;

&lt;p&gt;ë¨¼ì € ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì´ êµ¬ì¶•í•œ ì·¨ì•½ì  í‰ê°€ ë°ì´í„°ì…‹(adversarial dataset)ì€ &lt;í‘œ1&gt;ê³¼ ê°™ì´ ì´ 2220ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì¼€ì´ìŠ¤ë¥¼ ê°ê° 7ê°€ì§€ ì˜¤ë¥˜ ìœ í˜•ìœ¼ë¡œ êµ¬ë¶„í•˜ì˜€ìŠµë‹ˆë‹¤. í•´ë‹¹ ìœ í˜•ìœ¼ë¡œëŠ” ì´ì „ ë¬¸ì¥ì„ ë°˜ë³µ(Repetition)í•˜ê±°ë‚˜, ë¶€ì •ì–´(Negation) ë˜ëŠ” ì‹œì œ(Tense)ë¥¼ ì˜ëª» ì‚¬ìš©í•œ ê²½ìš°, ì£¼ì²´/ëŒ€ìƒì„ í˜¼ë™í•œ ê²½ìš°(Subject-Object), ëª¨ìˆœ ì–´íœ˜(Lexcial Contradiction)ë¥¼ ì‚¬ìš©í•œ ê²½ìš°, í† í”½ì— ë”°ë¥¸ í™”ìš©ì  ì˜¤ë¥˜(Topic)ê°€ ë°œìƒí•œ ê²½ìš° ë“±ì´ ìˆìŠµë‹ˆë‹¤. ë‚´ë¶€ ì„œë¹„ìŠ¤ ë¡œê·¸ì—ì„œ ì˜ëª» ì‘ë‹µë˜ê±°ë‚˜, ë¹ˆë²ˆí•˜ê²Œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìœ í˜•ì„ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/í‘œ1&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-07-emnlp-evaluation-dataset-and-strategy/001.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ1. ê°œë³„ ì˜¤ë¥˜ ìœ í˜•ì— ë”°ë¥¸ í‰ê°€ ë°ì´í„°ì…‹ ìƒ˜í”Œ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ì´ ë°ì´í„°ì…‹ì€ ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œ ì˜¤ê°ˆ ìˆ˜ ìˆëŠ” ë³µì¡í•œ ì‘ë‹µì— ëŒ€ë¹„í•˜ê¸° ìœ„í•´, ì–¸ì–´í•™ ì „ë¬¸ê°€ì˜ ìƒì„¸í•œ ì§€ì¹¨ í•˜ì— ì–´ë ¤ìš´ ë‹µë³€ìœ¼ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆê²Œë” ìˆ˜ì‘ì—…ì„ í†µí•´ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. 5ëª…ì˜ ì‘ì—…ì(annotator)ê°€ ì´ 200ê°œì˜ ëŒ€í™” ì„¸ì…˜ì„ ì‘ì„±í•˜ê³ , ê° ì„¸ì…˜ì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ ì •ë‹µê³¼ ìœ„ ì§€ì¹¨ì— ê¸°ë°˜í•œ ì˜¤ë‹µì„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜í•œ, ê°•ê±´í•œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ì˜ ì „ëµì„ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. ë¨¼ì € íƒˆí¸í–¥(debiasing) ì „ëµì…ë‹ˆë‹¤. ì‹ ê²½ë§(Neural Network)ì€ ë°ì´í„°ì˜ ì‰½ê³  í‘œì¸µì ì¸ íŒ¨í„´ì— í¸í–¥ë˜ê²Œ(biased) í•™ìŠµë˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤. ëŒ€í™” ë°ì´í„°ì—ì„œ ì ì ˆí•œ ë‹µë³€ì€ ë³´í†µ ë¬¸ë§¥ì—ì„œ ë‹¤ë£¬ ì£¼ì œ(Topic)ì— ê¸°ë°˜í•˜ê±°ë‚˜, ë¬¸ë§¥ì— ë‚˜ì™”ë˜ í‚¤ì›Œë“œë“¤ì„ í™œìš©í•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤. ë³¸ ì—°êµ¬íŒ€ì€ ëª¨ë¸ì´ ì£¼ì œì™€ í‚¤ì›Œë“œì— í¸í–¥ë˜ê²Œ í•™ìŠµë˜ëŠ” ê²ƒì„ ì·¨ì•½ì ì˜ ì›ì¸ì´ë¼ ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ì— DRiFt[1] íƒˆí¸í–¥ ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ &lt;ê·¸ë¦¼1&gt;ê³¼ ê°™ì´ ì „ì²´ì ì¸ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ì˜€ê³ , ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/ê·¸ë¦¼1&gt;&lt;/p&gt;

&lt;p&gt;ë‘ ë²ˆì§¸ë¡œëŠ” ë©€í‹°íƒœìŠ¤í¬ ëŸ¬ë‹ê³¼ì˜ ê²°í•©ì…ë‹ˆë‹¤. ìµœê·¼ ë‹µë³€ ì„ íƒ íƒœìŠ¤í¬ì—ì„œëŠ” UMS[2]ì™€ ê°™ì€ ìê¸°ì§€ë„í•™ìŠµ(Self-supervised Learning) ê¸°ë°˜ ë©€í‹°íƒœìŠ¤í¬ ëŸ¬ë‹ì„ í™œìš©í•œ ê¸°ë²•ì´ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. ì•ì„œ ì–¸ê¸‰í•œ íƒˆí¸í–¥ ê¸°ë²•ê³¼ UMS ê¸°ë²•ì„ ê²°í•©í•˜ëŠ” ì „ëµì„ ì œì‹œí•˜ì—¬ ë”ìš± í–¥ìƒëœ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-07-emnlp-evaluation-dataset-and-strategy/002.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼1. ì „ì²´ì ì¸ ëª¨ë¸ êµ¬ì¡°&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-ì„±ëŠ¥-í‰ê°€&quot;&gt;3. ì„±ëŠ¥ í‰ê°€&lt;/h1&gt;

&lt;p&gt;ì‹¤ì œ ì„±ëŠ¥ í‰ê°€ëŠ” ì•„ë˜ &lt;í‘œ2&gt;ì™€ ê°™ì€ ë°ì´í„°ì…‹ êµ¬ì„±ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ë°ì´í„°ì…‹ì€ ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ êµ¬ì¶•ëœ ë°ì´í„°(In-domain), ì•ì„œ ì„¤ëª…ë“œë¦° ì—°êµ¬íŒ€ì´ ìƒˆë¡­ê²Œ êµ¬ì¶•í•œ ì·¨ì•½ì  í‰ê°€ ë°ì´í„°(Adv), ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ê³¼ ë™ì¼í•œ ë°©ì‹ì˜ ë°ì´í„°(Real)ë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.&lt;/í‘œ2&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-07-emnlp-evaluation-dataset-and-strategy/003.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ2. ë°ì´í„°ì…‹ êµ¬ì„±&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;í‘œ3&amp;gt;ê³¼ ê°™ì´ íƒˆí¸í–¥ ê¸°ë²•(deb)ê³¼ UMS ë°©ë²•ë¡ ì„ ê²°í•©í•œ ì „ëµì´ ì·¨ì•½ì  í‰ê°€ ë°ì´í„°(Adversarial)ì—ì„œ baseline ëª¨ë¸ ëŒ€ë¹„ +7.8%ì˜ ìƒë‹¹í•œ ë†’ì€ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ì‹¤ì œ í™˜ê²½ê³¼ ìœ ì‚¬í•œ ë°ì´í„°ì…‹(Real Env.)ì—ì„œë„ ë³¸ ì—°êµ¬íŒ€ì´ ì œì‹œí•œ ì „ëµì´ +4.2%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Adversarialí•œ í™˜ê²½ì—ì„œì˜ ê°•ê±´ì„±ì´ ì‹¤ì œ í™˜ê²½ì—ì„œë„ ìœ íš¨í•¨ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-07-emnlp-evaluation-dataset-and-strategy/004.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ3. ê° ë°ì´í„°ì…‹ê³¼ ë°©ë²•ë¡ ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;í‘œ4&amp;gt;ëŠ” ë³¸ ì—°êµ¬íŒ€ì´ ì œì‹œí•œ ì „ëµì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ì˜ ì‘ë™í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. ê¸°ì¡´ baseline ëª¨ë¸ì€ ë‹¨ìˆœíˆ â€˜í•¸ë“œí°â€™, â€˜ë°”ê¾¸ë‹¤â€™ ë¼ëŠ” í‚¤ì›Œë“œë§Œ ë“¤ì–´ê°€ìˆê³ , ë¬¸ë§¥ìƒ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ë‹µë³€ì— ì •ë‹µë³´ë‹¤ë„ ë” ë†’ì€ 0.998 ì´ë¼ëŠ” ì ìˆ˜ë¥¼ ì£¼ì—ˆìŠµë‹ˆë‹¤. ë°˜ë©´ ì œì‹œí•œ ì „ëµì„ í™œìš©í•˜ë©´ ì´ì™€ ê°™ì€ ì˜¤ë‹µì— 0.094ë¼ëŠ” ë‚®ì€ ì ìˆ˜ë¥¼ ì£¼ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-07-emnlp-evaluation-dataset-and-strategy/005.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ4. ì œì•ˆí•œ ì „ëµì´ ì˜ ì‘ë™í•˜ëŠ” ì˜ˆì‹œ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-í–¥í›„-ì—°êµ¬-ê³„íš&quot;&gt;4. í–¥í›„ ì—°êµ¬ ê³„íš&lt;/h1&gt;

&lt;p&gt;í˜„ì¬ ì´ ì—°êµ¬ ê²°ê³¼ëŠ” ì™¸ê°œì¸ì•„ê°€ ì±—ë´‡ì˜ ë‹µë³€ ì„ íƒ ë¡œì§ì— ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì™¸ê°œì¸ì•„ê°€ëŠ” ëŒ€í™” ì£¼ì œê°€ ì—´ë ¤ìˆëŠ” ë¹„ëª©ì ì„± ëŒ€í™”, ì¦‰ ì¼ìƒ ëŒ€í™”ì— ì´ˆì ì„ ë‘” ì±—ë´‡ì…ë‹ˆë‹¤. ì‹¤ì œ ì‚¬ëŒ ê°„ì˜ ëŒ€í™”ë¥¼ ì‚´í´ë³´ë©´ ì¼ìƒ ëŒ€í™”ë‚˜ ê°ì • êµë¥˜ ëª©ì ì˜ â€˜ìŠ¤ëª°í†¡(small talk)â€™ì´ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì´ ë†’ì€ë°ìš”. ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì—ì„œëŠ” ì´ëŸ¬í•œ ìŠ¤ëª°í†¡ì—ì„œ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ë°˜ì˜í•˜ì—¬ ì ì ˆí•œ ë°œí™”ë¥¼ ì„ íƒí•˜ëŠ” ê¸°ìˆ ì„ ì§€ì†ì ìœ¼ë¡œ ì—°êµ¬í•´, ì„œë¹„ìŠ¤ë¥¼ ê³ ë„í™”ì‹œì¼œ ë‚˜ê°ˆ ê³„íšì…ë‹ˆë‹¤. ì•ìœ¼ë¡œë„ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì˜ AI ì—°êµ¬ì™€ ì„œë¹„ìŠ¤ì— ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;ì°¸ì¡°-ë¬¸í—Œ&quot;&gt;ì°¸ì¡° ë¬¸í—Œ&lt;/h1&gt;

&lt;p&gt;[1] He et al., â€œ&lt;a href=&quot;https://arxiv.org/abs/1908.10763&quot;&gt;Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual&lt;/a&gt;â€, EMNLP, 2019.&lt;/p&gt;

&lt;p&gt;[2] Whang et al., â€œ&lt;a href=&quot;https://kakaoenterprise.github.io/papers/aaai2021-multi-turn-response-selection&quot;&gt;Do Response Selection Models Really Know Whatâ€™s Next? Utterance Manipulation Strategies for Multi-turn Response Selection&lt;/a&gt;â€, AAAI, 2021.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;í˜„ì¬ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Labì—ì„œëŠ” ë‹¤ì–‘í•œ AI ì—°êµ¬ì™€ ì„œë¹„ìŠ¤í™”ë¥¼ í•¨ê»˜ ê³ ë¯¼í•´ë‚˜ê°ˆ ì—¬ëŸ¬ë¶„ì˜ ì§€ì›ì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. AIë¥¼ í†µí•´ ë”ìš± ê°€ì¹˜ìˆëŠ” ì„¸ìƒì„ ë§Œë“¤ê³ , ê¿ˆì„ í˜„ì‹¤ë¡œ ë§Œë“¤ì–´ê°€ëŠ” ì—¬ì •ì— í•¨ê»˜í•˜ì„¸ìš”!&lt;/p&gt;

&lt;p&gt;ğŸ‘¨ğŸ»â€ğŸ’» &lt;a href=&quot;http://kko.to/ailab_career&quot;&gt;ì¸ì¬ì±„ìš©&lt;/a&gt;&lt;/p&gt;</content><author><name>mat:ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate</title><link href="https://kakaoenterprise.github.io/papers/emnlp-alignart" rel="alternate" type="text/html" title="AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate" /><published>2021-11-07T00:00:00-05:00</published><updated>2021-11-07T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/emnlp-alignart</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/emnlp-alignart">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition. Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we introduce AligNART, which leverages full alignment information to explicitly reduce the modality of the target distribution. AligNART divides the machine translation task into (i) alignment estimation and (ii) translation with aligned decoder inputs, guiding the decoder to focus on simplified one-to-one translation. To alleviate the alignment estimation problem, we further propose a novel alignment decomposition method. Our experiments show that AligNART outperforms previous non-iterative NART models that focus on explicit modality reduction on WMT14 Enâ†”De and WMT16 Roâ†’En. Furthermore, AligNART achieves BLEU scores comparable to those of the state-of-the-art connectionist temporal classification based models on WMT14 Enâ†”De. We also observe that AligNART effectively addresses the token repetition problem even without sequence-level knowledge distillation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Labì—ì„œëŠ” ê³ í’ˆì§ˆì˜ ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³ ì ì‹ ê²½ë§ ê¸°ê³„ ë²ˆì—­(Neural  Machine Translation, ì´í•˜ NMT) ì—°êµ¬ë¥¼ ì§„í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ê¸€ì—ì„œëŠ” ì†¡ì¢…ìœ¤ ë‹˜ì´ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Lab ì¸í„´ ë‹¹ì‹œ, ì„œìš¸ëŒ€ ì—°êµ¬íŒ€ê³¼ í•¨ê»˜ ì—°êµ¬í•œ AligNART ë°©ë²•ë¡ ì„ ê°„ëµí•˜ê²Œ ì†Œê°œë“œë¦¬ê³ ì í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-nmtneural--machine-translationì˜-ë“±ì¥&quot;&gt;1. NMT(Neural  Machine Translation)ì˜ ë“±ì¥&lt;/h1&gt;

&lt;p&gt;ë¨¼ì € ê¸°ê³„ ë²ˆì—­ ë°œì „ê³¼ì •ì„ ì§§ê²Œ ì‚´í´ë³´ë©´, ê¸°ì¡´ ê¸°ê³„ ë²ˆì—­ ë¶„ì•¼ëŠ” ìˆ˜ë§ì€ ê³¼ì •ì„ ê±°ì³ í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ë‚œ 2014ë…„ ë§, ì²˜ìŒ ë“±ì¥í•œ NMTëŠ” ê´€ë ¨ ì—…ê³„ì— í° í˜ì‹ ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;NMTëŠ” encoderì™€ decoderë¡œ êµ¬ì„±ëœ í•˜ë‚˜ì˜ í° ì¸ê³µì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ, ê°„ë‹¨íˆ ê¸°ê³„ ë²ˆì—­ì„ ì¶œë ¥í•  ìˆ˜ ìˆë„ë¡ ê·¸ êµ¬ì¡°ë¥¼ ì™„ì „íˆ ìƒˆë¡­ê²Œ ë°”ê¾¸ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ë„ ë²ˆì—­ ë¶„ì•¼ëŠ” ë”¥ëŸ¬ë‹ì´ ê°€ì¥ í¬ê²Œ í™œìš©ë˜ëŠ” ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì¸ë°ìš”. NMTì˜ ì²« ë“±ì¥ ì´í›„, NMTëŠ” ì§€ì†ì ì¸ ë°œì „ì„ ê±°ë“­í•´ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-ê¸°ì¡´-nmt-ì—°êµ¬ì˜-í•œê³„&quot;&gt;2. ê¸°ì¡´ NMT ì—°êµ¬ì˜ í•œê³„&lt;/h1&gt;

&lt;p&gt;ì¼ë°˜ì ìœ¼ë¡œ NMTì—ì„œëŠ” ìê¸°íšŒê·€(autoregressive) ëª¨ë¸ì´ í™œìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ìê¸°íšŒê·€ ëª¨ë¸ì€ ì¶œë ¥ ë‹¨ì–´ ê°„ì˜ ì¢…ì†ì„±ì„ ê³ ë ¤í•˜ì—¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì§€ë§Œ, ì´ë¡œ ì¸í•´ ë³‘ë ¬í™”ê°€ ì–´ë ¤ì›Œ ë””ì½”ë”© ì†ë„ê°€ ëŠë¦¬ë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ ìµœê·¼ì—ëŠ” ë‹¨ì–´ ê°„ì˜ ì¢…ì†ì„±ì„ ì§ì ‘ì ìœ¼ë¡œ ê³ ë ¤í•˜ê¸°ë³´ë‹¤, ì´ê°™ì€ ê³ ë ¤ë¥¼ ìµœì†Œí™”í•œ ë¹„ìê¸°íšŒê·€(non-autoregressive) ëª¨ë¸ì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë¹„ìê¸°íšŒê·€ ëª¨ë¸ì€ ë…ë¦½ì ì¸ ë‹¨ì–´ ìƒì„±ì„ í†µí•´ ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•´, ì†ë„ë¥¼ í¬ê²Œ ê°œì„ í•œ ì ì´ íŠ¹ì§•ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ, ì´ ê³¼ì •ì—ì„œ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ í† í°ì´ ë°˜ë³µë˜ê±°ë‚˜ ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ ëˆ„ë½í•˜ì—¬ ì˜ëª»ëœ ë²ˆì—­ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” ë©€í‹° ëª¨ë‹¬ë¦¬í‹°(multi modality) ë¬¸ì œë¥¼ ì•ˆê³  ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì—°êµ¬ëŠ” ë°”ë¡œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì‹œëœ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-alignart-ë°©ë²•ë¡ -ì†Œê°œ&quot;&gt;3. AligNART ë°©ë²•ë¡  ì†Œê°œ&lt;/h1&gt;

&lt;p&gt;AligNARTëŠ” ë‹¨ì–´ì˜ ì •ë ¬(alignment) ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë©€í‹° ëª¨ë‹¬ë¦¬í‹° ë¬¸ì œë¥¼ ê°œì„ í•˜ì˜€ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ëª¨ë“ˆì´ ë°”ë¡œ alignerì…ë‹ˆë‹¤. alignerëŠ” ìˆ˜ë§ì€ íƒ€ê²Ÿ í† í°ì˜ ì •ë ¬ ì •ë³´ë¥¼ ë™ì‹œì— ì˜ˆì¸¡í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ë¥¼ ìœ„í•´ ì •ë ¬ ë¶„í•´(alignment decomposition) êµ¬ì¡°ë¥¼ ìƒˆë¡­ê²Œ êµ¬ì„±í•˜ì—¬ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. â€˜Duplication - Permutation - Groupingâ€™ 3ë‹¨ê³„ë¥¼ í†µí•´ ë‹¨ì–´ì˜ ì¤‘ì²©ì„ ì˜ˆìƒí•˜ê³ , ë‹¨ì–´ì˜ ì–´ìˆœì´ ë°”ë€Œì§€ ì•Šê³  ë²ˆì—­ë  ìˆ˜ ìˆë„ë¡ ê·¸ ìˆœì„œë°°ì—´ì„ ì •ë¦¬í•˜ê³ , ë³µí•© í˜•íƒœì˜ ë‹¨ì–´ê°€ ë‹´ê³  ìˆëŠ” ë‹¨ì¼ ì˜ë¯¸ë¥¼ ê³ ë ¤í•˜ì—¬ ìµœì¢… ë‹¨ì–´ë¥¼ ë§¤í•‘í•˜ëŠ” ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ì¶œë ¥ëœ ì •ë ¬ ì •ë³´ë¥¼ ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ê°’(input)ìœ¼ë¡œ í™œìš©í•˜ì—¬ ë²ˆì—­ ì„±ëŠ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-07-EMNLP-AligNART/001.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼1. (ì¢Œ)AligNART ì „ì²´ êµ¬ì¡°, (ìš°)Alignment decomposition êµ¬ì¡°&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-ì„±ëŠ¥-í‰ê°€&quot;&gt;4. ì„±ëŠ¥ í‰ê°€&lt;/h1&gt;

&lt;p&gt;ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´, WMT14 Enâ†”Deì™€ WMT16 Roâ†”En ì¼€ì´ìŠ¤ì—ì„œ BLEU scoreë¥¼ ì¸¡ì •í•˜ì˜€ìŠµë‹ˆë‹¤. BLEUëŠ” ê¸°ê³„ ë²ˆì—­ ê²°ê³¼ì™€ ì‚¬ëŒì´ ì§ì ‘ ë²ˆì—­í•œ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ê·¸ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ê¸°ê³„ ë²ˆì—­ì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;AligNARTëŠ” í•´ë‹¹ í‰ê°€ì—ì„œ SOTA ìˆ˜ì¤€ì˜ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” aligner ëª¨ë“ˆì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” ê²ƒì´ ì£¼ìš” ëª©ì ì´ì—ˆê¸° ë•Œë¬¸ì—, ë³„ë„ì˜ ëª©ì í•¨ìˆ˜ë¥¼ ì¡°ì •í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í–¥í›„ í›„ì† ì—°êµ¬ì—ì„œ ëª©ì í•¨ìˆ˜ë¥¼ ìµœì í™”í•œë‹¤ë©´ í˜„ì¬ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ê°€ ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-07-EMNLP-AligNART/002.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ1. BLEU Score ì¸¡ì • ê²°ê³¼&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;5-í–¥í›„-ì—°êµ¬-ê³„íš&quot;&gt;5. í–¥í›„ ì—°êµ¬ ê³„íš&lt;/h1&gt;

&lt;p&gt;AligNART ëª¨ë¸ì€ ê¸°ì¡´ NMTë³´ë‹¤ ì¶”ë¡ ì´ ë‹¨ìˆœí•˜ê³  ì†ë„ê°€ ë¹¨ë¼, ì—£ì§€ ë””ë°”ì´ìŠ¤ì™€ ê°™ì´ ê°€ë³ê³  ë¹ ë¥¸ ë²ˆì—­ ê²°ê³¼ê°€ í•„ìš”í•œ ê³³ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í–¥í›„ ì—£ì§€ ë””ë°”ì´ìŠ¤ ìƒì—ì„œ ë³´ë‹¤ ë¹ ë¥´ê³  ì •í™•í•œ ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ê´€ë ¨ ì—°êµ¬ë¥¼ ì§€ì†ì ìœ¼ë¡œ ë°œì „ì‹œì¼œ ë‚˜ê°ˆ ê³„íšì…ë‹ˆë‹¤. ì•ìœ¼ë¡œë„ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì˜ AI ì—°êµ¬ì— ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;â–º ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì˜ ë²ˆì—­ì„œë¹„ìŠ¤ê°€ ê¶ê¸ˆí•˜ë‹¤ë©´ ì§€ê¸ˆ ë°”ë¡œ &lt;a href=&quot;https://translate.kakao.com/&quot;&gt;ì¹´ì¹´ì˜¤ i ì—”ì§„&lt;/a&gt;ì„ í™•ì¸í•´ë³´ì„¸ìš”!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;í˜„ì¬ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Labì—ì„œëŠ” ë‹¤ì–‘í•œ AI ì—°êµ¬ì™€ ì„œë¹„ìŠ¤í™”ë¥¼ í•¨ê»˜ ê³ ë¯¼í•´ë‚˜ê°ˆ ì—¬ëŸ¬ë¶„ì˜ ì§€ì›ì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. AIë¥¼ í†µí•´ ë”ìš± ê°€ì¹˜ìˆëŠ” ì„¸ìƒì„ ë§Œë“¤ê³ , ê¿ˆì„ í˜„ì‹¤ë¡œ ë§Œë“¤ì–´ê°€ëŠ” ì—¬ì •ì— í•¨ê»˜í•˜ì„¸ìš”!&lt;/p&gt;

&lt;p&gt;ğŸ‘¨ğŸ»â€ğŸ’» &lt;a href=&quot;http://kko.to/ailab_career&quot;&gt;ì¸ì¬ì±„ìš©&lt;/a&gt;&lt;/p&gt;</content><author><name>ì†¡ì¢…ìœ¤:ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ, ì„œìš¸ëŒ€</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">Distilling Global and Local Logits with Densely Connected Relations</title><link href="https://kakaoenterprise.github.io/papers/iccv-distilling-global-and-local-logits" rel="alternate" type="text/html" title="Distilling Global and Local Logits with Densely Connected Relations" /><published>2021-10-11T00:00:00-05:00</published><updated>2021-10-11T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/iccv-distilling-global-and-local-logits</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/iccv-distilling-global-and-local-logits">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;In prevalent knowledge distillation, logits in most image recognition models are computed by global average pooling, then used to learn to encode the high-level and task-relevant knowledge. In this work, we solve the limitation of this global logit transfer in this distillation context. We point out that it prevents the transfer of informative spatial information, which provides localized knowledge as well as rich relational information across contexts of an input scene. To exploit the rich spatial information, we propose a simple yet effective logit distillation approach. We add a local spatial pooling layer branch to the penultimate layer, thereby our method extends the standard logit distillation and enables learning of both finely-localized knowledge and holistic representation. Our proposed method shows favorable accuracy improvement against the state-of-the-art methods on several image classification datasets. We show that our distilled students trained on the image classification task can be successfully leveraged for object detection and semantic segmentation tasks; this result demonstrates our methodâ€™s high transferability.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ë³¸ ê¸€ì—ì„œëŠ” ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Labê³¼ ê²½í¬ëŒ€, í¬í•­ê³µëŒ€ì—ì„œ í•¨ê»˜ ì—°êµ¬í•œ ìƒˆë¡œìš´ ì§€ì‹ ì¦ë¥˜(Knowledge Distillation) ë°©ë²•ë¡ ì´ ICCV 2021 í•™íšŒë¥¼ í†µí•´ ë°œí‘œë˜ì–´, ê°„ëµí•˜ê²Œ ì†Œê°œë“œë¦¬ê³ ì í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-ì§€ì‹-ì¦ë¥˜-ê¸°ë²•ì˜-ë“±ì¥&quot;&gt;1. ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì˜ ë“±ì¥&lt;/h1&gt;

&lt;p&gt;ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì€ ê¸° í•™ìŠµëœ ê±°ëŒ€ ëª¨ë¸(teacher)ì˜ ì§€ì‹ì„ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬(student)ê°€ ì „ë‹¬ë°›ì•„ í•™ìŠµí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, â€˜teacher-studentâ€™ ëª¨ë¸ì´ë¼ê³ ë„ í•©ë‹ˆë‹¤. ì˜ í•™ìŠµëœ teacher ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ, ê²½ëŸ‰í™”ëœ student ëª¨ë¸ì´ í•™ìŠµì„ í†µí•´ ê·¸ì— ë²„ê¸ˆê°€ëŠ” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‚´ê³ ì í•œë‹¤ëŠ” ì ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì€ ê¸°ì¡´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ê°€ì§€ëŠ” ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ê³¼ ë©”ëª¨ë¦¬ì˜ í•œê³„, ê¸´ ì¶”ë¡ ì‹œê°„ ë“±ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì ì œì‹œëœ ë°©ë²•ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡, ì¦‰ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ë” ë†’ì€ ì„±ëŠ¥ì„ ë‚´ê²Œ ë©ë‹ˆë‹¤. ë” ë§ì€ ì—°ì‚°ëŸ‰ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” ëŒ€ëŸ‰ì˜ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ì™€ í•™ìŠµì‹œê°„ì„ í•„ìš”ë¡œ í•˜ê¸° ë•Œë¬¸ì—, ìƒëŒ€ì ìœ¼ë¡œ ì €ì„±ëŠ¥, ì €ì „ë ¥ì˜ ì†Œí˜• ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ë‚˜ IoT ê¸°ê¸°ì—ëŠ” ì´ë¥¼ í™œìš©í•˜ê¸° ì–´ë µë‹¤ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  íš¨ìœ¨ì ì¸ ë”¥ëŸ¬ë‹ í•™ìŠµì„ ì‹œë„í•˜ê¸° ìœ„í•œ ì›€ì§ì„ìœ¼ë¡œ, í˜„ì¬ ì§€ì‹ ì¦ë¥˜ ê¸°ë²• ì—°êµ¬ê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-ê¸°ì¡´-ì§€ì‹-ì¦ë¥˜-ê¸°ë²•ì˜-í•œê³„&quot;&gt;2. ê¸°ì¡´ ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì˜ í•œê³„&lt;/h1&gt;

&lt;p&gt;ì§€ì‹ ì¦ë¥˜ ê³¼ì •ì—ì„œ teacher ëª¨ë¸ì˜ ì •ë³´ë¥¼ student ëª¨ë¸ì— ì „ë‹¬í•˜ê¸° ìœ„í•´, logit ê°’ì´ í™œìš©ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ logitì€ í•´ë‹¹ taskì™€ ì§ì ‘ì ì¸ ì—°ê´€ì„ ê°€ì§€ëŠ” ëª¨ë¸ì˜ ì¶œë ¥ê°’ìœ¼ë¡œ, featureë³´ë‹¤ ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•œ ëª¨ë¸ì˜ representation ì •ë³´ë¥¼ í’ë¶€í•˜ê²Œ ë‹´ê³  ìˆì–´ í•™ìŠµ ë°ì´í„°ì™€ í•¨ê»˜ ì‚¬ìš©ë  ê²½ìš° íš¨ìœ¨ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ë•Œ logitì€ ê³¼ì í•©(overfitting)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ global average pooling(GAP)ìœ¼ë¡œ ê³„ì‚°ë˜ëŠ”ë°, ì´ë¡œ ì¸í•´ logitì„ í™œìš©í•œ ê¸°ì¡´ ì—°êµ¬ë“¤ì—ì„œëŠ” ì„¸ë¶€ ê³µê°„ì— ëŒ€í•œ logitì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ëŠ” í•œê³„ê°€ ì¡´ì¬í–ˆìŠµë‹ˆë‹¤. ì´ì— ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì€ global logitê³¼ local logitì´ë¼ëŠ” ê°œë…ì„ ìƒˆë¡­ê²Œ ì œì‹œí•œ â€˜Global and Local Logit with Densely Connected Relations(GLD)â€™ ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ì—¬ ì´ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-global-and-local-logit-with-densely-connected-relationsgld-ë°©ë²•ë¡ -ì†Œê°œ&quot;&gt;3. Global and Local Logit with Densely Connected Relations(GLD) ë°©ë²•ë¡  ì†Œê°œ&lt;/h1&gt;

&lt;p&gt;í•´ë‹¹ ë°©ë²•ë¡ ì˜ ê°€ì¥ í° íŠ¹ì§•ì€ ì•ì„œ ì–¸ê¸‰í•œ global logitê³¼ local logitì„ ëª¨ë‘ ì‚¬ìš©í–ˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë¨¼ì € global logitì€ ì…ë ¥ëœ ì´ë¯¸ì§€ì˜ global featureì—ì„œ classifierë¥¼ ê±°ì³ ìµœì¢…ì ìœ¼ë¡œ ì‚°ì¶œëœ ê°’ìœ¼ë¡œ, ê¸°ì¡´ ì—°êµ¬ì—ì„œ ì£¼ë¡œ í™œìš©ë˜ë˜ ì •ë³´ì…ë‹ˆë‹¤. ì—¬ê¸°ì— global featureë¥¼ ì„¸ë¶„í™”í•œ ë’¤, ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‚°ì¶œí•œ ê°’ì´ local logitì…ë‹ˆë‹¤. ë³´ë‹¤ ì„¸ë°€í•œ ê³µê°„ ì •ë³´ë¥¼ ë‹´ì€ local logitì„ ì¶”ê°€ì ìœ¼ë¡œ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ë””í…Œì¼í•œ ì •ë³´ë¥¼ í•™ìŠµì— ë°˜ì˜í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜í•œ, ê°œë³„ ì´ë¯¸ì§€ ìƒ˜í”Œì—ì„œ ì„¸ë¶€ ì˜ì—­ì˜ ê´€ê³„ ì •ë³´ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŒì€ ë¬¼ë¡ , ì—¬ëŸ¬ ìƒ˜í”Œê°’ ê°„ì˜ ê´€ê³„ ì •ë³´ë„ íŒŒì•…í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/001.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼1. GLD ì „ì²´ êµ¬ì¡°&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ì´ë°–ì—ë„ GLDì—ì„œëŠ” logit ê°’ì„ ì •ê·œí™”í•˜ê¸° ìœ„í•´ loss í•¨ìˆ˜ LNDì„ ìƒˆë¡­ê²Œ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë¡ ì—ì„œëŠ” softmax í•¨ìˆ˜ ì‚¬ìš©ìœ¼ë¡œ ì •ë³´ëŸ‰ì´ ì ì€ í™•ë¥  ë¶„í¬ê°€ ë„ì¶œë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ê³ ì •ëœ temperature íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í™•ë¥  ë¶„í¬ì˜ ì •ë³´ëŸ‰ì„ ì¦ê°€ì‹œì¼°ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì´ëŠ” ê°œë³„ ì´ë¯¸ì§€ë§ˆë‹¤ í™•ë¥  ë¶„í¬ì˜ íŠ¹ì§•ì´ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì§€ ì•Šì•˜ë‹¤ëŠ” í•œê³„ê°€ ìˆì—ˆê¸° ë•Œë¬¸ì—, GLDì—ì„œëŠ” ì…ë ¥ ì´ë¯¸ì§€ë§ˆë‹¤ logitì˜ í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œë³„ ì¡°ì •í•˜ëŠ” ë°©ì‹ì„ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-ì„±ëŠ¥-í‰ê°€&quot;&gt;4. ì„±ëŠ¥ í‰ê°€&lt;/h1&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•´, 9ê°€ì§€ì˜ ì§€ì‹ ì¦ë¥˜ SOTA(State-of-the-art) ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ì‹¤í—˜ê²°ê³¼ëŠ” ê°ê° CIFAR-100, ImageNet, CINIC-10, STL-10, VOC2007, COCO2017 ë°ì´í„°ì…‹ìœ¼ë¡œ ì¸¡ì •í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë¨¼ì € ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ CIFAR-100, ImageNet ë°ì´í„°ë¥¼ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤. CIFAR-100 í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œì˜ ë°©ë²•ë¡  ê²€ì¦ì„ ìœ„í•´ ì´ 4ê°€ì§€ì˜ ì‹¤í—˜ëª¨ë¸ì„ êµ¬ì„±í•˜ì—¬ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ì˜€ê³ , ImageNet í…ŒìŠ¤íŠ¸ì—ì„œëŠ” Top-1ê³¼ Top-5 ì •í™•ë„ë¥¼ êµ¬ë¶„í•˜ì—¬ í‰ê°€í•œ ê²°ê³¼ ê¸°ì¡´ ì—°êµ¬ ëª¨ë¸ë³´ë‹¤ í‰ê· ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/002.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ1. ì‹¤í—˜ëª¨ë¸ êµ¬ì„± (CIFAR-100 ë°ì´í„°ì…‹ ê¸°ì¤€)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/003.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ2. GLDì™€ ê¸°ì¡´ ë°©ë²•ë¡  ì„±ëŠ¥ ë¹„êµ (CIFAR-100 ë°ì´í„°ì…‹ ê¸°ì¤€)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/004.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ3. GLDì™€ ê¸°ì¡´ ë°©ë²•ë¡  ì„±ëŠ¥ ë¹„êµ (ImageNet ë°ì´í„°ì…‹ ê¸°ì¤€)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ë˜í•œ, ì‹¤ì§ˆì ìœ¼ë¡œ teacher ëª¨ë¸ì—ì„œ student ëª¨ë¸ë¡œ ì–¼ë§ˆë‚˜ ì§€ì‹ì´ ì˜ ì „ë‹¬ë  ìˆ˜ ìˆëŠ”ì§€, ê·¸ ì„±ëŠ¥ì„ í‰ê°€í•´ë³´ê³ ì í•™ìŠµë˜ì§€ ì•Šì€ CINIC-10ê³¼ STL-10 ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í•œ ê²°ê³¼ SOTAë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/005.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ4. GLDì™€ ê¸°ì¡´ ë°©ë²•ë¡  ì„±ëŠ¥ ë¹„êµ (CINIC-10ê³¼ STL-10 ë°ì´í„°ì…‹ ê¸°ì¤€)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ì´ë°–ì—ë„ ê°ì²´ íƒì§€ì™€ ì‹œë©˜í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ ë¬¸ì œì—ì„œ ì„±ëŠ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´ VOC2007, COCO2017 ë°ì´í„°ì…‹ì„ í™œìš©í•œ ê²°ê³¼, ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ í™•ì¸í•¨ì€ ë¬¼ë¡ , ì´ë¯¸ì§€ ë¶„ë¥˜ë¿ë§Œ ì•„ë‹ˆë¼ feature representation ì˜ì—­ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒ„ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/006.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ5. ê°ì²´ íƒì§€ ë¬¸ì œì—ì„œì˜ GLDì™€ ê¸°ì¡´ ë°©ë²•ë¡  ì„±ëŠ¥ ë¹„êµ (VOC2007ê³¼ COCO2017 ë°ì´í„°ì…‹ ê¸°ì¤€)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-10-11-ICCV-distilling-global-and-local-logits/007.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ6. ì‹œë©˜í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ ë¬¸ì œì—ì„œì˜ GLDì™€ ê¸°ì¡´ ë°©ë²•ë¡  ì„±ëŠ¥ ë¹„êµ (VOC2007ê³¼ COCO2017 ë°ì´í„°ì…‹ ê¸°ì¤€)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;5-í–¥í›„-ì—°êµ¬-ê³„íš&quot;&gt;5. í–¥í›„ ì—°êµ¬ ê³„íš&lt;/h1&gt;

&lt;p&gt;í–¥í›„ í•´ë‹¹ ë°©ë²•ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ê²½ëŸ‰í™”í•˜ì—¬ ì–¼êµ´ ì¸ì‹ ë° ì—¬ëŸ¬ê°€ì§€ ë¹„ì „ ê¸°ë°˜ ì„œë¹„ìŠ¤ì˜ ê³„ì‚°ëŸ‰ì„ íš¨ìœ¨í™”í•˜ëŠ” ë°ì— í™œìš©í•  ê³„íšì…ë‹ˆë‹¤. ì•ìœ¼ë¡œë„ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬ì— ë§ì€ ê´€ì‹¬ê³¼ ì‘ì› ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.&lt;/p&gt;</content><author><name>harry:ê²½í¬ëŒ€, ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><category term="papers" /><category term="ICCV" /><category term="distillation" /><category term="GLD" /><summary type="html">Abstract</summary></entry><entry><title type="html">Improving End-to-End Contextual Speech Recognition via a Word-Matching Algorithm with Backward Search</title><link href="https://kakaoenterprise.github.io/papers/ieee-e2e-csr" rel="alternate" type="text/html" title="Improving End-to-End Contextual Speech Recognition via a Word-Matching Algorithm with Backward Search" /><published>2021-10-04T00:00:00-05:00</published><updated>2021-10-04T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/ieee-e2e-csr</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/ieee-e2e-csr">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;End-to-end automatic speech recognition (E2E-ASR) prefers the common words during training rather than rare ones related to contextual information such as song names. Thus, recognizing contextual information correctly is a hurdle for E2E-ASR to reach the production-level. To overcome the limitations of E2E-ASR in recognizing contextual information, this work presents a post-processing followed by E2E-ASR in an algorithmic way, referred to as a word-matching algorithm with backward search (WMA-BS). At first, we allow E2E-ASR to roughly detect the position of target words that has similar pronunciation with desired contextual phrases. After that, given the hypothesis from E2E-ASR with the rough position of target words, WMA-BS estimates the correct target words and decides whether to replace the target words with the contextual phrase or not, according to their phonetic and literal similarity. Applying the proposed method to E2E-ASR achieved relative improvement up to 52.7% in word error rate across several harsh conditions.&lt;/p&gt;</content><author><name>jaytee:ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><category term="papers" /><summary type="html">Abstract</summary></entry><entry><title type="html">UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation</title><link href="https://kakaoenterprise.github.io/papers/interspeech2021-univnet" rel="alternate" type="text/html" title="UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation" /><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech2021-univnet</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech2021-univnet">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Lab ìŒì„±ì²˜ë¦¬íŒ€ì€ ì¹´ì¹´ì˜¤ iì— ì ìš©ë˜ëŠ” TTS(Text to Speech) ì—°êµ¬ë¥¼ ì§„í–‰í•´ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.â€‹â€‹â€‹ TTS ì‹œìŠ¤í…œì€ í¬ê²Œ í…ìŠ¤íŠ¸ì—ì„œ acoustic featureë¥¼ ìƒì„±í•˜ëŠ” ì–´ì¿ ìŠ¤í‹± ëª¨ë¸(acoustic model)ê³¼ ì´ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì—ì„œ ìŒì„±ì‹ í˜¸ë¥¼ í•©ì„±í•´ AI ìŒì„±ì„ ë§Œë“¤ì–´ë‚´ëŠ” ë³´ì½”ë”(vocoder)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë³´ì½”ë”ëŠ” ê³ í’ˆì§ˆì˜ ìŒì„±ì„ ìƒì„±í•˜ëŠ”ë° ì£¼ìš”í•œ ì—­í• ì„ ë‹´ë‹¹í•˜ê³  ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/001.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼1. TTS êµ¬ì¡°&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì€ ê¸°ì¡´ ì—°êµ¬ë³´ë‹¤ ê°œì„ ëœ ê³ í’ˆì§ˆ ìŒì„± í•©ì„±ì„ ê°€ëŠ¥ì¼€í•˜ëŠ” ë‰´ëŸ´ ë³´ì½”ë” ê¸°ìˆ  â€˜UnivNetâ€™ì„ ê³ ì•ˆí•´, ì´ë²ˆ INTERSPEECH 2021ì—ì„œ ì—°êµ¬ ë‚´ìš©ì„ ê³µê°œí•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ë‚œí•´ ìŒì„±í•©ì„± ëª¨ë¸ê³¼ ìŒì†Œ-ì˜¤ë””ì˜¤ ì •ë ¬ ëª¨ë¸ì„ í•œêº¼ë²ˆì— í›ˆë ¨í•˜ëŠ” ì•„í‚¤í…ì²˜ â€˜JDI-Tâ€™ë¥¼ ê³µê°œí•œë° ì´ì–´, 2ë…„ ì—°ì† ì—°êµ¬ ì„±ê³¼ë¥¼ ë°œí‘œí•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ê¸€ì—ì„œëŠ” ì´ë²ˆ ì—°êµ¬ ì„±ê³¼ì— ëŒ€í•´ ê°„ëµí•˜ê²Œ ì†Œê°œë“œë¦¬ê³ ì í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-ê¸°ì¡´-ë‰´ëŸ´-ë³´ì½”ë”-ì—°êµ¬ì˜-í•œê³„&quot;&gt;1. ê¸°ì¡´ ë‰´ëŸ´ ë³´ì½”ë” ì—°êµ¬ì˜ í•œê³„&lt;/h1&gt;

&lt;p&gt;ëŒ€ë‹¤ìˆ˜ ë‰´ëŸ´ ë³´ì½”ë”(neural vocoder) ì—°êµ¬ì—ì„œëŠ” ì „ì²´ ì£¼íŒŒìˆ˜ ëŒ€ì—­ ì¤‘ ì¼ë¶€(0-8kHz)ì— í•´ë‹¹í•˜ëŠ” ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨(mel-spectrogram)ì„ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì€ ì¸ê°„ì˜ ì¸ì§€ ê¸°ì¤€ì— ë”°ë¼ í—¤ë¥´ì¸ (Hz) ë‹¨ìœ„ì˜ ì£¼íŒŒìˆ˜ë¥¼ mel-scaleì— ë”°ë¼ ë³€í™˜í•œ ê°’ìœ¼ë¡œ, ë”¥ëŸ¬ë‹ì—ì„œ ì˜¤ë””ì˜¤ ì‹ í˜¸ ì²˜ë¦¬ì— ë§ì´ í™œìš©ë˜ëŠ” í”¼ì³(feature)ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ëŒë“¤ì€ ê³ ì£¼íŒŒë³´ë‹¤ ì €ì£¼íŒŒë¥¼ ì˜ ì¸ì§€í•˜ê¸° ë•Œë¬¸ì—, ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì˜ ì €ì£¼íŒŒ ë¶€ë¶„ì„ ë³´ë‹¤ ì˜ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ì €ì£¼íŒŒ ë¶€ë¶„ì„ í™•ì¥ì‹œí‚¨ ì ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ë•Œ, ì „ì²´ ëŒ€ì—­í­ì„ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ìŒí–¥ì •ë³´ê°€ ë”ìš± ë§ì•„ì ¸ ê¹¨ë—í•œ ìŒì„±ì„ ì–»ì„ ìˆ˜ ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , ì¼ë¶€ê°’ë§Œì´ í™œìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì „ì²´ ê°’ì„ í™œìš©í•œ ì¼ë¶€ ì—°êµ¬ì—ì„œëŠ” í•©ì„± ìŒì„±ì˜ ê³ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì´ íë¦¿í•´ì§€ëŠ” ë¬¸ì œ(over smoothing)ê°€ ë°œìƒí•´, ìŒì„±ì— ì§€ì§€ì§ê±°ë¦¬ëŠ” ì†Œë¦¬ ë“± ì¡ìŒì´ ì„ì—¬ ê¸°ëŒ€í–ˆë˜ ê²ƒ ì´ìƒì˜ ìŒì„± í’ˆì§ˆì„ ì–»ê¸° ì–´ë ¤ìš´ ê²½ìš°ë“¤ì´ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì€ ì´ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì ìƒˆë¡œìš´ ë‰´ëŸ´ ë³´ì½”ë” ë°©ë²•ë¡  â€˜UnivNetâ€™ì„ ê³ ì•ˆí•´, ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì—ì„œ ë”ìš± ê¹¨ë—í•œ ìŒì„±ì„ ì œê³µí•˜ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-univnet-íŠ¹ì§•-ì†Œê°œ&quot;&gt;2. UnivNet íŠ¹ì§• ì†Œê°œ&lt;/h1&gt;

&lt;p&gt;ë¨¼ì € UnivNetì˜ êµ¬ì¡°ëŠ” &lt;ê·¸ë¦¼2&gt;ì™€ ê°™ì´ í¬ê²Œ ìƒì„±ê¸°(generator)ì™€ íŒë³„ê¸°(discriminator)ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤. generatorëŠ” MelGAN ê¸°ë°˜ êµ¬ì¡°ì´ë©°, ì—¬ê¸°ì— ë”í•´ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ë¡œì»¬ ì •ë³´ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•´ LVC(Location-Variable Convolution)ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ mel-spectrogramì˜ ì§€ì—­ë³„ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì— ì œê³µí•˜ì—¬, ë” ì ì€ íŒŒë¼ë¯¸í„° ìˆ˜ë¡œë„ ë” ë†’ì€ ìŒì§ˆì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, íš¨ìœ¨ì ì¸ ì—°ì‚°ì„ ìœ„í•´ GAU(Gated Activation Unit)ë¥¼ ë”í–ˆìŠµë‹ˆë‹¤.&lt;/ê·¸ë¦¼2&gt;&lt;/p&gt;

&lt;p&gt;ë‹¤ìŒìœ¼ë¡œ discriminatorì—ì„œëŠ” generatorì—ì„œ ìƒì„±ëœ ê°€ì§œ ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„°ë¥¼ êµ¬ë³„í•˜ë„ë¡ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì£¼ëª©í•  ì ì€ multi-resolution spectrogram discriminator(ì´í•˜ MRSD)ë¥¼ ì‚¬ìš©í–ˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. MRSDëŠ” ë‹¤ì–‘í•œ STFT íŒŒë¼ë¯¸í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ì™€ ìƒì„±ëœ ê°€ì§œ ë°ì´í„°ì˜ ì—¬ëŸ¬ ì„ í˜• ìŠ¤í™íŠ¸ë¡œê·¸ë¨ í¬ê¸°ë“¤ì„ ê³„ì‚°í•´, ê° í•˜ìœ„ íŒë³„ê¸°ì— ì…ë ¥ê°’ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤. ì´ë•Œ, STFT íŒŒë¼ë¯¸í„°ì…‹ì—ëŠ” 1)í‘¸ë¦¬ì— ë³€í™˜ ì°¨ìˆ˜, 2)ì‹œê°„(frame) ì´ë™ ê°„ê²©, 3)ìœˆë„ìš°(window) ê¸¸ì´ê°€ í¬í•¨ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;UnivNetì€ MRSD êµ¬ì¡°ë¥¼ í†µí•´ ì „ì²´ ëŒ€ì—­í­ ë°ì´í„°ê°€ ê°€ì§„ ë‹¤ì–‘í•œ ì‹œê°„ê³¼ í•´ìƒë„(resolution) ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì‚¬ëŒ ìŒì„±ê³¼ ê°™ì€ ë†’ì€ ìˆ˜ì¤€ì˜ ìŒì„±ì„ ìƒì„±í•˜ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤. MRSD êµ¬ì¡°ëŠ” MelGANì˜ multi-scale waveform discriminator(MSWD) êµ¬ì¡°ì— ê¸°ë°˜í•˜ë©°, ì—¬ê¸°ì— ì‹œê°„ ì˜ì—­ì—ì„œ ì ëŒ€ì  ëª¨ë¸ë§ì„ ê°œì„ í•˜ê¸° ìœ„í•´ multi-period waveform discriminator(MPWD)ë¥¼ ë”í•œ ì ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/002.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼2. UnivNet êµ¬ì¡°&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-ì‹¤í—˜-ê²°ê³¼&quot;&gt;3. ì‹¤í—˜ ê²°ê³¼&lt;/h1&gt;

&lt;h3 id=&quot;1-ë°ì´í„°-êµ¬ì„±&quot;&gt;1) ë°ì´í„° êµ¬ì„±&lt;/h3&gt;

&lt;p&gt;í•´ë‹¹ ì‹¤í—˜ì—ëŠ” LibriTTS ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤. LibriTTS ë°ì´í„°ì…‹ì€ ì˜ì–´ ì˜¤ë””ì˜¤ë¶ ë°ì´í„°ì…‹ìœ¼ë¡œ, ì´ ì¤‘ 192ì‹œê°„ ë¶„ëŸ‰, 11ë§Œ 6ì²œê°œì˜ ë°œí™”, 904ëª…ì˜ í™”ì ë°ì´í„°ë¡œ êµ¬ì„±ëœ â€˜train-clean-360â€™ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨ì„ ì§„í–‰í•˜ê³ , ì´ë¯¸ ì•„ëŠ” í™”ì(seen speaker)ì— ëŒ€í•œ í‰ê°€ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. 9ì‹œê°„ ë¶„ëŸ‰, 4ì²œê°œì˜ ë°œí™”, 39ëª…ì˜ í™”ì ë°ì´í„°ë¡œ êµ¬ì„±ëœ â€˜train-cleanâ€™ ë°ì´í„°ì…‹ìœ¼ë¡œëŠ” ì²˜ìŒ ë³´ëŠ” í™”ì(unseen speaker)ì— ëŒ€í•œ í‰ê°€ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, TTS ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ” 24ì‹œê°„ ë¶„ëŸ‰, 1ë§Œ 3ì²œê°œ ë°œí™” ë°ì´í„°ë¡œ êµ¬ì„±ëœ LJSpeech ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤. í•´ë‹¹ ë°ì´í„°ì…‹ì€ ì˜ì–´ë¡œ êµ¬ì„±ëœ ë‹¨ì¼ í™”ìì˜ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;2-ablation-study&quot;&gt;2) Ablation study&lt;/h3&gt;

&lt;p&gt;ë¨¼ì € í•´ë‹¹ ëª¨ë¸ì˜ ìì²´ ì„±ëŠ¥ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ Ablation studyë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ê° êµ¬ì„±ìš”ì†ŒëŠ” G1=LVC, G2=GAU, D1=MRSD, D2=MPWD, D3=MSWDì™€ ê°™ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì£¼ëª©í•  ì ì€ &lt;í‘œ1&gt;ì—ì„œ D1(MRSD)ì´ ì œê±°ë˜ë©´ &lt;ê·¸ë¦¼3&gt;ì˜ ì™¼ìª½ ê·¸ë¦¼ê³¼ ê°™ì´ ìƒì„±ëœ ìŒì„±ì˜ ê³ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì´ íë¦¿í•´ì§€ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì œì•ˆí•˜ëŠ” ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” &lt;ê·¸ë¦¼3&gt;ì˜ ê°€ìš´ë° ê·¸ë¦¼ì„ ë³´ë©´ ì´ëŸ¬í•œ ë¬¸ì œê°€ ê°œì„ ë˜ì–´ ì‹¤ì œ ë…¹ìŒ ìŒì„±ì˜ ê³ ì£¼íŒŒìˆ˜ ëŒ€ì—­ê³¼ ë¹„ìŠ·í•´ì§„ ì ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Univnet ëª¨ë¸ì—ì„œ MRSD êµ¬ì¡°ê°€ ê°€ì§€ëŠ” ì¤‘ìš”ì„±ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/ê·¸ë¦¼3&gt;&lt;/ê·¸ë¦¼3&gt;&lt;/í‘œ1&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/003.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ1. Ablation study ê²°ê³¼&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/004.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼3. ì˜¤ë””ì˜¤ í´ë¦½ì—ì„œ ìƒì„±ëœ ìŠ¤í™íŠ¸ë¡œê·¸ë¨&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-ê¸°ì¡´-ë³´ì½”ë”-ëª¨ë¸ê³¼-ì„±ëŠ¥-ë¹„êµ&quot;&gt;3) ê¸°ì¡´ ë³´ì½”ë” ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ&lt;/h3&gt;

&lt;p&gt;ë‹¤ìŒìœ¼ë¡œëŠ” generatorì˜ ì±„ë„ í¬ê¸°ê°€ ë‹¤ë¥¸ ë‘ ê°€ì§€ ë²„ì „ì˜ UnivNet(UnivNet-16, UnivNet-32)ì„ ì¤€ë¹„í•˜ì—¬ GAN ê¸°ë°˜ ë³´ì½”ë”(MelGAN, Parallel WaveGAN, HiFi-GAN)ì™€ ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ì•˜ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë³´ì½”ë” ìì²´ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì‹¤ì œ í™”ìì˜ ìŒì„±(Seen/Unseen speakers)ì—ì„œ ì¶”ì¶œëœ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ë³´ì½”ë”ë¥¼ ì´ìš©í•˜ì—¬ ìŒì„±ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. &lt;í‘œ2&gt;ë¥¼ ë³´ë©´ UnivNet-16ì€ í•™ìŠµ ë•Œ í™œìš©ëœ í™”ì ë°ì´í„° ì™¸ì— ì²˜ìŒ ë³´ëŠ” í™”ì ë°ì´í„°ì—ì„œë„ ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ìŒì„±ì„ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ê¸°ì¡´ ë³´ì½”ë” ëª¨ë¸ì˜ ê²½ìš° ìƒˆë¡œìš´ í™”ìê°€ ì¶”ê°€ë  ë•Œë§ˆë‹¤ ëª¨ë¸ì„ ì¶”ê°€ í•™ìŠµí•´ì•¼ í•˜ëŠ” ë¶ˆí¸í•¨ì´ ìˆì—ˆì§€ë§Œ, UnivNetì€ ì²˜ìŒ ë³´ëŠ” í™”ì ë°ì´í„°ì—ì„œë„ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ë‹¤ëŠ” ì ì—ì„œ ì´ê°™ì€ ë¬¸ì œë¥¼ ìƒë‹¹ìˆ˜ ê°œì„ í•˜ì˜€ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, UnivNet-32ì˜ ê²½ìš° ì „ì²´ ë¶„ì•¼ì—ì„œ ê¸°ì¡´ ë³´ì½”ë” ëª¨ë¸ë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ê³ , ì¶”ë¡  ì†ë„ë„ ë‹¤ì†Œ ì ˆê°ì‹œí‚¤ëŠ” ë“± ì˜ë¯¸ìˆëŠ” ê²°ê³¼ê°’ì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ì´ì–´ ì§„í–‰ëœ TTS ì„±ëŠ¥ í‰ê°€ì—ì„œë„ UnivNet-32ì€ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë³´ì½”ë” ì„±ëŠ¥ë¿ë§Œ ì•„ë‹ˆë¼, ì‹¤ì œ TTS êµ¬ì¡° ìƒì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/í‘œ2&gt;&lt;/p&gt;

&lt;p&gt;í•´ë‹¹ ì—°êµ¬ëŠ” ê¸°ì¡´ ë³´ì½”ë” ëª¨ë¸ë³´ë‹¤ ë” ë§ì€ ëŒ€ì—­í­ì„ ì“°ë©´ì„œë„ over-smoothing ë¬¸ì œ ì—†ì´ ë” ê¹¨ë—í•œ, ì„ ëª…í•œ í•©ì„±ìŒì„ ì–»ì—ˆë‹¤ëŠ” ì ì—ì„œ ì˜ì˜ê°€ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ìŒì„± í•©ì„± ë¶„ì•¼ì—ì„œ í•©ì„±ìŒì˜ í’ˆì§ˆë¿ë§Œ ì•„ë‹ˆë¼, ì¤‘ìš”í•œ ì¶”ë¡  ì†ë„ë¥¼ ì ˆê°ì‹œì¼°ë‹¤ëŠ” ì ë„ ì£¼ëª©í• ë§Œí•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-UnivNet/005.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ2. ê¸°ì¡´ ë³´ì½”ë” ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-í–¥í›„-ê³„íš&quot;&gt;4. í–¥í›„ ê³„íš&lt;/h1&gt;

&lt;p&gt;í–¥í›„ UnivNetì€ ì¹´ì¹´ì˜¤ i ì„œë¹„ìŠ¤ì— ì ìš©ë˜ì–´ ì‹¤ì œ ë‹¤ìˆ˜ì˜ ì‚¬ìš©ìë¥¼ ëŒ€ìƒìœ¼ë¡œ ì„œë¹„ìŠ¤ë  ì˜ˆì •ì…ë‹ˆë‹¤. í•©ì„±ìŒì˜ ëª…ë£Œë„ì™€ ìì—°ì„±ì´ ì‹¤ì œ ì‚¬ëŒì˜ ë°œí™” ìˆ˜ì¤€ê³¼ ë™ì¼í•  ì •ë„ë¡œ, ê·¸ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì§€ì† ì—°êµ¬í•  ê³„íšì…ë‹ˆë‹¤. ë˜í•œ, fine-tuning ê³¼ì • ì—†ì´ë„ ì—¬ëŸ¬ í™”ìì˜ TTS íŒŒì´í”„ë¼ì¸ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ìœ ë‹ˆë²„ì…œ ë³´ì½”ë” ì—°êµ¬ë¥¼ ì§„í–‰í•˜ê³ ì í•©ë‹ˆë‹¤. ì•ìœ¼ë¡œë„ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆì˜ ìŒì„±ì²˜ë¦¬ ì—°êµ¬ì— ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.&lt;/p&gt;</content><author><name>taylor:ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ</name></author><category term="papers" /><category term="INTERSPEECH" /><category term="UnivNet" /><category term="vocoder" /><summary type="html">Abstract</summary></entry><entry><title type="html">Auxiliary Sequence Labeling Tasks for Disfluency Detection</title><link href="https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection" rel="alternate" type="text/html" title="Auxiliary Sequence Labeling Tasks for Disfluency Detection" /><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><id>https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection</id><content type="html" xml:base="https://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Detecting disfluencies in spontaneous speech is an important preprocessing step in natural language processing and speech recognition applications. Existing works for disfluency detection have focused on designing a single objective only for disfluency detection, while auxiliary objectives utilizing linguistic information of a word such as named entity or part-of-speech information can be effective. In this paper, we focus on detecting disfluencies on spoken transcripts and propose a method utilizing named entity recognition(NER) and part-of-speech(POS) as auxiliary sequence labeling(SL) tasks for disfluency detection. First, we investigate cases that utilizing linguistic information of a word can prevent mispredicting important words and can be helpful for the correct detection of disfluencies. Second, we show that training a disfluency detection model with auxiliary SL tasks can improve its F-score in disfluency detection. Then, we analyze which auxiliary SL tasks are influential depending on baseline models. Experimental results on the widely used English Switchboard dataset show that our method outperforms the previous state-of-the-art in disfluency detection.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ê·¸ë™ì•ˆ ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ AI Labì—ì„œëŠ” ìŒì„±ì¸ì‹ ì˜ì—­ê³¼ ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ì— í™œìš© ê°€ëŠ¥í•œ â€˜ì‚¬ì¡±ì œê±°(disfluency detection)â€™ë¥¼ ì£¼ì œë¡œ, ì—°êµ¬ë¥¼ ì§„í–‰í•´ ì™”ìŠµë‹ˆë‹¤. ì´ë²ˆ INTERSPEECH 2021ì—ì„œ NER(ê°œì²´ëª… ì¸ì‹), POS(í’ˆì‚¬ íƒœê·¸) ì •ë³´ë¥¼ í™œìš©í•´ ì‚¬ì¡±ì œê±° ì‘ì—…ì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ê³µê°œí•˜ê²Œ ë˜ì–´, í•´ë‹¹ ë‚´ìš©ì„ ê°„ëµí•˜ê²Œ ì†Œê°œë“œë¦¬ê³ ì í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-ì‚¬ì¡±ì œê±°-ì—°êµ¬ì˜-í•„ìš”ì„±&quot;&gt;1. ì‚¬ì¡±ì œê±° ì—°êµ¬ì˜ í•„ìš”ì„±&lt;/h1&gt;

&lt;p&gt;ë¨¼ì € ì‚¬ì¡±ì œê±° ì‘ì—…ì€ í™”ìê°€ â€œìŒâ€, â€œì•„â€ì™€ ê°™ì´ ì˜ë¯¸ ì—†ì´ ë°œí™”í•œ ê°„íˆ¬ì‚¬ë¥¼ êµì •í•˜ê³ , ë°œí™” ì¤‘ ë°˜ë³µ ì‚¬ìš©í•œ í‘œí˜„ë“¤ì„ ì •ì œí•˜ëŠ” ê³¼ì •ì„ ë§í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ â€œì–´â€¦ ì˜¤ëŠ˜, ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì°¸ ì¢‹ë„¤â€ë¼ëŠ” ë¬¸ì¥ì—ì„œ ì‚¬ì¡±ì„ ì œê±°í•œë‹¤ë©´, ë³„ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ì§€ì§€ ì•ŠëŠ” â€œì–´â€¦â€ì™€ ë°˜ë³µëœ â€œì˜¤ëŠ˜â€ì„ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì‹¤ì œ ë°œí™” ìƒí™©ì—ì„œëŠ” ì´ì™€ ê°™ì€ ê°„íˆ¬ì‚¬ë‚˜ ìˆ¨ì†Œë¦¬, ë¨¸ë­‡ê±°ë¦¼ ë“± ìŒì„± ì „ì‚¬ í›„ ì–¸ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ”ë° ë¶ˆí•„ìš”í•œ ë°œí™”ê°€ ë‹¤ìˆ˜ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ë°œí™”ë¥¼ ìŒì„±ì¸ì‹ ëª¨ë¸ì´ ì¸ì‹í•˜ê³ , ìŒì„±ë²ˆì—­, ì–¸ì–´ì´í•´ ë“± ìì—°ì–´ ì²˜ë¦¬(NLP)ë¥¼ í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ëŸ¬í•œ ì‚¬ì¡±ì„ ì œê±°í•˜ëŠ” ì‘ì—…ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-ê¸°ì¡´-ì‚¬ì¡±ì œê±°-ì—°êµ¬ì˜-í•œê³„&quot;&gt;2. ê¸°ì¡´ ì‚¬ì¡±ì œê±° ì—°êµ¬ì˜ í•œê³„&lt;/h1&gt;

&lt;p&gt;ê¸°ì¡´ ì‚¬ì¡±ì œê±° ì—°êµ¬ëŠ” word ë‹¨ìœ„ì˜ ìì§ˆ(feature)ì„ ê°ê° reparandum(RM), interregnum(IM), repair(RP)ë¡œ êµ¬ë¶„í•˜ì—¬ ìš°ë¦¬ê°€ ì œê±°í•´ì•¼í•  ëŒ€ìƒ(RM, IM)ê³¼ ì•„ë‹Œ ê²ƒ(RP)ì„ ëª…í™•íˆ íŒŒì•…í•˜ëŠ”ë° ì´ˆì ì„ ë‘ì—ˆìŠµë‹ˆë‹¤. IMì—ëŠ” â€˜ìŒ, ì•„â€™ì™€ ê°™ì€ ê°„íˆ¬ì‚¬ í‘œí˜„ë“¤ì´ í•´ë‹¹ë˜ê³ , ì´ë“¤ì€ ì‰½ê²Œ êµ¬ë¶„ë˜ëŠ” íŠ¹ì„±ì„ ê°€ì¡Œê¸° ë•Œë¬¸ì— IMë³´ë‹¤ëŠ” ì£¼ë¡œ RMì— ëŒ€í•œ ì˜ˆì¸¡ì •í™•ë„ë¥¼ ë†’ì´ëŠ”ë° ì¤‘ì ì„ ë‘” ì—°êµ¬ê°€ ì£¼ë¥¼ ì´ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ RMì€ ì‰½ê²Œ ë§í•´, ë¬¸ì¥ ì‹œì‘ì— ë“¤ì–´ê°€ëŠ” ì¿ ì…˜ì–´, ë°˜ë³µì–´ í‘œí˜„ë“¤ì„ ì¼ì»«ëŠ”ë°, ë¨¼ì € ì…ë ¥ëœ ê°’(RM)ì— ëŒ€í•´ ì˜¤ë¥˜ë¼ê³  íŒë‹¨í•˜ê³  ë’¤ì— ì˜¤ëŠ” ê°’(RP)ì„ ì˜¬ë°”ë¥¸ ê°’ìœ¼ë¡œ ë³´ì•˜ìŠµë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì´ RMì„ íŒŒì•…í•˜ëŠ”ë° ëª©ì ì„ ë‘ê³ , RMìœ¼ë¡œ ë¶„ë¥˜ëœ ì •ë‹µ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ ì£¼ë¡œ í™œìš©ë˜ì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/001.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼1. ê¸°ì¡´ ì‚¬ì¡±ì œê±° ì—°êµ¬ì˜ ì–´ë…¸í…Œì´ì…˜(annotation) ì˜ˆì‹œ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;í•˜ì§€ë§Œ, ì´ì™€ ê°™ì´ ì œê±° ëŒ€ìƒë§Œì„ ê°€ë ¤ë‚´ëŠ”ë° ì´ˆì ì„ ë‘ë©´ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ë‹¨ì–´ë¥¼ ì‚¬ì¡±ìœ¼ë¡œ ì˜¤ì˜ˆì¸¡(false positive)í•˜ê²Œ ë˜ê³  ì˜ˆì¸¡ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” í•œê³„ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ì—, ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì€ ì¶”ê°€ì ì¸  ì •ë³´ë“¤ì„ í•™ìŠµì‹œì¼œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°©ë²•ë¡ ì„ ê³ ì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-auxiliary-sequence-labeling-tasks-ë°©ë²•ë¡ -ì†Œê°œ&quot;&gt;3. Auxiliary Sequence Labeling Tasks ë°©ë²•ë¡  ì†Œê°œ&lt;/h1&gt;

&lt;p&gt;ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì´ ìƒˆë¡­ê²Œ ì œì•ˆí•˜ëŠ” ë°©ë²•ë¡ ì€ â€˜Auxiliary Sequence Labeling Tasksâ€™ì…ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ì—ì„œ ì¶”ê°€ì ì¸ ì •ë³´ë¡œ NER(ê°œì²´ëª… ì¸ì‹, Named Entity Recognition), POS(í’ˆì‚¬ íƒœê·¸, Part-Of-Speech)ë¥¼ Multi-Task Learning ë°©ì‹ì— í™œìš©í•´, ì´ 3ê°œì˜ ëª©ì í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ë©´, ë¨¼ì € NERì€ í•´ë‹¹ ë‹¨ì–´ì˜ ê°œì²´ëª…(Entity Name)ì´ ì¸ëª…, ì¥ì†Œ, ì‹œê°„ í‘œí˜„ ë“±ì„ ì •ì˜í•˜ëŠ” ê³¼ì œ(task)ì…ë‹ˆë‹¤.  ê·¸ë¦¼2ë¥¼ ì˜ˆì‹œë¡œ ë³´ë©´ NER ì •ë³´ëŠ” ì´ˆë¡ìƒ‰ìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” ì‚¬ì¡±ì— í•´ë‹¹í•˜ëŠ” â€˜i wouldâ€™ë¥¼ ëª…í™•íˆ íŒŒì•…í•˜ì§€ ëª»í•˜ê³ , ì˜¤íˆë ¤ ë¬¸ì¥ì˜ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆëŠ” â€˜my fortyâ€™ë¥¼ ì‚¬ì¡±ìœ¼ë¡œ ì˜¤ì˜ˆì¸¡í•˜ì˜€ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì´ì²˜ëŸ¼ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°–ëŠ” ë‹¨ì–´ì˜ ì˜¤ì˜ˆì¸¡ì„ ë°©ì§€í•˜ê³ , ì •í™•í•œ ì‚¬ì¡± ì˜ˆì¸¡ì´ ê°€ëŠ¥í•´ì§€ë„ë¡ NER ì •ë³´ë¥¼ í•™ìŠµì— í™œìš©í–ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/002.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼2. NER ì •ë³´ ì˜ˆì‹œ (ì´ˆë¡ìƒ‰ì´ NERì— í•´ë‹¹í•˜ë©°, íŒŒë€ìƒ‰ì´ ì‚¬ì¡±ì— í•´ë‹¹í•¨)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ë˜í•œ, ì¶”ê°€ì ìœ¼ë¡œ ì‚¬ìš©ëœ POSëŠ” ê° ë¬¸ì¥ ì„±ë¶„ì˜ í’ˆì‚¬ íƒœê·¸ ì •ë³´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ê·¸ë¦¼3ì—ì„œëŠ” RMê³¼ RPì˜ POSê°€ ë™ì¼í•œ íƒœê·¸ í˜•íƒœë¥¼ ê°€ì§ˆ ë•Œ, ë³´ë‹¤ ì •í™•í•œ ì‚¬ì¡± ì˜ˆì¸¡ì´ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ì—ì„œë„ POSë¥¼ í™œìš©í•œ ì—°êµ¬ë“¤ì€ ì§„í–‰ë˜ì—ˆì§€ë§Œ, rule-base ë°©ì‹ì˜ ê²½ìš° ë‹¨ì–´ ì˜ë¯¸ê°€ ë‹´ê¸´ ì‹œë§¨í‹± ì •ë³´ê°€ ì œëŒ€ë¡œ ë°˜ì˜ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” í•œê³„ê°€ ìˆì—ˆê³ , MLë°©ì‹ì˜ ê²½ìš° ì¶”ë¡ (inference) ì‹œ í•´ë‹¹ ì…ë ¥ ë‹¨ì–´ë“¤ì— ëŒ€í•œ featureë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì¶”ê°€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ ì—°êµ¬íŒ€ì€ POSì™€ NER ì •ë³´ë¥¼ ì¶”ê°€ í™œìš©í•˜ëŠ” Auxiliary Sequence Labeling Tasks ë°©ë²•ë¡ ì„ ìƒˆë¡­ê²Œ ì œì•ˆí•˜ì—¬ ê·¸ë™ì•ˆì˜ ë¬¸ì œì ë“¤ì„ í•´ê²°í•˜ê³  ë” ë†’ì€ ì„±ëŠ¥ì„ ì–»ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/003.png&quot; width=&quot;70%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;ê·¸ë¦¼3. POS ì •ë³´ ì˜ˆì‹œ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ë³¸ ì—°êµ¬ì—ì„œëŠ” ì‚¬ì¡±ì œê±°, NER, ê·¸ë¦¬ê³  POSíƒœê·¸ë“¤ì„ ì˜ˆì¸¡ì„ ìœ„í•´ 3ê°€ì§€ì˜ negative log likelihood loss í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì‚¬ì¡±ì œê±°, NER, ê·¸ë¦¬ê³  POS í…ŒìŠ¤í¬ë¥¼ ìœ„í•œ loss í•¨ìˆ˜ë“¤ì„ ê°ê° Ld, Le, Lpë¼ê³  ì •ì˜í• ë•Œ, ìµœì¢… loss í•¨ìˆ˜ëŠ” L= Ld + *(Le +Lp)ë¡œ ì •ì˜í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ  ëŠ” ê³„ìˆ˜(coefficients)ë¡œì¨, NERê³¼ POS í…ŒìŠ¤í¬ë“¤ì˜ ì˜í–¥ë„ë¥¼ í•™ìŠµì— ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€ ê²°ì •í•˜ëŠ” ìš”ì†Œì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-ì„±ëŠ¥-í‰ê°€&quot;&gt;4. ì„±ëŠ¥ í‰ê°€&lt;/h1&gt;

&lt;p&gt;ì œì•ˆí•œ ë°©ë²•ì˜ ì„±ëŠ¥í‰ê°€ë¥¼ ìœ„í•´ ê¸°ì¡´ ì—°êµ¬ì™€ì˜ ëª¨ë¸ ì„±ëŠ¥ë¹„êµì™€ í•™ìŠµìš”ì†Œë³„ ì„±ëŠ¥ë¹„êµ 2ê°€ì§€ ë°©ì‹ì„ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë¨¼ì € ë™ì¼í•œ CRF Layer(Decoder)ì— ì…ë ¥ ìì§ˆ(feature input)ë¡œ ì‚¬ì „í›ˆë ¨ëœ ì–¸ì–´ëª¨ë¸ transformer, BERT, ELECTRAì˜ ìµœì¢… output layerë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ê°ì˜ ì„±ëŠ¥ì„ ë¹„êµí•´ ë³´ì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ì— ë§ì´ í™œìš©ë˜ê³  ìˆëŠ” English Switchboard ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•´ë³¸ ê²°ê³¼, F1 scoreì—ì„œ ê¸°ì¡´ ì—°êµ¬ ëª¨ë¸ë³´ë‹¤ í‰ê· ì ìœ¼ë¡œ ë†’ì€ ìˆ˜ì¹˜ë¥¼ ê¸°ë¡í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆê³ , ìµœê³ ì¹˜ì˜ ê²½ìš° 93.1(ELECTRA)ì— ë‹¬í•˜ëŠ” ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/004.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ1. ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ê³¼ Auxiliary SL(Sequence Labeling) Tasks ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ë‹¤ìŒìœ¼ë¡œ, ë™ì¼í•œ English Switchboard ë°ì´í„°ì…‹ì„ ì´ìš©í•´ Ablation ë¶„ì„ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. 1)ê¸°ì¡´ ì—°êµ¬ ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸, 2)NERì„ ì¶”ê°€í•œ ê²½ìš°, 3)POSë¥¼ ì¶”ê°€í•œ ê²½ìš°, 4)AUXILIARY SL Tasks ë°©ì‹(NERê³¼ POSë¥¼ ëª¨ë‘ ì¶”ê°€, SL : Sequence Labeling)ì„ ê°ê° ë¹„êµí•´ ë³´ì•˜ìŠµë‹ˆë‹¤. í‘œ2ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ì¶”ê°€ ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•œ ê²½ìš°ê°€ F1 scoreì—ì„œ ê°€ì¥ ë†’ì€ ìˆ˜ì¹˜ë¥¼ ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/005.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ2. Ablation Analysis ë¶„ì„ ê²°ê³¼&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ì´ë¥¼ í†µí•´ ì¶”ê°€ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ëª©ì í•¨ìˆ˜ë¥¼ í™•ëŒ€í•˜ëŠ” ê²ƒì´ ì‹¤ì œ ì„±ëŠ¥ì— ì˜í–¥ë ¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤ëŠ” ì ì—ì„œ í•´ë‹¹ ì—°êµ¬ê°€ ê°–ëŠ” ì˜ì˜ê°€ í¬ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì¶”ë¡ ì—ì„œë„ ë³¸ ì—°êµ¬ì˜ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ì—°êµ¬ì—ì„œë„ ì´ì™€ ë¹„ìŠ·í•˜ê²Œ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë‹¨ì–´ì˜ NERê³¼ POSì™€ ê°™ì€ ì¶”ê°€ ìì§ˆë“¤ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë“¤ì´ ìˆì—ˆì§€ë§Œ, ì´ëŸ¬í•œ ë°©ë²•ë“¤ì€ ì¶”ë¡ ì‹œ, ë‹¨ì–´ì˜ ìì§ˆë“¤ì„ ì¶”ì¶œí•˜ëŠ”ë° ì¶”ê°€ ì‹œê°„ì´ í•„ìš”í•œ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.í•˜ì§€ë§Œ ë³¸ ì—°êµ¬ì—ì„œëŠ” NERì™€ POS tasksë¥¼ í•™ìŠµì‹œê°„ì—ë§Œ í™œìš©í•˜ê³ , ì¶”ë¡ ì‹œì—ëŠ” ì‚¬ìš©ì„ í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì¶”ë¡  ì‹œê°„ì„ ë‹¨ì¶•ì‹œí‚¤ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, ì•„ë˜ì˜ í‘œ3ê³¼ ê°™ì´ ì¶”ë¡ ì„ ìœ„í•´ ì¶”ê°€ì ìœ¼ë¡œ ìš”êµ¬ë˜ëŠ” ì‹œê°„ì€ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-08-30-INTERSPEECH-SL-tasks-for-disfluency-detection/006.png&quot; width=&quot;50%&quot; align=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em class=&quot;center&quot;&gt;í‘œ3. Auxiliary SL Tasksë¥¼ í™œìš©í•œ ë°©ì‹(Ours)ê³¼ í™œìš©í•˜ì§€ ì•Šì€ ë°©ì‹(Ours w/o aux)ê°„ì˜ ì¶”ë¡  ì†ë„ ë¹„êµ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;5-í–¥í›„-ì—°êµ¬-ê³„íš&quot;&gt;5. í–¥í›„ ì—°êµ¬ ê³„íš&lt;/h1&gt;

&lt;p&gt;í•´ë‹¹ ì—°êµ¬ëŠ” í˜„ì¬ í—¤ì´ì¹´ì¹´ì˜¤ì•± ì¤‘ ë°›ì•„ì“°ê¸° ê¸°ëŠ¥ì— ì ìš©ë˜ì—ˆê³ , ë…¹ìŒ ë‚´ìš©ì„ ì „ì‚¬í•œ ê²°ê³¼ì— ì‚¬ì¡±ì œê±°ê¸°ëŠ¥ìœ¼ë¡œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. í–¥í›„ í•œêµ­ì–´ ì„œë¹„ìŠ¤ ê³ ë„í™”ë¥¼ ìœ„í•´ í•œêµ­ì–´ìš© ì‚¬ì¡±ì œê±° ëª¨ë¸ ê°œì„ ì— ì§‘ì¤‘í•˜ê³  ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆë„ë¡ ì—°êµ¬ë¥¼ ë°œì „ì‹œì¼œë‚˜ê°ˆ ê³„íšì…ë‹ˆë‹¤. ì•ìœ¼ë¡œë„ ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.&lt;/p&gt;</content><author><name>ì´ë™ì—½:ì¹´ì¹´ì˜¤</name></author><category term="papers" /><category term="INTERSPEECH" /><category term="NLP" /><category term="Disfluency-detection" /><summary type="html">Abstract</summary></entry></feed>